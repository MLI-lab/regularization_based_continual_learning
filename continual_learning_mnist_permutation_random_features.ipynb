{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continual learning on the MNIST permutation task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num GPUs 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from torchvision import datasets\n",
    "from torch.nn import functional as F\n",
    "#from torch import nn\n",
    "from torch import autograd\n",
    "\n",
    "GPU = True\n",
    "if GPU == True:\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    dtype = torch.cuda.FloatTensor\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "    print(\"num GPUs\",torch.cuda.device_count())\n",
    "else:\n",
    "    dtype = torch.FloatTensor\n",
    "    \n",
    "from include import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.MNIST(root = './data', train = True,\n",
    "                        transform = transforms.ToTensor(), download = True)\n",
    "\n",
    "test_data = datasets.MNIST(root = './data', train = False,\n",
    "                       transform = transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get training and test data loader for permuated MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PermutedMNIST(datasets.MNIST):\n",
    "\n",
    "    def __init__(self, root=\"~/.torch/data/mnist\", train=True, permute_idx=None):\n",
    "        super(PermutedMNIST, self).__init__(root, train, download=True)\n",
    "        assert len(permute_idx) == 28 * 28\n",
    "        self.data = torch.stack([img.float().view(-1)[permute_idx] / 255 for img in self.data])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, target = self.data[index], self.targets[index]\n",
    "        return img, target\n",
    "\n",
    "    def get_sample(self, sample_size):\n",
    "        sample_idx = random.sample(range(len(self)), sample_size)\n",
    "        return [img for img in self.data[sample_idx]]\n",
    "    \n",
    "###\n",
    "\n",
    "def get_permute_mnist(num_tasks=5):\n",
    "    train_loader = {}\n",
    "    test_loader = {}\n",
    "    idx = list(range(28 * 28))\n",
    "    for i in range(num_tasks):\n",
    "        print(i)\n",
    "        #train_loader[i] = torch.utils.data.DataLoader(PermutedMNIST(train=True, permute_idx=idx),\n",
    "        #                                              batch_size=batch_size,\n",
    "        #                                              num_workers=4)\n",
    "        train_loader[i] = torch.utils.data.DataLoader(PermutedMNIST(train=True, permute_idx=idx),\n",
    "                                                      batch_size=batch_size)\n",
    "        \n",
    "        test_loader[i] = torch.utils.data.DataLoader(PermutedMNIST(train=False, permute_idx=idx),\n",
    "                                                     batch_size=batch_size)\n",
    "        random.shuffle(idx)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to evaluate the EWC variants, and training on all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,train_gen,ewc,num_epochs=20):\n",
    "    optimizer = torch.optim.Adam( model.parameters(), lr=lr)\n",
    "    #optimizer = torch.optim.SGD( model.parameters(), lr=lr)    \n",
    "    for epoch in range(num_epochs):\n",
    "        for i ,(images,labels) in enumerate(train_gen):\n",
    "            images = Variable(images).cuda()\n",
    "            labels = Variable(labels).cuda()\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            \n",
    "            loss_function = nn.MSELoss()\n",
    "            labels = torch.nn.functional.one_hot(labels).to(torch.float32)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            #if i==0 and epoch==0:\n",
    "            #    print(\"std:\",loss.data)\n",
    "            loss += ewc.loss(model)\n",
    "            #if i==0 and epoch==0:\n",
    "            #    print(\"ewc:\",loss.data)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            if i == 0: #and epoch == num_epochs - 1:\n",
    "                print('Epoch [%d/%d], Loss: %.4f' %(epoch+1, num_epochs, loss.data.item()))\n",
    "\n",
    "# train over multiple data loaders, used to evaluate performance for training on all sets\n",
    "def train_model_over_sets(model,train_gens,num_epochs=20,shuffle=True):\n",
    "    optimizer = torch.optim.Adam( model.parameters(), lr=lr)\n",
    "    #optimizer = torch.optim.SGD( model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        if shuffle:\n",
    "            perm = np.random.permutation(len(train_gens))\n",
    "        else:\n",
    "            perm = [i for i in range( len(train_gens) )]\n",
    "        for j in perm:\n",
    "            train_gen = train_gens[j]\n",
    "            for i ,(images,labels) in enumerate(train_gen):\n",
    "                images = Variable(images).cuda()\n",
    "                labels = Variable(labels).cuda()\n",
    "    \n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(images)\n",
    "                \n",
    "                loss_function = nn.MSELoss()\n",
    "                labels = torch.nn.functional.one_hot(labels).to(torch.float32)\n",
    "                loss = loss_function(outputs, labels)\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    \n",
    "                if i == 0:\n",
    "                    print('Epoch [%d/%d], Loss: %.4f' %(epoch+1, num_epochs, loss.data.item()))\n",
    "                            \n",
    "def test_model(model,test_gen):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images,labels in test_gen:\n",
    "        images = Variable(images).cuda()\n",
    "        labels = labels.cuda()\n",
    "  \n",
    "        output = model(images)\n",
    "        _, predicted = torch.max(output,1)\n",
    "        correct += (predicted == labels).sum()\n",
    "        total += labels.size(0)\n",
    "    #return ((100.0*correct)/(total+1))\n",
    "    return ((100.0*correct)/(total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation(net,train_loader,test_loader,ewc,num_epochs=20):\n",
    "    #net = Net(input_size, hidden_size, num_classes).cuda()\n",
    "    num_tasks = len(train_loader)\n",
    "    res = np.zeros(num_tasks)\n",
    "    for k in range(num_tasks): \n",
    "        \n",
    "        # train model\n",
    "        train_model(net,train_loader[k],ewc,num_epochs=num_epochs)\n",
    "        # compute Hessian before updating\n",
    "        if ewc.lam>0 and k < num_tasks-1:\n",
    "            #ewc.compute_data( net,train_loader[k] )\n",
    "            ewc.update( net,train_loader[k] )\n",
    "\n",
    "        inderrors = []\n",
    "        for i in range(k+1):\n",
    "            erri = test_model(net,test_loader[i])\n",
    "            inderrors += [erri]\n",
    "            res[k] += erri / (k+1)\n",
    "        print(\"test performance : \", res)\n",
    "        print(\"individual errors: \", inderrors)\n",
    "    return res\n",
    "\n",
    "def train_on_all(net,train_loader,test_loader,num_epochs=20):\n",
    "    #net = Net(input_size, hidden_size, num_classes).cuda()\n",
    "    num_tasks = len(train_loader)\n",
    "    res = np.zeros(num_tasks)\n",
    "    for k in range(num_tasks): \n",
    "        train_model_over_sets(net,[train_loader[j] for j in range(k+1)],num_epochs=20)\n",
    "        for i in range(k+1):\n",
    "            res[k] += test_model(net,test_loader[i]) / (k+1)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random feature model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size=784, hidden_size=500, num_classes=10):\n",
    "        super(Net,self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "  \n",
    "    def forward(self,x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "def get_random_feature_model(input_size = 784,hidden_size = 6*784,num_classes = 10):\n",
    "    net = Net(input_size, hidden_size, num_classes).cuda()\n",
    "    for param in net.fc1.parameters():\n",
    "        param.requires_grad = False\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tasks = 10\n",
    "#num_classes = 10 # number of output classes discrete range [0,9]\n",
    "num_epochs = 80 # number of times which the entire dataset is passed throughout the model\n",
    "batch_size = 100 # the size of input data took for one iteration\n",
    "lr = 1e-4 # size of step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "train_loader, test_loader = get_permute_mnist(num_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EWC++  100 0.0015\n",
      "Epoch [1/80], Loss: 0.1014\n",
      "Epoch [2/80], Loss: 0.0255\n",
      "Epoch [3/80], Loss: 0.0201\n",
      "Epoch [4/80], Loss: 0.0178\n",
      "Epoch [5/80], Loss: 0.0165\n",
      "Epoch [6/80], Loss: 0.0157\n",
      "Epoch [7/80], Loss: 0.0151\n",
      "Epoch [8/80], Loss: 0.0146\n",
      "Epoch [9/80], Loss: 0.0143\n",
      "Epoch [10/80], Loss: 0.0140\n",
      "Epoch [11/80], Loss: 0.0137\n",
      "Epoch [12/80], Loss: 0.0135\n",
      "Epoch [13/80], Loss: 0.0133\n",
      "Epoch [14/80], Loss: 0.0131\n",
      "Epoch [15/80], Loss: 0.0129\n",
      "Epoch [16/80], Loss: 0.0128\n",
      "Epoch [17/80], Loss: 0.0127\n",
      "Epoch [18/80], Loss: 0.0125\n",
      "Epoch [19/80], Loss: 0.0124\n",
      "Epoch [20/80], Loss: 0.0123\n",
      "Epoch [21/80], Loss: 0.0122\n",
      "Epoch [22/80], Loss: 0.0122\n",
      "Epoch [23/80], Loss: 0.0121\n",
      "Epoch [24/80], Loss: 0.0120\n",
      "Epoch [25/80], Loss: 0.0119\n",
      "Epoch [26/80], Loss: 0.0119\n",
      "Epoch [27/80], Loss: 0.0118\n",
      "Epoch [28/80], Loss: 0.0118\n",
      "Epoch [29/80], Loss: 0.0117\n",
      "Epoch [30/80], Loss: 0.0117\n",
      "Epoch [31/80], Loss: 0.0116\n",
      "Epoch [32/80], Loss: 0.0116\n",
      "Epoch [33/80], Loss: 0.0115\n",
      "Epoch [34/80], Loss: 0.0115\n",
      "Epoch [35/80], Loss: 0.0115\n",
      "Epoch [36/80], Loss: 0.0114\n",
      "Epoch [37/80], Loss: 0.0114\n",
      "Epoch [38/80], Loss: 0.0114\n",
      "Epoch [39/80], Loss: 0.0113\n",
      "Epoch [40/80], Loss: 0.0113\n",
      "Epoch [41/80], Loss: 0.0113\n",
      "Epoch [42/80], Loss: 0.0112\n",
      "Epoch [43/80], Loss: 0.0112\n",
      "Epoch [44/80], Loss: 0.0112\n",
      "Epoch [45/80], Loss: 0.0112\n",
      "Epoch [46/80], Loss: 0.0111\n",
      "Epoch [47/80], Loss: 0.0111\n",
      "Epoch [48/80], Loss: 0.0111\n",
      "Epoch [49/80], Loss: 0.0111\n",
      "Epoch [50/80], Loss: 0.0111\n",
      "Epoch [51/80], Loss: 0.0111\n",
      "Epoch [52/80], Loss: 0.0110\n",
      "Epoch [53/80], Loss: 0.0110\n",
      "Epoch [54/80], Loss: 0.0110\n",
      "Epoch [55/80], Loss: 0.0110\n",
      "Epoch [56/80], Loss: 0.0110\n",
      "Epoch [57/80], Loss: 0.0110\n",
      "Epoch [58/80], Loss: 0.0109\n",
      "Epoch [59/80], Loss: 0.0109\n",
      "Epoch [60/80], Loss: 0.0109\n",
      "Epoch [61/80], Loss: 0.0109\n",
      "Epoch [62/80], Loss: 0.0109\n",
      "Epoch [63/80], Loss: 0.0109\n",
      "Epoch [64/80], Loss: 0.0109\n",
      "Epoch [65/80], Loss: 0.0109\n",
      "Epoch [66/80], Loss: 0.0109\n",
      "Epoch [67/80], Loss: 0.0108\n",
      "Epoch [68/80], Loss: 0.0108\n",
      "Epoch [69/80], Loss: 0.0108\n",
      "Epoch [70/80], Loss: 0.0108\n",
      "Epoch [71/80], Loss: 0.0108\n",
      "Epoch [72/80], Loss: 0.0108\n",
      "Epoch [73/80], Loss: 0.0108\n",
      "Epoch [74/80], Loss: 0.0108\n",
      "Epoch [75/80], Loss: 0.0108\n",
      "Epoch [76/80], Loss: 0.0108\n",
      "Epoch [77/80], Loss: 0.0108\n",
      "Epoch [78/80], Loss: 0.0107\n",
      "Epoch [79/80], Loss: 0.0107\n",
      "Epoch [80/80], Loss: 0.0107\n",
      "generate task data..\n",
      "task data norm and number entries: tensor(7502.4375, device='cuda:0') torch.Size([100, 47050])\n",
      "..done\n",
      "test performance :  [97.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "individual errors:  [tensor(97., device='cuda:0')]\n",
      "Epoch [1/80], Loss: 0.1725\n",
      "Epoch [2/80], Loss: 0.0805\n",
      "Epoch [3/80], Loss: 0.0587\n",
      "Epoch [4/80], Loss: 0.0511\n",
      "Epoch [5/80], Loss: 0.0472\n",
      "Epoch [6/80], Loss: 0.0448\n",
      "Epoch [7/80], Loss: 0.0427\n",
      "Epoch [8/80], Loss: 0.0405\n",
      "Epoch [9/80], Loss: 0.0450\n",
      "Epoch [10/80], Loss: 0.0381\n",
      "Epoch [11/80], Loss: 0.0368\n",
      "Epoch [12/80], Loss: 0.0372\n",
      "Epoch [13/80], Loss: 0.0353\n",
      "Epoch [14/80], Loss: 0.0347\n",
      "Epoch [15/80], Loss: 0.0349\n",
      "Epoch [16/80], Loss: 0.0334\n",
      "Epoch [17/80], Loss: 0.0338\n",
      "Epoch [18/80], Loss: 0.0318\n",
      "Epoch [19/80], Loss: 0.0316\n",
      "Epoch [20/80], Loss: 0.0306\n",
      "Epoch [21/80], Loss: 0.0302\n",
      "Epoch [22/80], Loss: 0.0321\n",
      "Epoch [23/80], Loss: 0.0297\n",
      "Epoch [24/80], Loss: 0.0309\n",
      "Epoch [25/80], Loss: 0.0324\n",
      "Epoch [26/80], Loss: 0.0287\n",
      "Epoch [27/80], Loss: 0.0287\n",
      "Epoch [28/80], Loss: 0.0301\n",
      "Epoch [29/80], Loss: 0.0286\n",
      "Epoch [30/80], Loss: 0.0270\n",
      "Epoch [31/80], Loss: 0.0272\n",
      "Epoch [32/80], Loss: 0.0278\n",
      "Epoch [33/80], Loss: 0.0317\n",
      "Epoch [34/80], Loss: 0.0265\n",
      "Epoch [35/80], Loss: 0.0289\n",
      "Epoch [36/80], Loss: 0.0261\n",
      "Epoch [37/80], Loss: 0.0259\n",
      "Epoch [38/80], Loss: 0.0270\n",
      "Epoch [39/80], Loss: 0.0253\n",
      "Epoch [40/80], Loss: 0.0254\n",
      "Epoch [41/80], Loss: 0.0250\n",
      "Epoch [42/80], Loss: 0.0249\n",
      "Epoch [43/80], Loss: 0.0248\n",
      "Epoch [44/80], Loss: 0.0254\n",
      "Epoch [45/80], Loss: 0.0286\n",
      "Epoch [46/80], Loss: 0.0244\n",
      "Epoch [47/80], Loss: 0.0246\n",
      "Epoch [48/80], Loss: 0.0237\n",
      "Epoch [49/80], Loss: 0.0295\n",
      "Epoch [50/80], Loss: 0.0249\n",
      "Epoch [51/80], Loss: 0.0238\n",
      "Epoch [52/80], Loss: 0.0247\n",
      "Epoch [53/80], Loss: 0.0253\n",
      "Epoch [54/80], Loss: 0.0232\n",
      "Epoch [55/80], Loss: 0.0232\n",
      "Epoch [56/80], Loss: 0.0237\n",
      "Epoch [57/80], Loss: 0.0234\n",
      "Epoch [58/80], Loss: 0.0226\n",
      "Epoch [59/80], Loss: 0.0228\n",
      "Epoch [60/80], Loss: 0.0229\n",
      "Epoch [61/80], Loss: 0.0247\n",
      "Epoch [62/80], Loss: 0.0231\n",
      "Epoch [63/80], Loss: 0.0238\n",
      "Epoch [64/80], Loss: 0.0242\n",
      "Epoch [65/80], Loss: 0.0223\n",
      "Epoch [66/80], Loss: 0.0238\n",
      "Epoch [67/80], Loss: 0.0228\n",
      "Epoch [68/80], Loss: 0.0221\n",
      "Epoch [69/80], Loss: 0.0241\n",
      "Epoch [70/80], Loss: 0.0219\n",
      "Epoch [71/80], Loss: 0.0239\n",
      "Epoch [72/80], Loss: 0.0222\n",
      "Epoch [73/80], Loss: 0.0214\n",
      "Epoch [74/80], Loss: 0.0254\n",
      "Epoch [75/80], Loss: 0.0215\n",
      "Epoch [76/80], Loss: 0.0212\n",
      "Epoch [77/80], Loss: 0.0211\n",
      "Epoch [78/80], Loss: 0.0314\n",
      "Epoch [79/80], Loss: 0.0220\n",
      "Epoch [80/80], Loss: 0.0215\n",
      "update data..\n",
      "task data norm and number entries: tensor(7373.0186, device='cuda:0') torch.Size([100, 47050])\n",
      "..done\n",
      "test performance :  [97.         94.83999634  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "individual errors:  [tensor(96.0300, device='cuda:0'), tensor(93.6500, device='cuda:0')]\n",
      "Epoch [1/80], Loss: 0.1338\n",
      "Epoch [2/80], Loss: 0.0912\n",
      "Epoch [3/80], Loss: 0.0709\n",
      "Epoch [4/80], Loss: 0.0614\n",
      "Epoch [5/80], Loss: 0.0559\n",
      "Epoch [6/80], Loss: 0.0526\n",
      "Epoch [7/80], Loss: 0.0501\n",
      "Epoch [8/80], Loss: 0.0482\n",
      "Epoch [9/80], Loss: 0.0469\n",
      "Epoch [10/80], Loss: 0.0477\n",
      "Epoch [11/80], Loss: 0.0453\n",
      "Epoch [12/80], Loss: 0.0438\n",
      "Epoch [13/80], Loss: 0.0431\n",
      "Epoch [14/80], Loss: 0.0415\n",
      "Epoch [15/80], Loss: 0.0409\n",
      "Epoch [16/80], Loss: 0.0406\n",
      "Epoch [17/80], Loss: 0.0396\n",
      "Epoch [18/80], Loss: 0.0389\n",
      "Epoch [19/80], Loss: 0.0383\n",
      "Epoch [20/80], Loss: 0.0378\n",
      "Epoch [21/80], Loss: 0.0374\n",
      "Epoch [22/80], Loss: 0.0369\n",
      "Epoch [23/80], Loss: 0.0368\n",
      "Epoch [24/80], Loss: 0.0361\n",
      "Epoch [25/80], Loss: 0.0357\n",
      "Epoch [26/80], Loss: 0.0353\n",
      "Epoch [27/80], Loss: 0.0400\n",
      "Epoch [28/80], Loss: 0.0345\n",
      "Epoch [29/80], Loss: 0.0342\n",
      "Epoch [30/80], Loss: 0.0343\n",
      "Epoch [31/80], Loss: 0.0337\n",
      "Epoch [32/80], Loss: 0.0341\n",
      "Epoch [33/80], Loss: 0.0433\n",
      "Epoch [34/80], Loss: 0.0350\n",
      "Epoch [35/80], Loss: 0.0326\n",
      "Epoch [36/80], Loss: 0.0322\n",
      "Epoch [37/80], Loss: 0.0424\n",
      "Epoch [38/80], Loss: 0.0326\n",
      "Epoch [39/80], Loss: 0.0330\n",
      "Epoch [40/80], Loss: 0.0338\n",
      "Epoch [41/80], Loss: 0.0312\n",
      "Epoch [42/80], Loss: 0.0315\n",
      "Epoch [43/80], Loss: 0.0307\n",
      "Epoch [44/80], Loss: 0.0326\n",
      "Epoch [45/80], Loss: 0.0303\n",
      "Epoch [46/80], Loss: 0.0325\n",
      "Epoch [47/80], Loss: 0.0300\n",
      "Epoch [48/80], Loss: 0.0298\n",
      "Epoch [49/80], Loss: 0.0298\n",
      "Epoch [50/80], Loss: 0.0294\n",
      "Epoch [51/80], Loss: 0.0294\n",
      "Epoch [52/80], Loss: 0.0294\n",
      "Epoch [53/80], Loss: 0.0291\n",
      "Epoch [54/80], Loss: 0.0294\n",
      "Epoch [55/80], Loss: 0.0287\n",
      "Epoch [56/80], Loss: 0.0351\n",
      "Epoch [57/80], Loss: 0.0285\n",
      "Epoch [58/80], Loss: 0.0405\n",
      "Epoch [59/80], Loss: 0.0298\n",
      "Epoch [60/80], Loss: 0.0283\n",
      "Epoch [61/80], Loss: 0.0279\n",
      "Epoch [62/80], Loss: 0.0286\n",
      "Epoch [63/80], Loss: 0.0278\n",
      "Epoch [64/80], Loss: 0.0304\n",
      "Epoch [65/80], Loss: 0.0276\n",
      "Epoch [66/80], Loss: 0.0286\n",
      "Epoch [67/80], Loss: 0.0344\n",
      "Epoch [68/80], Loss: 0.0411\n",
      "Epoch [69/80], Loss: 0.0271\n",
      "Epoch [70/80], Loss: 0.0279\n",
      "Epoch [71/80], Loss: 0.0268\n",
      "Epoch [72/80], Loss: 0.0273\n",
      "Epoch [73/80], Loss: 0.0266\n",
      "Epoch [74/80], Loss: 0.0270\n",
      "Epoch [75/80], Loss: 0.0305\n",
      "Epoch [76/80], Loss: 0.0276\n",
      "Epoch [77/80], Loss: 0.0262\n",
      "Epoch [78/80], Loss: 0.0262\n",
      "Epoch [79/80], Loss: 0.0260\n",
      "Epoch [80/80], Loss: 0.0262\n",
      "update data..\n",
      "task data norm and number entries: tensor(7357.7593, device='cuda:0') torch.Size([100, 47050])\n",
      "..done\n",
      "test performance :  [97.         94.83999634 93.11000061  0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "individual errors:  [tensor(95.3400, device='cuda:0'), tensor(92.2800, device='cuda:0'), tensor(91.7100, device='cuda:0')]\n",
      "Epoch [1/80], Loss: 0.1396\n",
      "Epoch [2/80], Loss: 0.0984\n",
      "Epoch [3/80], Loss: 0.0896\n",
      "Epoch [4/80], Loss: 0.0698\n",
      "Epoch [5/80], Loss: 0.0630\n",
      "Epoch [6/80], Loss: 0.0588\n",
      "Epoch [7/80], Loss: 0.0555\n",
      "Epoch [8/80], Loss: 0.0545\n",
      "Epoch [9/80], Loss: 0.0538\n",
      "Epoch [10/80], Loss: 0.0493\n",
      "Epoch [11/80], Loss: 0.0486\n",
      "Epoch [12/80], Loss: 0.0475\n",
      "Epoch [13/80], Loss: 0.0456\n",
      "Epoch [14/80], Loss: 0.0447\n",
      "Epoch [15/80], Loss: 0.0438\n",
      "Epoch [16/80], Loss: 0.0434\n",
      "Epoch [17/80], Loss: 0.0426\n",
      "Epoch [18/80], Loss: 0.0418\n",
      "Epoch [19/80], Loss: 0.0418\n",
      "Epoch [20/80], Loss: 0.0408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/80], Loss: 0.0411\n",
      "Epoch [22/80], Loss: 0.0398\n",
      "Epoch [23/80], Loss: 0.0400\n",
      "Epoch [24/80], Loss: 0.0389\n",
      "Epoch [25/80], Loss: 0.0405\n",
      "Epoch [26/80], Loss: 0.0433\n",
      "Epoch [27/80], Loss: 0.0484\n",
      "Epoch [28/80], Loss: 0.0396\n",
      "Epoch [29/80], Loss: 0.0453\n",
      "Epoch [30/80], Loss: 0.0428\n",
      "Epoch [31/80], Loss: 0.0377\n",
      "Epoch [32/80], Loss: 0.0376\n",
      "Epoch [33/80], Loss: 0.0360\n",
      "Epoch [34/80], Loss: 0.0444\n",
      "Epoch [35/80], Loss: 0.0353\n",
      "Epoch [36/80], Loss: 0.0375\n",
      "Epoch [37/80], Loss: 0.0350\n",
      "Epoch [38/80], Loss: 0.0349\n",
      "Epoch [39/80], Loss: 0.0343\n",
      "Epoch [40/80], Loss: 0.0348\n",
      "Epoch [41/80], Loss: 0.0339\n",
      "Epoch [42/80], Loss: 0.0353\n",
      "Epoch [43/80], Loss: 0.0337\n",
      "Epoch [44/80], Loss: 0.0396\n",
      "Epoch [45/80], Loss: 0.0335\n",
      "Epoch [46/80], Loss: 0.0332\n",
      "Epoch [47/80], Loss: 0.0349\n",
      "Epoch [48/80], Loss: 0.0365\n",
      "Epoch [49/80], Loss: 0.0328\n",
      "Epoch [50/80], Loss: 0.0348\n",
      "Epoch [51/80], Loss: 0.0363\n",
      "Epoch [52/80], Loss: 0.0318\n",
      "Epoch [53/80], Loss: 0.0317\n",
      "Epoch [54/80], Loss: 0.0351\n",
      "Epoch [55/80], Loss: 0.0454\n",
      "Epoch [56/80], Loss: 0.0312\n",
      "Epoch [57/80], Loss: 0.0311\n",
      "Epoch [58/80], Loss: 0.0309\n",
      "Epoch [59/80], Loss: 0.0308\n",
      "Epoch [60/80], Loss: 0.0319\n",
      "Epoch [61/80], Loss: 0.0305\n",
      "Epoch [62/80], Loss: 0.0306\n",
      "Epoch [63/80], Loss: 0.0302\n",
      "Epoch [64/80], Loss: 0.0300\n",
      "Epoch [65/80], Loss: 0.0305\n",
      "Epoch [66/80], Loss: 0.0322\n",
      "Epoch [67/80], Loss: 0.0341\n",
      "Epoch [68/80], Loss: 0.0296\n",
      "Epoch [69/80], Loss: 0.0301\n",
      "Epoch [70/80], Loss: 0.0308\n",
      "Epoch [71/80], Loss: 0.0292\n",
      "Epoch [72/80], Loss: 0.0295\n",
      "Epoch [73/80], Loss: 0.0331\n",
      "Epoch [74/80], Loss: 0.0289\n",
      "Epoch [75/80], Loss: 0.0288\n",
      "Epoch [76/80], Loss: 0.0287\n",
      "Epoch [77/80], Loss: 0.0321\n",
      "Epoch [78/80], Loss: 0.0299\n",
      "Epoch [79/80], Loss: 0.0284\n",
      "Epoch [80/80], Loss: 0.0300\n",
      "update data..\n",
      "task data norm and number entries: tensor(7329.7822, device='cuda:0') torch.Size([100, 47050])\n",
      "..done\n",
      "test performance :  [97.         94.83999634 93.11000061 91.30750275  0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "individual errors:  [tensor(94.0300, device='cuda:0'), tensor(90.8500, device='cuda:0'), tensor(90.2800, device='cuda:0'), tensor(90.0700, device='cuda:0')]\n",
      "Epoch [1/80], Loss: 0.1263\n",
      "Epoch [2/80], Loss: 0.0963\n",
      "Epoch [3/80], Loss: 0.0804\n",
      "Epoch [4/80], Loss: 0.0929\n",
      "Epoch [5/80], Loss: 0.0642\n",
      "Epoch [6/80], Loss: 0.0647\n",
      "Epoch [7/80], Loss: 0.0776\n",
      "Epoch [8/80], Loss: 0.0538\n",
      "Epoch [9/80], Loss: 0.0526\n",
      "Epoch [10/80], Loss: 0.0502\n",
      "Epoch [11/80], Loss: 0.0507\n",
      "Epoch [12/80], Loss: 0.0540\n",
      "Epoch [13/80], Loss: 0.0474\n",
      "Epoch [14/80], Loss: 0.0459\n",
      "Epoch [15/80], Loss: 0.0456\n",
      "Epoch [16/80], Loss: 0.0438\n",
      "Epoch [17/80], Loss: 0.0432\n",
      "Epoch [18/80], Loss: 0.0502\n",
      "Epoch [19/80], Loss: 0.0433\n",
      "Epoch [20/80], Loss: 0.0421\n",
      "Epoch [21/80], Loss: 0.0415\n",
      "Epoch [22/80], Loss: 0.0407\n",
      "Epoch [23/80], Loss: 0.0401\n",
      "Epoch [24/80], Loss: 0.0414\n",
      "Epoch [25/80], Loss: 0.0396\n",
      "Epoch [26/80], Loss: 0.0483\n",
      "Epoch [27/80], Loss: 0.0386\n",
      "Epoch [28/80], Loss: 0.0475\n",
      "Epoch [29/80], Loss: 0.0451\n",
      "Epoch [30/80], Loss: 0.0434\n",
      "Epoch [31/80], Loss: 0.0372\n",
      "Epoch [32/80], Loss: 0.0371\n",
      "Epoch [33/80], Loss: 0.0407\n",
      "Epoch [34/80], Loss: 0.0653\n",
      "Epoch [35/80], Loss: 0.0384\n",
      "Epoch [36/80], Loss: 0.0361\n",
      "Epoch [37/80], Loss: 0.0374\n",
      "Epoch [38/80], Loss: 0.0368\n",
      "Epoch [39/80], Loss: 0.0364\n",
      "Epoch [40/80], Loss: 0.0394\n",
      "Epoch [41/80], Loss: 0.0473\n",
      "Epoch [42/80], Loss: 0.0347\n",
      "Epoch [43/80], Loss: 0.0427\n",
      "Epoch [44/80], Loss: 0.0342\n",
      "Epoch [45/80], Loss: 0.0349\n",
      "Epoch [46/80], Loss: 0.0368\n",
      "Epoch [47/80], Loss: 0.0736\n",
      "Epoch [48/80], Loss: 0.0337\n",
      "Epoch [49/80], Loss: 0.0344\n",
      "Epoch [50/80], Loss: 0.0329\n",
      "Epoch [51/80], Loss: 0.0346\n",
      "Epoch [52/80], Loss: 0.0352\n",
      "Epoch [53/80], Loss: 0.0326\n",
      "Epoch [54/80], Loss: 0.0323\n",
      "Epoch [55/80], Loss: 0.0335\n",
      "Epoch [56/80], Loss: 0.0330\n",
      "Epoch [57/80], Loss: 0.0374\n",
      "Epoch [58/80], Loss: 0.0326\n",
      "Epoch [59/80], Loss: 0.0335\n",
      "Epoch [60/80], Loss: 0.0337\n",
      "Epoch [61/80], Loss: 0.0327\n",
      "Epoch [62/80], Loss: 0.0321\n",
      "Epoch [63/80], Loss: 0.0313\n",
      "Epoch [64/80], Loss: 0.0450\n",
      "Epoch [65/80], Loss: 0.0310\n",
      "Epoch [66/80], Loss: 0.0369\n",
      "Epoch [67/80], Loss: 0.0370\n",
      "Epoch [68/80], Loss: 0.0303\n",
      "Epoch [69/80], Loss: 0.0302\n",
      "Epoch [70/80], Loss: 0.0313\n",
      "Epoch [71/80], Loss: 0.0623\n",
      "Epoch [72/80], Loss: 0.0811\n",
      "Epoch [73/80], Loss: 0.0312\n",
      "Epoch [74/80], Loss: 0.0380\n",
      "Epoch [75/80], Loss: 0.0310\n",
      "Epoch [76/80], Loss: 0.0482\n",
      "Epoch [77/80], Loss: 0.0320\n",
      "Epoch [78/80], Loss: 0.0350\n",
      "Epoch [79/80], Loss: 0.0293\n",
      "Epoch [80/80], Loss: 0.0293\n",
      "update data..\n",
      "task data norm and number entries: tensor(7355.1304, device='cuda:0') torch.Size([100, 47050])\n",
      "..done\n",
      "test performance :  [97.         94.83999634 93.11000061 91.30750275 89.83599854  0.\n",
      "  0.          0.          0.          0.        ]\n",
      "individual errors:  [tensor(93.2200, device='cuda:0'), tensor(89.0700, device='cuda:0'), tensor(88.6300, device='cuda:0'), tensor(88.4900, device='cuda:0'), tensor(89.7700, device='cuda:0')]\n",
      "Epoch [1/80], Loss: 0.1182\n",
      "Epoch [2/80], Loss: 0.0981\n",
      "Epoch [3/80], Loss: 0.0840\n",
      "Epoch [4/80], Loss: 0.0952\n",
      "Epoch [5/80], Loss: 0.0694\n",
      "Epoch [6/80], Loss: 0.0678\n",
      "Epoch [7/80], Loss: 0.0617\n",
      "Epoch [8/80], Loss: 0.0592\n",
      "Epoch [9/80], Loss: 0.0674\n",
      "Epoch [10/80], Loss: 0.0558\n",
      "Epoch [11/80], Loss: 0.0552\n",
      "Epoch [12/80], Loss: 0.0550\n",
      "Epoch [13/80], Loss: 0.0528\n",
      "Epoch [14/80], Loss: 0.0522\n",
      "Epoch [15/80], Loss: 0.0509\n",
      "Epoch [16/80], Loss: 0.0500\n",
      "Epoch [17/80], Loss: 0.0487\n",
      "Epoch [18/80], Loss: 0.0479\n",
      "Epoch [19/80], Loss: 0.0535\n",
      "Epoch [20/80], Loss: 0.0502\n",
      "Epoch [21/80], Loss: 0.0489\n",
      "Epoch [22/80], Loss: 0.0461\n",
      "Epoch [23/80], Loss: 0.0836\n",
      "Epoch [24/80], Loss: 0.0530\n",
      "Epoch [25/80], Loss: 0.0473\n",
      "Epoch [26/80], Loss: 0.0447\n",
      "Epoch [27/80], Loss: 0.0740\n",
      "Epoch [28/80], Loss: 0.0455\n",
      "Epoch [29/80], Loss: 0.0438\n",
      "Epoch [30/80], Loss: 0.0435\n",
      "Epoch [31/80], Loss: 0.0435\n",
      "Epoch [32/80], Loss: 0.0439\n",
      "Epoch [33/80], Loss: 0.0453\n",
      "Epoch [34/80], Loss: 0.0416\n",
      "Epoch [35/80], Loss: 0.0476\n",
      "Epoch [36/80], Loss: 0.0419\n",
      "Epoch [37/80], Loss: 0.0452\n",
      "Epoch [38/80], Loss: 0.0425\n",
      "Epoch [39/80], Loss: 0.0454\n",
      "Epoch [40/80], Loss: 0.0521\n",
      "Epoch [41/80], Loss: 0.0419\n",
      "Epoch [42/80], Loss: 0.0778\n",
      "Epoch [43/80], Loss: 0.0609\n",
      "Epoch [44/80], Loss: 0.0444\n",
      "Epoch [45/80], Loss: 0.0408\n",
      "Epoch [46/80], Loss: 0.0557\n",
      "Epoch [47/80], Loss: 0.0443\n",
      "Epoch [48/80], Loss: 0.0388\n",
      "Epoch [49/80], Loss: 0.0419\n",
      "Epoch [50/80], Loss: 0.0584\n",
      "Epoch [51/80], Loss: 0.0379\n",
      "Epoch [52/80], Loss: 0.0585\n",
      "Epoch [53/80], Loss: 0.0904\n",
      "Epoch [54/80], Loss: 0.0417\n",
      "Epoch [55/80], Loss: 0.0485\n",
      "Epoch [56/80], Loss: 0.0521\n",
      "Epoch [57/80], Loss: 0.0442\n",
      "Epoch [58/80], Loss: 0.0370\n",
      "Epoch [59/80], Loss: 0.0366\n",
      "Epoch [60/80], Loss: 0.0367\n",
      "Epoch [61/80], Loss: 0.0474\n",
      "Epoch [62/80], Loss: 0.0362\n",
      "Epoch [63/80], Loss: 0.0402\n",
      "Epoch [64/80], Loss: 0.0363\n",
      "Epoch [65/80], Loss: 0.0474\n",
      "Epoch [66/80], Loss: 0.0364\n",
      "Epoch [67/80], Loss: 0.0360\n",
      "Epoch [68/80], Loss: 0.0483\n",
      "Epoch [69/80], Loss: 0.0611\n",
      "Epoch [70/80], Loss: 0.0363\n",
      "Epoch [71/80], Loss: 0.0350\n",
      "Epoch [72/80], Loss: 0.0459\n",
      "Epoch [73/80], Loss: 0.0440\n",
      "Epoch [74/80], Loss: 0.0633\n",
      "Epoch [75/80], Loss: 0.0353\n",
      "Epoch [76/80], Loss: 0.0397\n",
      "Epoch [77/80], Loss: 0.0349\n",
      "Epoch [78/80], Loss: 0.0356\n",
      "Epoch [79/80], Loss: 0.0501\n",
      "Epoch [80/80], Loss: 0.0338\n",
      "update data..\n",
      "task data norm and number entries: tensor(7362.4907, device='cuda:0') torch.Size([100, 47050])\n",
      "..done\n",
      "test performance :  [97.         94.83999634 93.11000061 91.30750275 89.83599854 88.45666504\n",
      "  0.          0.          0.          0.        ]\n",
      "individual errors:  [tensor(93.0500, device='cuda:0'), tensor(86.2900, device='cuda:0'), tensor(87.7100, device='cuda:0'), tensor(87.0500, device='cuda:0'), tensor(88.1900, device='cuda:0'), tensor(88.4500, device='cuda:0')]\n",
      "Epoch [1/80], Loss: 0.1276\n",
      "Epoch [2/80], Loss: 0.1083\n",
      "Epoch [3/80], Loss: 0.0921\n",
      "Epoch [4/80], Loss: 0.0931\n",
      "Epoch [5/80], Loss: 0.0771\n",
      "Epoch [6/80], Loss: 0.0674\n",
      "Epoch [7/80], Loss: 0.0701\n",
      "Epoch [8/80], Loss: 0.0605\n",
      "Epoch [9/80], Loss: 0.0587\n",
      "Epoch [10/80], Loss: 0.0563\n",
      "Epoch [11/80], Loss: 0.0672\n",
      "Epoch [12/80], Loss: 0.0694\n",
      "Epoch [13/80], Loss: 0.1136\n",
      "Epoch [14/80], Loss: 0.0630\n",
      "Epoch [15/80], Loss: 0.0828\n",
      "Epoch [16/80], Loss: 0.0496\n",
      "Epoch [17/80], Loss: 0.0552\n",
      "Epoch [18/80], Loss: 0.0729\n",
      "Epoch [19/80], Loss: 0.0526\n",
      "Epoch [20/80], Loss: 0.0472\n",
      "Epoch [21/80], Loss: 0.0477\n",
      "Epoch [22/80], Loss: 0.0497\n",
      "Epoch [23/80], Loss: 0.0958\n",
      "Epoch [24/80], Loss: 0.0450\n",
      "Epoch [25/80], Loss: 0.0466\n",
      "Epoch [26/80], Loss: 0.0598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27/80], Loss: 0.0437\n",
      "Epoch [28/80], Loss: 0.0435\n",
      "Epoch [29/80], Loss: 0.0437\n",
      "Epoch [30/80], Loss: 0.0812\n",
      "Epoch [31/80], Loss: 0.0433\n",
      "Epoch [32/80], Loss: 0.0802\n",
      "Epoch [33/80], Loss: 0.0608\n",
      "Epoch [34/80], Loss: 0.0444\n",
      "Epoch [35/80], Loss: 0.0428\n",
      "Epoch [36/80], Loss: 0.0412\n",
      "Epoch [37/80], Loss: 0.0578\n",
      "Epoch [38/80], Loss: 0.0409\n",
      "Epoch [39/80], Loss: 0.0474\n",
      "Epoch [40/80], Loss: 0.0424\n",
      "Epoch [41/80], Loss: 0.0409\n",
      "Epoch [42/80], Loss: 0.0400\n",
      "Epoch [43/80], Loss: 0.0397\n",
      "Epoch [44/80], Loss: 0.0405\n",
      "Epoch [45/80], Loss: 0.0557\n",
      "Epoch [46/80], Loss: 0.0409\n",
      "Epoch [47/80], Loss: 0.0431\n",
      "Epoch [48/80], Loss: 0.0385\n",
      "Epoch [49/80], Loss: 0.0527\n",
      "Epoch [50/80], Loss: 0.0383\n",
      "Epoch [51/80], Loss: 0.0382\n",
      "Epoch [52/80], Loss: 0.0376\n",
      "Epoch [53/80], Loss: 0.1059\n",
      "Epoch [54/80], Loss: 0.0476\n",
      "Epoch [55/80], Loss: 0.0371\n",
      "Epoch [56/80], Loss: 0.0378\n",
      "Epoch [57/80], Loss: 0.0368\n",
      "Epoch [58/80], Loss: 0.0368\n",
      "Epoch [59/80], Loss: 0.0456\n",
      "Epoch [60/80], Loss: 0.0372\n",
      "Epoch [61/80], Loss: 0.0368\n",
      "Epoch [62/80], Loss: 0.0376\n",
      "Epoch [63/80], Loss: 0.0987\n",
      "Epoch [64/80], Loss: 0.0359\n",
      "Epoch [65/80], Loss: 0.0395\n",
      "Epoch [66/80], Loss: 0.0718\n",
      "Epoch [67/80], Loss: 0.0362\n",
      "Epoch [68/80], Loss: 0.0666\n",
      "Epoch [69/80], Loss: 0.0352\n",
      "Epoch [70/80], Loss: 0.0359\n",
      "Epoch [71/80], Loss: 0.0354\n",
      "Epoch [72/80], Loss: 0.0551\n",
      "Epoch [73/80], Loss: 0.0347\n",
      "Epoch [74/80], Loss: 0.0369\n",
      "Epoch [75/80], Loss: 0.0344\n",
      "Epoch [76/80], Loss: 0.1104\n",
      "Epoch [77/80], Loss: 0.0376\n",
      "Epoch [78/80], Loss: 0.0342\n",
      "Epoch [79/80], Loss: 0.0340\n",
      "Epoch [80/80], Loss: 0.0523\n",
      "update data..\n",
      "task data norm and number entries: tensor(7257.6113, device='cuda:0') torch.Size([100, 47050])\n",
      "..done\n",
      "test performance :  [97.         94.83999634 93.11000061 91.30750275 89.83599854 88.45666504\n",
      " 87.24571991  0.          0.          0.        ]\n",
      "individual errors:  [tensor(91.5900, device='cuda:0'), tensor(85.8000, device='cuda:0'), tensor(87.0400, device='cuda:0'), tensor(85.6800, device='cuda:0'), tensor(86.7000, device='cuda:0'), tensor(86.7500, device='cuda:0'), tensor(87.1600, device='cuda:0')]\n",
      "Epoch [1/80], Loss: 0.1273\n",
      "Epoch [2/80], Loss: 0.1094\n",
      "Epoch [3/80], Loss: 0.1003\n",
      "Epoch [4/80], Loss: 0.0860\n",
      "Epoch [5/80], Loss: 0.0934\n",
      "Epoch [6/80], Loss: 0.0805\n",
      "Epoch [7/80], Loss: 0.0727\n",
      "Epoch [8/80], Loss: 0.0649\n",
      "Epoch [9/80], Loss: 0.0676\n",
      "Epoch [10/80], Loss: 0.0641\n",
      "Epoch [11/80], Loss: 0.0616\n",
      "Epoch [12/80], Loss: 0.0640\n",
      "Epoch [13/80], Loss: 0.0584\n",
      "Epoch [14/80], Loss: 0.0592\n",
      "Epoch [15/80], Loss: 0.0689\n",
      "Epoch [16/80], Loss: 0.0567\n",
      "Epoch [17/80], Loss: 0.0518\n",
      "Epoch [18/80], Loss: 0.0608\n",
      "Epoch [19/80], Loss: 0.0603\n",
      "Epoch [20/80], Loss: 0.0498\n",
      "Epoch [21/80], Loss: 0.0493\n",
      "Epoch [22/80], Loss: 0.0483\n",
      "Epoch [23/80], Loss: 0.0588\n",
      "Epoch [24/80], Loss: 0.0516\n",
      "Epoch [25/80], Loss: 0.0550\n",
      "Epoch [26/80], Loss: 0.0468\n",
      "Epoch [27/80], Loss: 0.0465\n",
      "Epoch [28/80], Loss: 0.0466\n",
      "Epoch [29/80], Loss: 0.0465\n",
      "Epoch [30/80], Loss: 0.0509\n",
      "Epoch [31/80], Loss: 0.0466\n",
      "Epoch [32/80], Loss: 0.0571\n",
      "Epoch [33/80], Loss: 0.0470\n",
      "Epoch [34/80], Loss: 0.0722\n",
      "Epoch [35/80], Loss: 0.0547\n",
      "Epoch [36/80], Loss: 0.0480\n",
      "Epoch [37/80], Loss: 0.0429\n",
      "Epoch [38/80], Loss: 0.0511\n",
      "Epoch [39/80], Loss: 0.0445\n",
      "Epoch [40/80], Loss: 0.0423\n",
      "Epoch [41/80], Loss: 0.0418\n",
      "Epoch [42/80], Loss: 0.0609\n",
      "Epoch [43/80], Loss: 0.0441\n",
      "Epoch [44/80], Loss: 0.0433\n",
      "Epoch [45/80], Loss: 0.0564\n",
      "Epoch [46/80], Loss: 0.0416\n",
      "Epoch [47/80], Loss: 0.0420\n",
      "Epoch [48/80], Loss: 0.0412\n",
      "Epoch [49/80], Loss: 0.0401\n",
      "Epoch [50/80], Loss: 0.0452\n",
      "Epoch [51/80], Loss: 0.0541\n",
      "Epoch [52/80], Loss: 0.0532\n",
      "Epoch [53/80], Loss: 0.0393\n",
      "Epoch [54/80], Loss: 0.0408\n",
      "Epoch [55/80], Loss: 0.0390\n",
      "Epoch [56/80], Loss: 0.0514\n",
      "Epoch [57/80], Loss: 0.0418\n",
      "Epoch [58/80], Loss: 0.0541\n",
      "Epoch [59/80], Loss: 0.0382\n",
      "Epoch [60/80], Loss: 0.0454\n",
      "Epoch [61/80], Loss: 0.0380\n",
      "Epoch [62/80], Loss: 0.0380\n",
      "Epoch [63/80], Loss: 0.0392\n",
      "Epoch [64/80], Loss: 0.0620\n",
      "Epoch [65/80], Loss: 0.0376\n",
      "Epoch [66/80], Loss: 0.0416\n",
      "Epoch [67/80], Loss: 0.0451\n",
      "Epoch [68/80], Loss: 0.0370\n",
      "Epoch [69/80], Loss: 0.0418\n",
      "Epoch [70/80], Loss: 0.0384\n",
      "Epoch [71/80], Loss: 0.0414\n",
      "Epoch [72/80], Loss: 0.0366\n",
      "Epoch [73/80], Loss: 0.1013\n",
      "Epoch [74/80], Loss: 0.0399\n",
      "Epoch [75/80], Loss: 0.0360\n",
      "Epoch [76/80], Loss: 0.0369\n",
      "Epoch [77/80], Loss: 0.0357\n",
      "Epoch [78/80], Loss: 0.0366\n",
      "Epoch [79/80], Loss: 0.0547\n",
      "Epoch [80/80], Loss: 0.0355\n",
      "update data..\n",
      "task data norm and number entries: tensor(7361.9312, device='cuda:0') torch.Size([100, 47050])\n",
      "..done\n",
      "test performance :  [97.         94.83999634 93.11000061 91.30750275 89.83599854 88.45666504\n",
      " 87.24571991 86.09124756  0.          0.        ]\n",
      "individual errors:  [tensor(90.8400, device='cuda:0'), tensor(83.9300, device='cuda:0'), tensor(85.6700, device='cuda:0'), tensor(84.4900, device='cuda:0'), tensor(85.2700, device='cuda:0'), tensor(85.2100, device='cuda:0'), tensor(86.4000, device='cuda:0'), tensor(86.9200, device='cuda:0')]\n",
      "Epoch [1/80], Loss: 0.1237\n",
      "Epoch [2/80], Loss: 0.1081\n",
      "Epoch [3/80], Loss: 0.0947\n",
      "Epoch [4/80], Loss: 0.0862\n",
      "Epoch [5/80], Loss: 0.0792\n",
      "Epoch [6/80], Loss: 0.0736\n",
      "Epoch [7/80], Loss: 0.1208\n",
      "Epoch [8/80], Loss: 0.0686\n",
      "Epoch [9/80], Loss: 0.0689\n",
      "Epoch [10/80], Loss: 0.0625\n",
      "Epoch [11/80], Loss: 0.0603\n",
      "Epoch [12/80], Loss: 0.0617\n",
      "Epoch [13/80], Loss: 0.0741\n",
      "Epoch [14/80], Loss: 0.0563\n",
      "Epoch [15/80], Loss: 0.0556\n",
      "Epoch [16/80], Loss: 0.0543\n",
      "Epoch [17/80], Loss: 0.0536\n",
      "Epoch [18/80], Loss: 0.0832\n",
      "Epoch [19/80], Loss: 0.0522\n",
      "Epoch [20/80], Loss: 0.0676\n",
      "Epoch [21/80], Loss: 0.0518\n",
      "Epoch [22/80], Loss: 0.0662\n",
      "Epoch [23/80], Loss: 0.0511\n",
      "Epoch [24/80], Loss: 0.0540\n",
      "Epoch [25/80], Loss: 0.0484\n",
      "Epoch [26/80], Loss: 0.0530\n",
      "Epoch [27/80], Loss: 0.0501\n",
      "Epoch [28/80], Loss: 0.0478\n",
      "Epoch [29/80], Loss: 0.0583\n",
      "Epoch [30/80], Loss: 0.0496\n",
      "Epoch [31/80], Loss: 0.0465\n",
      "Epoch [32/80], Loss: 0.0466\n",
      "Epoch [33/80], Loss: 0.0453\n",
      "Epoch [34/80], Loss: 0.0865\n",
      "Epoch [35/80], Loss: 0.0487\n",
      "Epoch [36/80], Loss: 0.0535\n",
      "Epoch [37/80], Loss: 0.0458\n",
      "Epoch [38/80], Loss: 0.0688\n",
      "Epoch [39/80], Loss: 0.0439\n",
      "Epoch [40/80], Loss: 0.0461\n",
      "Epoch [41/80], Loss: 0.1791\n",
      "Epoch [42/80], Loss: 0.0434\n",
      "Epoch [43/80], Loss: 0.0428\n",
      "Epoch [44/80], Loss: 0.0659\n",
      "Epoch [45/80], Loss: 0.0423\n",
      "Epoch [46/80], Loss: 0.0432\n",
      "Epoch [47/80], Loss: 0.0427\n",
      "Epoch [48/80], Loss: 0.0417\n",
      "Epoch [49/80], Loss: 0.0426\n",
      "Epoch [50/80], Loss: 0.0463\n",
      "Epoch [51/80], Loss: 0.0567\n",
      "Epoch [52/80], Loss: 0.0410\n",
      "Epoch [53/80], Loss: 0.0410\n",
      "Epoch [54/80], Loss: 0.0431\n",
      "Epoch [55/80], Loss: 0.0406\n",
      "Epoch [56/80], Loss: 0.0406\n",
      "Epoch [57/80], Loss: 0.0560\n",
      "Epoch [58/80], Loss: 0.0463\n",
      "Epoch [59/80], Loss: 0.1354\n",
      "Epoch [60/80], Loss: 0.0573\n",
      "Epoch [61/80], Loss: 0.0400\n",
      "Epoch [62/80], Loss: 0.0769\n",
      "Epoch [63/80], Loss: 0.0401\n",
      "Epoch [64/80], Loss: 0.0396\n",
      "Epoch [65/80], Loss: 0.0403\n",
      "Epoch [66/80], Loss: 0.0402\n",
      "Epoch [67/80], Loss: 0.0426\n",
      "Epoch [68/80], Loss: 0.0565\n",
      "Epoch [69/80], Loss: 0.0462\n",
      "Epoch [70/80], Loss: 0.0789\n",
      "Epoch [71/80], Loss: 0.0469\n",
      "Epoch [72/80], Loss: 0.0382\n",
      "Epoch [73/80], Loss: 0.0380\n",
      "Epoch [74/80], Loss: 0.0379\n",
      "Epoch [75/80], Loss: 0.0388\n",
      "Epoch [76/80], Loss: 0.0381\n",
      "Epoch [77/80], Loss: 0.0376\n",
      "Epoch [78/80], Loss: 0.0386\n",
      "Epoch [79/80], Loss: 0.0373\n",
      "Epoch [80/80], Loss: 0.0889\n",
      "update data..\n",
      "task data norm and number entries: tensor(7305.7710, device='cuda:0') torch.Size([100, 47050])\n",
      "..done\n",
      "test performance :  [97.         94.83999634 93.11000061 91.30750275 89.83599854 88.45666504\n",
      " 87.24571991 86.09124756 84.81221771  0.        ]\n",
      "individual errors:  [tensor(89.5800, device='cuda:0'), tensor(82.4200, device='cuda:0'), tensor(84.8000, device='cuda:0'), tensor(82.3100, device='cuda:0'), tensor(84.1200, device='cuda:0'), tensor(82.9800, device='cuda:0'), tensor(84.7300, device='cuda:0'), tensor(86., device='cuda:0'), tensor(86.3700, device='cuda:0')]\n",
      "Epoch [1/80], Loss: 0.1306\n",
      "Epoch [2/80], Loss: 0.1161\n",
      "Epoch [3/80], Loss: 0.1155\n",
      "Epoch [4/80], Loss: 0.0991\n",
      "Epoch [5/80], Loss: 0.0918\n",
      "Epoch [6/80], Loss: 0.0767\n",
      "Epoch [7/80], Loss: 0.0727\n",
      "Epoch [8/80], Loss: 0.0789\n",
      "Epoch [9/80], Loss: 0.0722\n",
      "Epoch [10/80], Loss: 0.0930\n",
      "Epoch [11/80], Loss: 0.0613\n",
      "Epoch [12/80], Loss: 0.0618\n",
      "Epoch [13/80], Loss: 0.0581\n",
      "Epoch [14/80], Loss: 0.0594\n",
      "Epoch [15/80], Loss: 0.0582\n",
      "Epoch [16/80], Loss: 0.0722\n",
      "Epoch [17/80], Loss: 0.0539\n",
      "Epoch [18/80], Loss: 0.0543\n",
      "Epoch [19/80], Loss: 0.0545\n",
      "Epoch [20/80], Loss: 0.0876\n",
      "Epoch [21/80], Loss: 0.0586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/80], Loss: 0.0546\n",
      "Epoch [23/80], Loss: 0.0508\n",
      "Epoch [24/80], Loss: 0.0501\n",
      "Epoch [25/80], Loss: 0.0635\n",
      "Epoch [26/80], Loss: 0.0496\n",
      "Epoch [27/80], Loss: 0.0492\n",
      "Epoch [28/80], Loss: 0.0599\n",
      "Epoch [29/80], Loss: 0.0510\n",
      "Epoch [30/80], Loss: 0.0555\n",
      "Epoch [31/80], Loss: 0.0688\n",
      "Epoch [32/80], Loss: 0.0488\n",
      "Epoch [33/80], Loss: 0.0556\n",
      "Epoch [34/80], Loss: 0.0547\n",
      "Epoch [35/80], Loss: 0.0471\n",
      "Epoch [36/80], Loss: 0.0595\n",
      "Epoch [37/80], Loss: 0.0476\n",
      "Epoch [38/80], Loss: 0.0557\n",
      "Epoch [39/80], Loss: 0.0803\n",
      "Epoch [40/80], Loss: 0.0563\n",
      "Epoch [41/80], Loss: 0.0453\n",
      "Epoch [42/80], Loss: 0.0483\n",
      "Epoch [43/80], Loss: 0.0445\n",
      "Epoch [44/80], Loss: 0.0523\n",
      "Epoch [45/80], Loss: 0.0450\n",
      "Epoch [46/80], Loss: 0.0430\n",
      "Epoch [47/80], Loss: 0.0428\n",
      "Epoch [48/80], Loss: 0.0531\n",
      "Epoch [49/80], Loss: 0.0452\n",
      "Epoch [50/80], Loss: 0.0425\n",
      "Epoch [51/80], Loss: 0.0571\n",
      "Epoch [52/80], Loss: 0.0442\n",
      "Epoch [53/80], Loss: 0.0425\n",
      "Epoch [54/80], Loss: 0.0422\n",
      "Epoch [55/80], Loss: 0.0423\n",
      "Epoch [56/80], Loss: 0.0429\n",
      "Epoch [57/80], Loss: 0.0412\n",
      "Epoch [58/80], Loss: 0.0675\n",
      "Epoch [59/80], Loss: 0.0412\n",
      "Epoch [60/80], Loss: 0.0420\n",
      "Epoch [61/80], Loss: 0.0475\n",
      "Epoch [62/80], Loss: 0.0512\n",
      "Epoch [63/80], Loss: 0.0661\n",
      "Epoch [64/80], Loss: 0.0409\n",
      "Epoch [65/80], Loss: 0.0418\n",
      "Epoch [66/80], Loss: 0.0449\n",
      "Epoch [67/80], Loss: 0.0493\n",
      "Epoch [68/80], Loss: 0.0499\n",
      "Epoch [69/80], Loss: 0.0412\n",
      "Epoch [70/80], Loss: 0.0397\n",
      "Epoch [71/80], Loss: 0.0421\n",
      "Epoch [72/80], Loss: 0.0399\n",
      "Epoch [73/80], Loss: 0.0404\n",
      "Epoch [74/80], Loss: 0.0403\n",
      "Epoch [75/80], Loss: 0.0394\n",
      "Epoch [76/80], Loss: 0.0461\n",
      "Epoch [77/80], Loss: 0.0423\n",
      "Epoch [78/80], Loss: 0.0460\n",
      "Epoch [79/80], Loss: 0.0410\n",
      "Epoch [80/80], Loss: 0.0456\n",
      "test performance :  [97.         94.83999634 93.11000061 91.30750275 89.83599854 88.45666504\n",
      " 87.24571991 86.09124756 84.81221771 83.74599457]\n",
      "individual errors:  [tensor(88.4100, device='cuda:0'), tensor(82.0300, device='cuda:0'), tensor(82.7200, device='cuda:0'), tensor(81.0700, device='cuda:0'), tensor(82.5700, device='cuda:0'), tensor(81.3900, device='cuda:0'), tensor(83.1200, device='cuda:0'), tensor(85.1900, device='cuda:0'), tensor(85.4100, device='cuda:0'), tensor(85.5500, device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "respp = run_simulation( get_random_feature_model() ,train_loader,test_loader,EWCplusplus(lam=1.5e-3,s=100),num_epochs=num_epochs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EWC++  400 0.0015\n",
      "Epoch [1/80], Loss: 0.1140\n",
      "Epoch [2/80], Loss: 0.0257\n",
      "Epoch [3/80], Loss: 0.0203\n",
      "Epoch [4/80], Loss: 0.0179\n",
      "Epoch [5/80], Loss: 0.0167\n",
      "Epoch [6/80], Loss: 0.0158\n",
      "Epoch [7/80], Loss: 0.0153\n",
      "Epoch [8/80], Loss: 0.0148\n",
      "Epoch [9/80], Loss: 0.0145\n",
      "Epoch [10/80], Loss: 0.0142\n",
      "Epoch [11/80], Loss: 0.0139\n",
      "Epoch [12/80], Loss: 0.0137\n",
      "Epoch [13/80], Loss: 0.0135\n",
      "Epoch [14/80], Loss: 0.0134\n",
      "Epoch [15/80], Loss: 0.0132\n",
      "Epoch [16/80], Loss: 0.0131\n",
      "Epoch [17/80], Loss: 0.0130\n",
      "Epoch [18/80], Loss: 0.0128\n",
      "Epoch [19/80], Loss: 0.0127\n",
      "Epoch [20/80], Loss: 0.0126\n",
      "Epoch [21/80], Loss: 0.0126\n",
      "Epoch [22/80], Loss: 0.0125\n",
      "Epoch [23/80], Loss: 0.0124\n",
      "Epoch [24/80], Loss: 0.0124\n",
      "Epoch [25/80], Loss: 0.0123\n",
      "Epoch [26/80], Loss: 0.0122\n",
      "Epoch [27/80], Loss: 0.0122\n",
      "Epoch [28/80], Loss: 0.0121\n",
      "Epoch [29/80], Loss: 0.0121\n",
      "Epoch [30/80], Loss: 0.0120\n",
      "Epoch [31/80], Loss: 0.0120\n",
      "Epoch [32/80], Loss: 0.0120\n",
      "Epoch [33/80], Loss: 0.0119\n",
      "Epoch [34/80], Loss: 0.0119\n",
      "Epoch [35/80], Loss: 0.0118\n",
      "Epoch [36/80], Loss: 0.0118\n",
      "Epoch [37/80], Loss: 0.0118\n",
      "Epoch [38/80], Loss: 0.0118\n",
      "Epoch [39/80], Loss: 0.0117\n",
      "Epoch [40/80], Loss: 0.0117\n",
      "Epoch [41/80], Loss: 0.0117\n",
      "Epoch [42/80], Loss: 0.0117\n",
      "Epoch [43/80], Loss: 0.0116\n",
      "Epoch [44/80], Loss: 0.0116\n",
      "Epoch [45/80], Loss: 0.0116\n",
      "Epoch [46/80], Loss: 0.0116\n",
      "Epoch [47/80], Loss: 0.0115\n",
      "Epoch [48/80], Loss: 0.0115\n",
      "Epoch [49/80], Loss: 0.0115\n",
      "Epoch [50/80], Loss: 0.0115\n",
      "Epoch [51/80], Loss: 0.0115\n",
      "Epoch [52/80], Loss: 0.0115\n",
      "Epoch [53/80], Loss: 0.0114\n",
      "Epoch [54/80], Loss: 0.0114\n",
      "Epoch [55/80], Loss: 0.0114\n",
      "Epoch [56/80], Loss: 0.0114\n",
      "Epoch [57/80], Loss: 0.0114\n",
      "Epoch [58/80], Loss: 0.0114\n",
      "Epoch [59/80], Loss: 0.0114\n",
      "Epoch [60/80], Loss: 0.0113\n",
      "Epoch [61/80], Loss: 0.0113\n",
      "Epoch [62/80], Loss: 0.0113\n",
      "Epoch [63/80], Loss: 0.0113\n",
      "Epoch [64/80], Loss: 0.0113\n",
      "Epoch [65/80], Loss: 0.0113\n",
      "Epoch [66/80], Loss: 0.0113\n",
      "Epoch [67/80], Loss: 0.0113\n",
      "Epoch [68/80], Loss: 0.0112\n",
      "Epoch [69/80], Loss: 0.0112\n",
      "Epoch [70/80], Loss: 0.0112\n",
      "Epoch [71/80], Loss: 0.0112\n",
      "Epoch [72/80], Loss: 0.0112\n",
      "Epoch [73/80], Loss: 0.0112\n",
      "Epoch [74/80], Loss: 0.0112\n",
      "Epoch [75/80], Loss: 0.0112\n",
      "Epoch [76/80], Loss: 0.0112\n",
      "Epoch [77/80], Loss: 0.0112\n",
      "Epoch [78/80], Loss: 0.0112\n",
      "Epoch [79/80], Loss: 0.0111\n",
      "Epoch [80/80], Loss: 0.0111\n",
      "generate task data..\n",
      "task data norm and number entries: tensor(7310.2354, device='cuda:0') torch.Size([400, 47050])\n",
      "..done\n",
      "test performance :  [96.91999817  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "individual errors:  [tensor(96.9200, device='cuda:0')]\n",
      "Epoch [1/80], Loss: 0.2443\n",
      "Epoch [2/80], Loss: 0.0826\n",
      "Epoch [3/80], Loss: 0.0643\n",
      "Epoch [4/80], Loss: 0.0529\n",
      "Epoch [5/80], Loss: 0.0481\n",
      "Epoch [6/80], Loss: 0.0475\n",
      "Epoch [7/80], Loss: 0.0427\n",
      "Epoch [8/80], Loss: 0.0409\n",
      "Epoch [9/80], Loss: 0.0379\n",
      "Epoch [10/80], Loss: 0.0379\n",
      "Epoch [11/80], Loss: 0.0371\n",
      "Epoch [12/80], Loss: 0.0350\n",
      "Epoch [13/80], Loss: 0.0333\n",
      "Epoch [14/80], Loss: 0.0331\n",
      "Epoch [15/80], Loss: 0.0317\n",
      "Epoch [16/80], Loss: 0.0308\n",
      "Epoch [17/80], Loss: 0.0328\n",
      "Epoch [18/80], Loss: 0.0299\n",
      "Epoch [19/80], Loss: 0.0289\n",
      "Epoch [20/80], Loss: 0.0332\n",
      "Epoch [21/80], Loss: 0.0283\n",
      "Epoch [22/80], Loss: 0.0288\n",
      "Epoch [23/80], Loss: 0.0309\n",
      "Epoch [24/80], Loss: 0.0278\n",
      "Epoch [25/80], Loss: 0.0272\n",
      "Epoch [26/80], Loss: 0.0287\n",
      "Epoch [27/80], Loss: 0.0258\n",
      "Epoch [28/80], Loss: 0.0261\n",
      "Epoch [29/80], Loss: 0.0257\n",
      "Epoch [30/80], Loss: 0.0265\n",
      "Epoch [31/80], Loss: 0.0257\n",
      "Epoch [32/80], Loss: 0.0325\n",
      "Epoch [33/80], Loss: 0.0270\n",
      "Epoch [34/80], Loss: 0.0246\n",
      "Epoch [35/80], Loss: 0.0261\n",
      "Epoch [36/80], Loss: 0.0246\n",
      "Epoch [37/80], Loss: 0.0241\n",
      "Epoch [38/80], Loss: 0.0241\n",
      "Epoch [39/80], Loss: 0.0242\n",
      "Epoch [40/80], Loss: 0.0236\n",
      "Epoch [41/80], Loss: 0.0229\n",
      "Epoch [42/80], Loss: 0.0227\n",
      "Epoch [43/80], Loss: 0.0226\n",
      "Epoch [44/80], Loss: 0.0246\n",
      "Epoch [45/80], Loss: 0.0249\n",
      "Epoch [46/80], Loss: 0.0230\n",
      "Epoch [47/80], Loss: 0.0231\n",
      "Epoch [48/80], Loss: 0.0220\n",
      "Epoch [49/80], Loss: 0.0247\n",
      "Epoch [50/80], Loss: 0.0217\n",
      "Epoch [51/80], Loss: 0.0253\n",
      "Epoch [52/80], Loss: 0.0217\n",
      "Epoch [53/80], Loss: 0.0214\n",
      "Epoch [54/80], Loss: 0.0252\n",
      "Epoch [55/80], Loss: 0.0214\n",
      "Epoch [56/80], Loss: 0.0219\n",
      "Epoch [57/80], Loss: 0.0227\n",
      "Epoch [58/80], Loss: 0.0220\n",
      "Epoch [59/80], Loss: 0.0224\n",
      "Epoch [60/80], Loss: 0.0208\n",
      "Epoch [61/80], Loss: 0.0208\n",
      "Epoch [62/80], Loss: 0.0221\n",
      "Epoch [63/80], Loss: 0.0205\n",
      "Epoch [64/80], Loss: 0.0205\n",
      "Epoch [65/80], Loss: 0.0216\n",
      "Epoch [66/80], Loss: 0.0209\n",
      "Epoch [67/80], Loss: 0.0204\n",
      "Epoch [68/80], Loss: 0.0210\n",
      "Epoch [69/80], Loss: 0.0207\n",
      "Epoch [70/80], Loss: 0.0203\n",
      "Epoch [71/80], Loss: 0.0204\n",
      "Epoch [72/80], Loss: 0.0227\n",
      "Epoch [73/80], Loss: 0.0247\n",
      "Epoch [74/80], Loss: 0.0199\n",
      "Epoch [75/80], Loss: 0.0208\n",
      "Epoch [76/80], Loss: 0.0201\n",
      "Epoch [77/80], Loss: 0.0204\n",
      "Epoch [78/80], Loss: 0.0195\n",
      "Epoch [79/80], Loss: 0.0211\n",
      "Epoch [80/80], Loss: 0.0223\n",
      "update data..\n",
      "task data norm and number entries: tensor(7227.3906, device='cuda:0') torch.Size([400, 47050])\n",
      "..done\n",
      "test performance :  [96.91999817 94.875       0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "individual errors:  [tensor(95.8500, device='cuda:0'), tensor(93.9000, device='cuda:0')]\n",
      "Epoch [1/80], Loss: 0.1433\n",
      "Epoch [2/80], Loss: 0.0948\n",
      "Epoch [3/80], Loss: 0.0715\n",
      "Epoch [4/80], Loss: 0.0604\n",
      "Epoch [5/80], Loss: 0.0586\n",
      "Epoch [6/80], Loss: 0.0511\n",
      "Epoch [7/80], Loss: 0.0484\n",
      "Epoch [8/80], Loss: 0.0469\n",
      "Epoch [9/80], Loss: 0.0448\n",
      "Epoch [10/80], Loss: 0.0432\n",
      "Epoch [11/80], Loss: 0.0441\n",
      "Epoch [12/80], Loss: 0.0410\n",
      "Epoch [13/80], Loss: 0.0401\n",
      "Epoch [14/80], Loss: 0.0393\n",
      "Epoch [15/80], Loss: 0.0385\n",
      "Epoch [16/80], Loss: 0.0377\n",
      "Epoch [17/80], Loss: 0.0375\n",
      "Epoch [18/80], Loss: 0.0364\n",
      "Epoch [19/80], Loss: 0.0361\n",
      "Epoch [20/80], Loss: 0.0363\n",
      "Epoch [21/80], Loss: 0.0356\n",
      "Epoch [22/80], Loss: 0.0343\n",
      "Epoch [23/80], Loss: 0.0363\n",
      "Epoch [24/80], Loss: 0.0344\n",
      "Epoch [25/80], Loss: 0.0330\n",
      "Epoch [26/80], Loss: 0.0469\n",
      "Epoch [27/80], Loss: 0.0400\n",
      "Epoch [28/80], Loss: 0.0325\n",
      "Epoch [29/80], Loss: 0.0315\n",
      "Epoch [30/80], Loss: 0.0313\n",
      "Epoch [31/80], Loss: 0.0311\n",
      "Epoch [32/80], Loss: 0.0378\n",
      "Epoch [33/80], Loss: 0.0303\n",
      "Epoch [34/80], Loss: 0.0300\n",
      "Epoch [35/80], Loss: 0.0302\n",
      "Epoch [36/80], Loss: 0.0295\n",
      "Epoch [37/80], Loss: 0.0332\n",
      "Epoch [38/80], Loss: 0.0299\n",
      "Epoch [39/80], Loss: 0.0332\n",
      "Epoch [40/80], Loss: 0.0344\n",
      "Epoch [41/80], Loss: 0.0285\n",
      "Epoch [42/80], Loss: 0.0289\n",
      "Epoch [43/80], Loss: 0.0286\n",
      "Epoch [44/80], Loss: 0.0281\n",
      "Epoch [45/80], Loss: 0.0279\n",
      "Epoch [46/80], Loss: 0.0277\n",
      "Epoch [47/80], Loss: 0.0273\n",
      "Epoch [48/80], Loss: 0.0273\n",
      "Epoch [49/80], Loss: 0.0272\n",
      "Epoch [50/80], Loss: 0.0272\n",
      "Epoch [51/80], Loss: 0.0269\n",
      "Epoch [52/80], Loss: 0.0268\n",
      "Epoch [53/80], Loss: 0.0297\n",
      "Epoch [54/80], Loss: 0.0271\n",
      "Epoch [55/80], Loss: 0.0324\n",
      "Epoch [56/80], Loss: 0.0262\n",
      "Epoch [57/80], Loss: 0.0325\n",
      "Epoch [58/80], Loss: 0.0271\n",
      "Epoch [59/80], Loss: 0.0258\n",
      "Epoch [60/80], Loss: 0.0255\n",
      "Epoch [61/80], Loss: 0.0254\n",
      "Epoch [62/80], Loss: 0.0374\n",
      "Epoch [63/80], Loss: 0.0252\n",
      "Epoch [64/80], Loss: 0.0251\n",
      "Epoch [65/80], Loss: 0.0268\n",
      "Epoch [66/80], Loss: 0.0438\n",
      "Epoch [67/80], Loss: 0.0253\n",
      "Epoch [68/80], Loss: 0.0247\n",
      "Epoch [69/80], Loss: 0.0256\n",
      "Epoch [70/80], Loss: 0.0359\n",
      "Epoch [71/80], Loss: 0.0245\n",
      "Epoch [72/80], Loss: 0.0248\n",
      "Epoch [73/80], Loss: 0.0251\n",
      "Epoch [74/80], Loss: 0.0242\n",
      "Epoch [75/80], Loss: 0.0256\n",
      "Epoch [76/80], Loss: 0.0243\n",
      "Epoch [77/80], Loss: 0.0298\n",
      "Epoch [78/80], Loss: 0.0238\n",
      "Epoch [79/80], Loss: 0.0251\n",
      "Epoch [80/80], Loss: 0.0246\n",
      "update data..\n",
      "task data norm and number entries: tensor(7350.7075, device='cuda:0') torch.Size([400, 47050])\n",
      "..done\n",
      "test performance :  [96.91999817 94.875      93.52999878  0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "individual errors:  [tensor(95.4200, device='cuda:0'), tensor(93.1000, device='cuda:0'), tensor(92.0700, device='cuda:0')]\n",
      "Epoch [1/80], Loss: 0.1555\n",
      "Epoch [2/80], Loss: 0.1105\n",
      "Epoch [3/80], Loss: 0.0905\n",
      "Epoch [4/80], Loss: 0.0729\n",
      "Epoch [5/80], Loss: 0.0652\n",
      "Epoch [6/80], Loss: 0.0606\n",
      "Epoch [7/80], Loss: 0.0577\n",
      "Epoch [8/80], Loss: 0.0549\n",
      "Epoch [9/80], Loss: 0.0531\n",
      "Epoch [10/80], Loss: 0.0513\n",
      "Epoch [11/80], Loss: 0.0504\n",
      "Epoch [12/80], Loss: 0.0531\n",
      "Epoch [13/80], Loss: 0.0477\n",
      "Epoch [14/80], Loss: 0.0472\n",
      "Epoch [15/80], Loss: 0.0462\n",
      "Epoch [16/80], Loss: 0.0475\n",
      "Epoch [17/80], Loss: 0.0446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/80], Loss: 0.0641\n",
      "Epoch [19/80], Loss: 0.0510\n",
      "Epoch [20/80], Loss: 0.0430\n",
      "Epoch [21/80], Loss: 0.0421\n",
      "Epoch [22/80], Loss: 0.0418\n",
      "Epoch [23/80], Loss: 0.0414\n",
      "Epoch [24/80], Loss: 0.0410\n",
      "Epoch [25/80], Loss: 0.0416\n",
      "Epoch [26/80], Loss: 0.0397\n",
      "Epoch [27/80], Loss: 0.0453\n",
      "Epoch [28/80], Loss: 0.0389\n",
      "Epoch [29/80], Loss: 0.0387\n",
      "Epoch [30/80], Loss: 0.0484\n",
      "Epoch [31/80], Loss: 0.0389\n",
      "Epoch [32/80], Loss: 0.0375\n",
      "Epoch [33/80], Loss: 0.0372\n",
      "Epoch [34/80], Loss: 0.0399\n",
      "Epoch [35/80], Loss: 0.0428\n",
      "Epoch [36/80], Loss: 0.0379\n",
      "Epoch [37/80], Loss: 0.0420\n",
      "Epoch [38/80], Loss: 0.0360\n",
      "Epoch [39/80], Loss: 0.0357\n",
      "Epoch [40/80], Loss: 0.0369\n",
      "Epoch [41/80], Loss: 0.0379\n",
      "Epoch [42/80], Loss: 0.0343\n",
      "Epoch [43/80], Loss: 0.0346\n",
      "Epoch [44/80], Loss: 0.0360\n",
      "Epoch [45/80], Loss: 0.0343\n",
      "Epoch [46/80], Loss: 0.0340\n",
      "Epoch [47/80], Loss: 0.0339\n",
      "Epoch [48/80], Loss: 0.0333\n",
      "Epoch [49/80], Loss: 0.0339\n",
      "Epoch [50/80], Loss: 0.0719\n",
      "Epoch [51/80], Loss: 0.0323\n",
      "Epoch [52/80], Loss: 0.0322\n",
      "Epoch [53/80], Loss: 0.0321\n",
      "Epoch [54/80], Loss: 0.0324\n",
      "Epoch [55/80], Loss: 0.0317\n",
      "Epoch [56/80], Loss: 0.0336\n",
      "Epoch [57/80], Loss: 0.0312\n",
      "Epoch [58/80], Loss: 0.0501\n",
      "Epoch [59/80], Loss: 0.0308\n",
      "Epoch [60/80], Loss: 0.0307\n",
      "Epoch [61/80], Loss: 0.0833\n",
      "Epoch [62/80], Loss: 0.0319\n",
      "Epoch [63/80], Loss: 0.0462\n",
      "Epoch [64/80], Loss: 0.0300\n",
      "Epoch [65/80], Loss: 0.0407\n",
      "Epoch [66/80], Loss: 0.0298\n",
      "Epoch [67/80], Loss: 0.0295\n",
      "Epoch [68/80], Loss: 0.0294\n",
      "Epoch [69/80], Loss: 0.0293\n",
      "Epoch [70/80], Loss: 0.0326\n",
      "Epoch [71/80], Loss: 0.0332\n",
      "Epoch [72/80], Loss: 0.0288\n",
      "Epoch [73/80], Loss: 0.0289\n",
      "Epoch [74/80], Loss: 0.0287\n",
      "Epoch [75/80], Loss: 0.0302\n",
      "Epoch [76/80], Loss: 0.0285\n",
      "Epoch [77/80], Loss: 0.0403\n",
      "Epoch [78/80], Loss: 0.0284\n",
      "Epoch [79/80], Loss: 0.0309\n",
      "Epoch [80/80], Loss: 0.0280\n",
      "update data..\n",
      "task data norm and number entries: tensor(7479.1626, device='cuda:0') torch.Size([400, 47050])\n",
      "..done\n",
      "test performance :  [96.91999817 94.875      93.52999878 92.21499634  0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "individual errors:  [tensor(94.8600, device='cuda:0'), tensor(92.3200, device='cuda:0'), tensor(91.4500, device='cuda:0'), tensor(90.2300, device='cuda:0')]\n",
      "Epoch [1/80], Loss: 0.1274\n",
      "Epoch [2/80], Loss: 0.1028\n",
      "Epoch [3/80], Loss: 0.0861\n",
      "Epoch [4/80], Loss: 0.0839\n",
      "Epoch [5/80], Loss: 0.0690\n",
      "Epoch [6/80], Loss: 0.0702\n",
      "Epoch [7/80], Loss: 0.0674\n",
      "Epoch [8/80], Loss: 0.0659\n",
      "Epoch [9/80], Loss: 0.0576\n",
      "Epoch [10/80], Loss: 0.0543\n",
      "Epoch [11/80], Loss: 0.0530\n",
      "Epoch [12/80], Loss: 0.0526\n",
      "Epoch [13/80], Loss: 0.0511\n",
      "Epoch [14/80], Loss: 0.0588\n",
      "Epoch [15/80], Loss: 0.0499\n",
      "Epoch [16/80], Loss: 0.0474\n",
      "Epoch [17/80], Loss: 0.0535\n",
      "Epoch [18/80], Loss: 0.0476\n",
      "Epoch [19/80], Loss: 0.0477\n",
      "Epoch [20/80], Loss: 0.0473\n",
      "Epoch [21/80], Loss: 0.0476\n",
      "Epoch [22/80], Loss: 0.0444\n",
      "Epoch [23/80], Loss: 0.0435\n",
      "Epoch [24/80], Loss: 0.0456\n",
      "Epoch [25/80], Loss: 0.0422\n",
      "Epoch [26/80], Loss: 0.0431\n",
      "Epoch [27/80], Loss: 0.0428\n",
      "Epoch [28/80], Loss: 0.0410\n",
      "Epoch [29/80], Loss: 0.0418\n",
      "Epoch [30/80], Loss: 0.0411\n",
      "Epoch [31/80], Loss: 0.0420\n",
      "Epoch [32/80], Loss: 0.0445\n",
      "Epoch [33/80], Loss: 0.0605\n",
      "Epoch [34/80], Loss: 0.0389\n",
      "Epoch [35/80], Loss: 0.0387\n",
      "Epoch [36/80], Loss: 0.0422\n",
      "Epoch [37/80], Loss: 0.0382\n",
      "Epoch [38/80], Loss: 0.0385\n",
      "Epoch [39/80], Loss: 0.0381\n",
      "Epoch [40/80], Loss: 0.0398\n",
      "Epoch [41/80], Loss: 0.0372\n",
      "Epoch [42/80], Loss: 0.0416\n",
      "Epoch [43/80], Loss: 0.0401\n",
      "Epoch [44/80], Loss: 0.0407\n",
      "Epoch [45/80], Loss: 0.0429\n",
      "Epoch [46/80], Loss: 0.0405\n",
      "Epoch [47/80], Loss: 0.0357\n",
      "Epoch [48/80], Loss: 0.0354\n",
      "Epoch [49/80], Loss: 0.0385\n",
      "Epoch [50/80], Loss: 0.0353\n",
      "Epoch [51/80], Loss: 0.0349\n",
      "Epoch [52/80], Loss: 0.0348\n",
      "Epoch [53/80], Loss: 0.0346\n",
      "Epoch [54/80], Loss: 0.0348\n",
      "Epoch [55/80], Loss: 0.0360\n",
      "Epoch [56/80], Loss: 0.0346\n",
      "Epoch [57/80], Loss: 0.0341\n",
      "Epoch [58/80], Loss: 0.0383\n",
      "Epoch [59/80], Loss: 0.0365\n",
      "Epoch [60/80], Loss: 0.0359\n",
      "Epoch [61/80], Loss: 0.0445\n",
      "Epoch [62/80], Loss: 0.0332\n",
      "Epoch [63/80], Loss: 0.0377\n",
      "Epoch [64/80], Loss: 0.0328\n",
      "Epoch [65/80], Loss: 0.0329\n",
      "Epoch [66/80], Loss: 0.0529\n",
      "Epoch [67/80], Loss: 0.0369\n",
      "Epoch [68/80], Loss: 0.0323\n",
      "Epoch [69/80], Loss: 0.0322\n",
      "Epoch [70/80], Loss: 0.0320\n",
      "Epoch [71/80], Loss: 0.0318\n",
      "Epoch [72/80], Loss: 0.0378\n",
      "Epoch [73/80], Loss: 0.0324\n",
      "Epoch [74/80], Loss: 0.0317\n",
      "Epoch [75/80], Loss: 0.0374\n",
      "Epoch [76/80], Loss: 0.0318\n",
      "Epoch [77/80], Loss: 0.0533\n",
      "Epoch [78/80], Loss: 0.0315\n",
      "Epoch [79/80], Loss: 0.0331\n",
      "Epoch [80/80], Loss: 0.0307\n",
      "update data..\n",
      "task data norm and number entries: tensor(7312.9614, device='cuda:0') torch.Size([400, 47050])\n",
      "..done\n",
      "test performance :  [96.91999817 94.875      93.52999878 92.21499634 91.05799866  0.\n",
      "  0.          0.          0.          0.        ]\n",
      "individual errors:  [tensor(94.4200, device='cuda:0'), tensor(92., device='cuda:0'), tensor(90.5300, device='cuda:0'), tensor(89.3600, device='cuda:0'), tensor(88.9800, device='cuda:0')]\n",
      "Epoch [1/80], Loss: 0.1156\n",
      "Epoch [2/80], Loss: 0.0996\n",
      "Epoch [3/80], Loss: 0.0855\n",
      "Epoch [4/80], Loss: 0.0767\n",
      "Epoch [5/80], Loss: 0.0707\n",
      "Epoch [6/80], Loss: 0.0884\n",
      "Epoch [7/80], Loss: 0.0629\n",
      "Epoch [8/80], Loss: 0.0667\n",
      "Epoch [9/80], Loss: 0.0585\n",
      "Epoch [10/80], Loss: 0.0615\n",
      "Epoch [11/80], Loss: 0.0548\n",
      "Epoch [12/80], Loss: 0.0545\n",
      "Epoch [13/80], Loss: 0.0683\n",
      "Epoch [14/80], Loss: 0.0738\n",
      "Epoch [15/80], Loss: 0.0606\n",
      "Epoch [16/80], Loss: 0.0510\n",
      "Epoch [17/80], Loss: 0.0792\n",
      "Epoch [18/80], Loss: 0.0584\n",
      "Epoch [19/80], Loss: 0.0504\n",
      "Epoch [20/80], Loss: 0.0491\n",
      "Epoch [21/80], Loss: 0.0546\n",
      "Epoch [22/80], Loss: 0.0809\n",
      "Epoch [23/80], Loss: 0.0465\n",
      "Epoch [24/80], Loss: 0.0498\n",
      "Epoch [25/80], Loss: 0.0484\n",
      "Epoch [26/80], Loss: 0.0452\n",
      "Epoch [27/80], Loss: 0.0482\n",
      "Epoch [28/80], Loss: 0.0574\n",
      "Epoch [29/80], Loss: 0.0449\n",
      "Epoch [30/80], Loss: 0.0463\n",
      "Epoch [31/80], Loss: 0.0440\n",
      "Epoch [32/80], Loss: 0.0433\n",
      "Epoch [33/80], Loss: 0.0545\n",
      "Epoch [34/80], Loss: 0.0512\n",
      "Epoch [35/80], Loss: 0.0601\n",
      "Epoch [36/80], Loss: 0.0513\n",
      "Epoch [37/80], Loss: 0.0422\n",
      "Epoch [38/80], Loss: 0.0443\n",
      "Epoch [39/80], Loss: 0.0414\n",
      "Epoch [40/80], Loss: 0.0413\n",
      "Epoch [41/80], Loss: 0.0411\n",
      "Epoch [42/80], Loss: 0.0408\n",
      "Epoch [43/80], Loss: 0.0472\n",
      "Epoch [44/80], Loss: 0.0405\n",
      "Epoch [45/80], Loss: 0.0767\n",
      "Epoch [46/80], Loss: 0.0400\n",
      "Epoch [47/80], Loss: 0.0400\n",
      "Epoch [48/80], Loss: 0.0490\n",
      "Epoch [49/80], Loss: 0.0418\n",
      "Epoch [50/80], Loss: 0.0394\n",
      "Epoch [51/80], Loss: 0.0391\n",
      "Epoch [52/80], Loss: 0.0405\n",
      "Epoch [53/80], Loss: 0.0474\n",
      "Epoch [54/80], Loss: 0.0432\n",
      "Epoch [55/80], Loss: 0.0426\n",
      "Epoch [56/80], Loss: 0.0387\n",
      "Epoch [57/80], Loss: 0.0380\n",
      "Epoch [58/80], Loss: 0.0386\n",
      "Epoch [59/80], Loss: 0.0380\n",
      "Epoch [60/80], Loss: 0.0389\n",
      "Epoch [61/80], Loss: 0.0378\n",
      "Epoch [62/80], Loss: 0.0371\n",
      "Epoch [63/80], Loss: 0.0370\n",
      "Epoch [64/80], Loss: 0.0370\n",
      "Epoch [65/80], Loss: 0.0437\n",
      "Epoch [66/80], Loss: 0.0367\n",
      "Epoch [67/80], Loss: 0.0422\n",
      "Epoch [68/80], Loss: 0.0373\n",
      "Epoch [69/80], Loss: 0.0366\n",
      "Epoch [70/80], Loss: 0.0363\n",
      "Epoch [71/80], Loss: 0.0394\n",
      "Epoch [72/80], Loss: 0.0427\n",
      "Epoch [73/80], Loss: 0.0374\n",
      "Epoch [74/80], Loss: 0.0443\n",
      "Epoch [75/80], Loss: 0.0368\n",
      "Epoch [76/80], Loss: 0.0414\n",
      "Epoch [77/80], Loss: 0.0352\n",
      "Epoch [78/80], Loss: 0.0408\n",
      "Epoch [79/80], Loss: 0.0406\n",
      "Epoch [80/80], Loss: 0.0373\n",
      "update data..\n",
      "task data norm and number entries: tensor(7431.7725, device='cuda:0') torch.Size([400, 47050])\n",
      "..done\n",
      "test performance :  [96.91999817 94.875      93.52999878 92.21499634 91.05799866 89.97000122\n",
      "  0.          0.          0.          0.        ]\n",
      "individual errors:  [tensor(93.9400, device='cuda:0'), tensor(90.9800, device='cuda:0'), tensor(89.7100, device='cuda:0'), tensor(88.9100, device='cuda:0'), tensor(87.7900, device='cuda:0'), tensor(88.4900, device='cuda:0')]\n",
      "Epoch [1/80], Loss: 0.1442\n",
      "Epoch [2/80], Loss: 0.1213\n",
      "Epoch [3/80], Loss: 0.1018\n",
      "Epoch [4/80], Loss: 0.0882\n",
      "Epoch [5/80], Loss: 0.0787\n",
      "Epoch [6/80], Loss: 0.0891\n",
      "Epoch [7/80], Loss: 0.0675\n",
      "Epoch [8/80], Loss: 0.0709\n",
      "Epoch [9/80], Loss: 0.0626\n",
      "Epoch [10/80], Loss: 0.0626\n",
      "Epoch [11/80], Loss: 0.0577\n",
      "Epoch [12/80], Loss: 0.0572\n",
      "Epoch [13/80], Loss: 0.0561\n",
      "Epoch [14/80], Loss: 0.0535\n",
      "Epoch [15/80], Loss: 0.0568\n",
      "Epoch [16/80], Loss: 0.0517\n",
      "Epoch [17/80], Loss: 0.0547\n",
      "Epoch [18/80], Loss: 0.0505\n",
      "Epoch [19/80], Loss: 0.0525\n",
      "Epoch [20/80], Loss: 0.0677\n",
      "Epoch [21/80], Loss: 0.0510\n",
      "Epoch [22/80], Loss: 0.0690\n",
      "Epoch [23/80], Loss: 0.0487\n",
      "Epoch [24/80], Loss: 0.0486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/80], Loss: 0.0616\n",
      "Epoch [26/80], Loss: 0.0463\n",
      "Epoch [27/80], Loss: 0.0485\n",
      "Epoch [28/80], Loss: 0.0450\n",
      "Epoch [29/80], Loss: 0.0454\n",
      "Epoch [30/80], Loss: 0.0447\n",
      "Epoch [31/80], Loss: 0.0449\n",
      "Epoch [32/80], Loss: 0.0471\n",
      "Epoch [33/80], Loss: 0.0440\n",
      "Epoch [34/80], Loss: 0.0428\n",
      "Epoch [35/80], Loss: 0.0426\n",
      "Epoch [36/80], Loss: 0.0442\n",
      "Epoch [37/80], Loss: 0.0420\n",
      "Epoch [38/80], Loss: 0.0586\n",
      "Epoch [39/80], Loss: 0.0520\n",
      "Epoch [40/80], Loss: 0.0428\n",
      "Epoch [41/80], Loss: 0.0443\n",
      "Epoch [42/80], Loss: 0.0423\n",
      "Epoch [43/80], Loss: 0.0498\n",
      "Epoch [44/80], Loss: 0.0410\n",
      "Epoch [45/80], Loss: 0.0454\n",
      "Epoch [46/80], Loss: 0.0458\n",
      "Epoch [47/80], Loss: 0.0516\n",
      "Epoch [48/80], Loss: 0.0408\n",
      "Epoch [49/80], Loss: 0.0396\n",
      "Epoch [50/80], Loss: 0.0445\n",
      "Epoch [51/80], Loss: 0.0447\n",
      "Epoch [52/80], Loss: 0.0395\n",
      "Epoch [53/80], Loss: 0.0414\n",
      "Epoch [54/80], Loss: 0.0537\n",
      "Epoch [55/80], Loss: 0.0412\n",
      "Epoch [56/80], Loss: 0.0432\n",
      "Epoch [57/80], Loss: 0.0435\n",
      "Epoch [58/80], Loss: 0.0378\n",
      "Epoch [59/80], Loss: 0.0777\n",
      "Epoch [60/80], Loss: 0.0386\n",
      "Epoch [61/80], Loss: 0.0374\n",
      "Epoch [62/80], Loss: 0.0372\n",
      "Epoch [63/80], Loss: 0.0399\n",
      "Epoch [64/80], Loss: 0.0373\n",
      "Epoch [65/80], Loss: 0.0367\n",
      "Epoch [66/80], Loss: 0.0365\n",
      "Epoch [67/80], Loss: 0.0366\n",
      "Epoch [68/80], Loss: 0.0523\n",
      "Epoch [69/80], Loss: 0.0391\n",
      "Epoch [70/80], Loss: 0.0373\n",
      "Epoch [71/80], Loss: 0.0774\n",
      "Epoch [72/80], Loss: 0.0360\n",
      "Epoch [73/80], Loss: 0.0402\n",
      "Epoch [74/80], Loss: 0.0355\n",
      "Epoch [75/80], Loss: 0.0365\n",
      "Epoch [76/80], Loss: 0.0356\n",
      "Epoch [77/80], Loss: 0.0399\n",
      "Epoch [78/80], Loss: 0.0357\n",
      "Epoch [79/80], Loss: 0.0368\n",
      "Epoch [80/80], Loss: 0.0565\n",
      "update data..\n",
      "task data norm and number entries: tensor(7420.4028, device='cuda:0') torch.Size([400, 47050])\n",
      "..done\n",
      "test performance :  [96.91999817 94.875      93.52999878 92.21499634 91.05799866 89.97000122\n",
      " 88.98001099  0.          0.          0.        ]\n",
      "individual errors:  [tensor(93.4600, device='cuda:0'), tensor(90.4800, device='cuda:0'), tensor(88.7900, device='cuda:0'), tensor(88.2800, device='cuda:0'), tensor(87.2000, device='cuda:0'), tensor(87.2000, device='cuda:0'), tensor(87.4500, device='cuda:0')]\n",
      "Epoch [1/80], Loss: 0.1411\n",
      "Epoch [2/80], Loss: 0.1224\n",
      "Epoch [3/80], Loss: 0.1049\n",
      "Epoch [4/80], Loss: 0.0919\n",
      "Epoch [5/80], Loss: 0.0840\n",
      "Epoch [6/80], Loss: 0.0803\n",
      "Epoch [7/80], Loss: 0.0731\n",
      "Epoch [8/80], Loss: 0.0695\n",
      "Epoch [9/80], Loss: 0.0665\n",
      "Epoch [10/80], Loss: 0.0642\n",
      "Epoch [11/80], Loss: 0.0790\n",
      "Epoch [12/80], Loss: 0.0603\n",
      "Epoch [13/80], Loss: 0.0656\n",
      "Epoch [14/80], Loss: 0.0580\n",
      "Epoch [15/80], Loss: 0.0952\n",
      "Epoch [16/80], Loss: 0.0663\n",
      "Epoch [17/80], Loss: 0.0589\n",
      "Epoch [18/80], Loss: 0.0903\n",
      "Epoch [19/80], Loss: 0.0606\n",
      "Epoch [20/80], Loss: 0.0527\n",
      "Epoch [21/80], Loss: 0.0512\n",
      "Epoch [22/80], Loss: 0.0505\n",
      "Epoch [23/80], Loss: 0.0505\n",
      "Epoch [24/80], Loss: 0.0504\n",
      "Epoch [25/80], Loss: 0.0499\n",
      "Epoch [26/80], Loss: 0.0486\n",
      "Epoch [27/80], Loss: 0.0492\n",
      "Epoch [28/80], Loss: 0.0475\n",
      "Epoch [29/80], Loss: 0.0712\n",
      "Epoch [30/80], Loss: 0.0486\n",
      "Epoch [31/80], Loss: 0.0463\n",
      "Epoch [32/80], Loss: 0.0462\n",
      "Epoch [33/80], Loss: 0.0494\n",
      "Epoch [34/80], Loss: 0.0458\n",
      "Epoch [35/80], Loss: 0.0618\n",
      "Epoch [36/80], Loss: 0.0453\n",
      "Epoch [37/80], Loss: 0.0511\n",
      "Epoch [38/80], Loss: 0.0947\n",
      "Epoch [39/80], Loss: 0.0437\n",
      "Epoch [40/80], Loss: 0.0438\n",
      "Epoch [41/80], Loss: 0.0447\n",
      "Epoch [42/80], Loss: 0.0429\n",
      "Epoch [43/80], Loss: 0.0432\n",
      "Epoch [44/80], Loss: 0.0426\n",
      "Epoch [45/80], Loss: 0.0716\n",
      "Epoch [46/80], Loss: 0.0485\n",
      "Epoch [47/80], Loss: 0.0418\n",
      "Epoch [48/80], Loss: 0.0509\n",
      "Epoch [49/80], Loss: 0.0414\n",
      "Epoch [50/80], Loss: 0.0412\n",
      "Epoch [51/80], Loss: 0.0423\n",
      "Epoch [52/80], Loss: 0.0408\n",
      "Epoch [53/80], Loss: 0.0419\n",
      "Epoch [54/80], Loss: 0.0656\n",
      "Epoch [55/80], Loss: 0.0405\n",
      "Epoch [56/80], Loss: 0.0402\n",
      "Epoch [57/80], Loss: 0.0403\n",
      "Epoch [58/80], Loss: 0.0849\n",
      "Epoch [59/80], Loss: 0.0403\n",
      "Epoch [60/80], Loss: 0.0402\n",
      "Epoch [61/80], Loss: 0.0553\n",
      "task data norm and number entries: tensor(7295.0674, device='cuda:0') torch.Size([400, 47050])\n",
      "..done\n",
      "test performance :  [96.91999817 94.875      93.52999878 92.21499634 91.05799866 89.97000122\n",
      " 88.98001099 88.04374695  0.          0.        ]\n",
      "individual errors:  [tensor(93.1000, device='cuda:0'), tensor(90.2500, device='cuda:0'), tensor(87.9000, device='cuda:0'), tensor(87.4300, device='cuda:0'), tensor(86.4900, device='cuda:0'), tensor(86.2900, device='cuda:0'), tensor(86.9000, device='cuda:0'), tensor(85.9900, device='cuda:0')]\n",
      "Epoch [1/80], Loss: 0.1350\n",
      "Epoch [2/80], Loss: 0.1190\n",
      "Epoch [3/80], Loss: 0.1030\n",
      "Epoch [4/80], Loss: 0.0919\n",
      "Epoch [5/80], Loss: 0.1217\n",
      "Epoch [6/80], Loss: 0.0881\n",
      "Epoch [7/80], Loss: 0.0751\n",
      "Epoch [8/80], Loss: 0.0737\n",
      "Epoch [9/80], Loss: 0.0683\n",
      "Epoch [10/80], Loss: 0.0661\n",
      "Epoch [11/80], Loss: 0.0640\n",
      "Epoch [12/80], Loss: 0.0623\n",
      "Epoch [13/80], Loss: 0.1093\n",
      "Epoch [14/80], Loss: 0.0937\n",
      "Epoch [15/80], Loss: 0.0606\n",
      "Epoch [16/80], Loss: 0.0574\n",
      "Epoch [17/80], Loss: 0.0563\n",
      "Epoch [18/80], Loss: 0.0577\n",
      "Epoch [19/80], Loss: 0.0544\n",
      "Epoch [20/80], Loss: 0.0536\n",
      "Epoch [21/80], Loss: 0.0529\n",
      "Epoch [22/80], Loss: 0.0531\n",
      "Epoch [23/80], Loss: 0.0538\n",
      "Epoch [24/80], Loss: 0.0630\n",
      "Epoch [25/80], Loss: 0.0509\n",
      "Epoch [26/80], Loss: 0.0723\n",
      "Epoch [27/80], Loss: 0.0503\n",
      "Epoch [28/80], Loss: 0.0497\n",
      "Epoch [29/80], Loss: 0.0493\n",
      "Epoch [30/80], Loss: 0.0482\n",
      "Epoch [31/80], Loss: 0.0636\n",
      "Epoch [32/80], Loss: 0.0480\n",
      "Epoch [33/80], Loss: 0.0511\n",
      "Epoch [34/80], Loss: 0.0510\n",
      "Epoch [35/80], Loss: 0.0582\n",
      "Epoch [36/80], Loss: 0.1322\n",
      "Epoch [37/80], Loss: 0.0461\n",
      "Epoch [38/80], Loss: 0.0466\n",
      "Epoch [39/80], Loss: 0.0453\n",
      "Epoch [40/80], Loss: 0.0470\n",
      "Epoch [41/80], Loss: 0.0605\n",
      "Epoch [42/80], Loss: 0.0447\n",
      "Epoch [43/80], Loss: 0.0442\n",
      "Epoch [44/80], Loss: 0.0440\n",
      "Epoch [45/80], Loss: 0.0437\n",
      "Epoch [46/80], Loss: 0.0459\n",
      "Epoch [47/80], Loss: 0.0434\n",
      "Epoch [48/80], Loss: 0.0444\n",
      "Epoch [49/80], Loss: 0.0430\n",
      "Epoch [50/80], Loss: 0.0426\n",
      "Epoch [51/80], Loss: 0.0432\n",
      "Epoch [52/80], Loss: 0.0731\n",
      "Epoch [53/80], Loss: 0.0423\n",
      "Epoch [54/80], Loss: 0.0419\n",
      "Epoch [55/80], Loss: 0.0431\n",
      "Epoch [56/80], Loss: 0.0424\n",
      "Epoch [57/80], Loss: 0.0440\n",
      "Epoch [58/80], Loss: 0.0411\n",
      "Epoch [59/80], Loss: 0.0412\n",
      "Epoch [60/80], Loss: 0.0410\n",
      "Epoch [61/80], Loss: 0.0413\n",
      "Epoch [62/80], Loss: 0.1597\n",
      "Epoch [63/80], Loss: 0.0408\n",
      "Epoch [64/80], Loss: 0.0402\n",
      "Epoch [65/80], Loss: 0.0796\n",
      "Epoch [66/80], Loss: 0.0401\n",
      "Epoch [67/80], Loss: 0.0427\n",
      "Epoch [68/80], Loss: 0.0480\n",
      "Epoch [69/80], Loss: 0.0519\n",
      "Epoch [70/80], Loss: 0.0600\n",
      "Epoch [71/80], Loss: 0.0396\n",
      "Epoch [72/80], Loss: 0.0401\n",
      "Epoch [73/80], Loss: 0.0390\n",
      "Epoch [74/80], Loss: 0.0399\n",
      "Epoch [75/80], Loss: 0.0412\n",
      "Epoch [76/80], Loss: 0.0386\n",
      "Epoch [77/80], Loss: 0.0384\n",
      "Epoch [78/80], Loss: 0.0419\n",
      "Epoch [79/80], Loss: 0.0390\n",
      "Epoch [80/80], Loss: 0.0457\n",
      "update data..\n",
      "task data norm and number entries: tensor(7351.2915, device='cuda:0') torch.Size([400, 47050])\n",
      "..done\n",
      "test performance :  [96.91999817 94.875      93.52999878 92.21499634 91.05799866 89.97000122\n",
      " 88.98001099 88.04374695 87.20000458  0.        ]\n",
      "individual errors:  [tensor(92.5500, device='cuda:0'), tensor(89.8600, device='cuda:0'), tensor(87.4500, device='cuda:0'), tensor(86.4600, device='cuda:0'), tensor(85.5700, device='cuda:0'), tensor(85.6100, device='cuda:0'), tensor(86.6500, device='cuda:0'), tensor(85.1500, device='cuda:0'), tensor(85.5000, device='cuda:0')]\n",
      "Epoch [1/80], Loss: 0.1399\n",
      "Epoch [2/80], Loss: 0.1254\n",
      "Epoch [3/80], Loss: 0.1099\n",
      "Epoch [4/80], Loss: 0.0977\n",
      "Epoch [5/80], Loss: 0.0911\n",
      "Epoch [6/80], Loss: 0.1480\n",
      "Epoch [7/80], Loss: 0.0782\n",
      "Epoch [8/80], Loss: 0.0757\n",
      "Epoch [9/80], Loss: 0.0712\n",
      "Epoch [10/80], Loss: 0.0696\n",
      "Epoch [11/80], Loss: 0.0678\n",
      "Epoch [12/80], Loss: 0.0644\n",
      "Epoch [13/80], Loss: 0.0623\n",
      "Epoch [14/80], Loss: 0.0610\n",
      "Epoch [15/80], Loss: 0.0594\n",
      "Epoch [16/80], Loss: 0.1158\n",
      "Epoch [17/80], Loss: 0.0632\n",
      "Epoch [18/80], Loss: 0.0573\n",
      "Epoch [19/80], Loss: 0.0684\n",
      "Epoch [20/80], Loss: 0.0622\n",
      "Epoch [21/80], Loss: 0.0572\n",
      "Epoch [22/80], Loss: 0.0539\n",
      "Epoch [23/80], Loss: 0.0547\n",
      "Epoch [24/80], Loss: 0.0597\n",
      "Epoch [25/80], Loss: 0.0523\n",
      "Epoch [26/80], Loss: 0.0528\n",
      "Epoch [27/80], Loss: 0.0541\n",
      "Epoch [28/80], Loss: 0.0608\n",
      "Epoch [29/80], Loss: 0.1071\n",
      "Epoch [30/80], Loss: 0.0497\n",
      "Epoch [31/80], Loss: 0.0494\n",
      "Epoch [32/80], Loss: 0.0518\n",
      "Epoch [33/80], Loss: 0.0557\n",
      "Epoch [34/80], Loss: 0.0487\n",
      "Epoch [35/80], Loss: 0.0487\n",
      "Epoch [36/80], Loss: 0.0752\n",
      "Epoch [37/80], Loss: 0.0518\n",
      "Epoch [38/80], Loss: 0.0468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [39/80], Loss: 0.0490\n",
      "Epoch [40/80], Loss: 0.0462\n",
      "Epoch [41/80], Loss: 0.1208\n",
      "Epoch [42/80], Loss: 0.0456\n",
      "Epoch [43/80], Loss: 0.0515\n",
      "Epoch [44/80], Loss: 0.0451\n",
      "Epoch [45/80], Loss: 0.0462\n",
      "Epoch [46/80], Loss: 0.0456\n",
      "Epoch [47/80], Loss: 0.0829\n",
      "Epoch [48/80], Loss: 0.0610\n",
      "Epoch [49/80], Loss: 0.0440\n",
      "Epoch [50/80], Loss: 0.0438\n",
      "Epoch [51/80], Loss: 0.0453\n",
      "Epoch [52/80], Loss: 0.0457\n",
      "Epoch [53/80], Loss: 0.0432\n",
      "Epoch [54/80], Loss: 0.0438\n",
      "Epoch [55/80], Loss: 0.0447\n",
      "Epoch [56/80], Loss: 0.0433\n",
      "Epoch [57/80], Loss: 0.0426\n",
      "Epoch [58/80], Loss: 0.0422\n",
      "Epoch [59/80], Loss: 0.0579\n",
      "Epoch [60/80], Loss: 0.0457\n",
      "Epoch [61/80], Loss: 0.0433\n",
      "Epoch [62/80], Loss: 0.0453\n",
      "Epoch [63/80], Loss: 0.0442\n",
      "Epoch [64/80], Loss: 0.0764\n",
      "Epoch [65/80], Loss: 0.0422\n",
      "Epoch [66/80], Loss: 0.0569\n",
      "Epoch [67/80], Loss: 0.0409\n",
      "Epoch [68/80], Loss: 0.0565\n",
      "Epoch [69/80], Loss: 0.0408\n",
      "Epoch [70/80], Loss: 0.0429\n",
      "Epoch [71/80], Loss: 0.0575\n",
      "Epoch [72/80], Loss: 0.0691\n",
      "Epoch [73/80], Loss: 0.0529\n",
      "Epoch [74/80], Loss: 0.0400\n",
      "Epoch [75/80], Loss: 0.0628\n",
      "Epoch [76/80], Loss: 0.0510\n",
      "Epoch [77/80], Loss: 0.0394\n",
      "Epoch [78/80], Loss: 0.0423\n",
      "Epoch [79/80], Loss: 0.0508\n",
      "Epoch [80/80], Loss: 0.0393\n",
      "test performance :  [96.91999817 94.875      93.52999878 92.21499634 91.05799866 89.97000122\n",
      " 88.98001099 88.04374695 87.20000458 86.42099762]\n",
      "individual errors:  [tensor(92.4700, device='cuda:0'), tensor(89.3300, device='cuda:0'), tensor(86.9400, device='cuda:0'), tensor(86.1200, device='cuda:0'), tensor(84.9900, device='cuda:0'), tensor(84.6300, device='cuda:0'), tensor(85.2800, device='cuda:0'), tensor(84.8000, device='cuda:0'), tensor(84.5100, device='cuda:0'), tensor(85.1400, device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "resppbig = run_simulation( get_random_feature_model() ,train_loader,test_loader,EWCplusplus(lam=1.5e-3,s=400),num_epochs=num_epochs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resppvery = run_simulation( get_random_feature_model() ,train_loader,test_loader,EWCplusplus(lam=1.5e-3,s=800),num_epochs=num_epochs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 0.1130\n",
      "Epoch [2/20], Loss: 0.0257\n",
      "Epoch [3/20], Loss: 0.0204\n",
      "Epoch [4/20], Loss: 0.0182\n",
      "Epoch [5/20], Loss: 0.0169\n",
      "Epoch [6/20], Loss: 0.0161\n",
      "Epoch [7/20], Loss: 0.0155\n",
      "Epoch [8/20], Loss: 0.0151\n",
      "Epoch [9/20], Loss: 0.0147\n",
      "Epoch [10/20], Loss: 0.0144\n",
      "Epoch [11/20], Loss: 0.0141\n",
      "Epoch [12/20], Loss: 0.0138\n",
      "Epoch [13/20], Loss: 0.0136\n",
      "Epoch [14/20], Loss: 0.0134\n",
      "Epoch [15/20], Loss: 0.0133\n",
      "Epoch [16/20], Loss: 0.0131\n",
      "Epoch [17/20], Loss: 0.0130\n",
      "Epoch [18/20], Loss: 0.0129\n",
      "Epoch [19/20], Loss: 0.0127\n",
      "Epoch [20/20], Loss: 0.0126\n",
      "Epoch [1/20], Loss: 0.0125\n",
      "Epoch [1/20], Loss: 0.1403\n",
      "Epoch [2/20], Loss: 0.0285\n",
      "Epoch [2/20], Loss: 0.0289\n",
      "Epoch [3/20], Loss: 0.0134\n",
      "Epoch [3/20], Loss: 0.0349\n",
      "Epoch [4/20], Loss: 0.0210\n",
      "Epoch [4/20], Loss: 0.0209\n",
      "Epoch [5/20], Loss: 0.0137\n",
      "Epoch [5/20], Loss: 0.0270\n",
      "Epoch [6/20], Loss: 0.0180\n",
      "Epoch [6/20], Loss: 0.0228\n",
      "Epoch [7/20], Loss: 0.0171\n",
      "Epoch [7/20], Loss: 0.0213\n",
      "Epoch [8/20], Loss: 0.0173\n",
      "Epoch [8/20], Loss: 0.0178\n",
      "Epoch [9/20], Loss: 0.0197\n",
      "Epoch [9/20], Loss: 0.0166\n",
      "Epoch [10/20], Loss: 0.0138\n",
      "Epoch [10/20], Loss: 0.0207\n",
      "Epoch [11/20], Loss: 0.0164\n",
      "Epoch [11/20], Loss: 0.0190\n",
      "Epoch [12/20], Loss: 0.0161\n",
      "Epoch [12/20], Loss: 0.0185\n",
      "Epoch [13/20], Loss: 0.0160\n",
      "Epoch [13/20], Loss: 0.0169\n",
      "Epoch [14/20], Loss: 0.0179\n",
      "Epoch [14/20], Loss: 0.0161\n",
      "Epoch [15/20], Loss: 0.0138\n",
      "Epoch [15/20], Loss: 0.0189\n",
      "Epoch [16/20], Loss: 0.0160\n",
      "Epoch [16/20], Loss: 0.0178\n",
      "Epoch [17/20], Loss: 0.0154\n",
      "Epoch [17/20], Loss: 0.0167\n",
      "Epoch [18/20], Loss: 0.0138\n",
      "Epoch [18/20], Loss: 0.0184\n",
      "Epoch [19/20], Loss: 0.0159\n",
      "Epoch [19/20], Loss: 0.0173\n",
      "Epoch [20/20], Loss: 0.0151\n",
      "Epoch [20/20], Loss: 0.0166\n",
      "Epoch [1/20], Loss: 0.0138\n",
      "Epoch [1/20], Loss: 0.1273\n",
      "Epoch [1/20], Loss: 0.0331\n",
      "Epoch [2/20], Loss: 0.0348\n",
      "Epoch [2/20], Loss: 0.0200\n",
      "Epoch [2/20], Loss: 0.0295\n",
      "Epoch [3/20], Loss: 0.0148\n",
      "Epoch [3/20], Loss: 0.0204\n",
      "Epoch [3/20], Loss: 0.0364\n",
      "Epoch [4/20], Loss: 0.0193\n",
      "Epoch [4/20], Loss: 0.0212\n",
      "Epoch [4/20], Loss: 0.0293\n",
      "Epoch [5/20], Loss: 0.0186\n",
      "Epoch [5/20], Loss: 0.0257\n",
      "Epoch [5/20], Loss: 0.0248\n",
      "Epoch [6/20], Loss: 0.0159\n",
      "Epoch [6/20], Loss: 0.0197\n",
      "Epoch [6/20], Loss: 0.0268\n",
      "Epoch [7/20], Loss: 0.0179\n",
      "Epoch [7/20], Loss: 0.0224\n",
      "Epoch [7/20], Loss: 0.0242\n",
      "Epoch [8/20], Loss: 0.0185\n",
      "Epoch [8/20], Loss: 0.0217\n",
      "Epoch [8/20], Loss: 0.0230\n",
      "Epoch [9/20], Loss: 0.0205\n",
      "Epoch [9/20], Loss: 0.0227\n",
      "Epoch [9/20], Loss: 0.0194\n",
      "Epoch [10/20], Loss: 0.0203\n",
      "Epoch [10/20], Loss: 0.0224\n",
      "Epoch [10/20], Loss: 0.0191\n",
      "Epoch [11/20], Loss: 0.0155\n",
      "Epoch [11/20], Loss: 0.0217\n",
      "Epoch [11/20], Loss: 0.0237\n",
      "Epoch [12/20], Loss: 0.0178\n",
      "Epoch [12/20], Loss: 0.0206\n",
      "Epoch [12/20], Loss: 0.0221\n",
      "Epoch [13/20], Loss: 0.0161\n",
      "Epoch [13/20], Loss: 0.0185\n",
      "Epoch [13/20], Loss: 0.0250\n",
      "Epoch [14/20], Loss: 0.0198\n",
      "Epoch [14/20], Loss: 0.0188\n",
      "Epoch [14/20], Loss: 0.0223\n",
      "Epoch [15/20], Loss: 0.0176\n",
      "Epoch [15/20], Loss: 0.0218\n",
      "Epoch [15/20], Loss: 0.0216\n",
      "Epoch [16/20], Loss: 0.0171\n",
      "Epoch [16/20], Loss: 0.0204\n",
      "Epoch [16/20], Loss: 0.0220\n",
      "Epoch [17/20], Loss: 0.0162\n",
      "Epoch [17/20], Loss: 0.0187\n",
      "Epoch [17/20], Loss: 0.0239\n",
      "Epoch [18/20], Loss: 0.0199\n",
      "Epoch [18/20], Loss: 0.0189\n",
      "Epoch [18/20], Loss: 0.0215\n",
      "Epoch [19/20], Loss: 0.0168\n",
      "Epoch [19/20], Loss: 0.0189\n",
      "Epoch [19/20], Loss: 0.0234\n",
      "Epoch [20/20], Loss: 0.0207\n",
      "Epoch [20/20], Loss: 0.0189\n",
      "Epoch [20/20], Loss: 0.0202\n",
      "Epoch [1/20], Loss: 0.1301\n",
      "Epoch [1/20], Loss: 0.0280\n",
      "Epoch [1/20], Loss: 0.0290\n",
      "Epoch [1/20], Loss: 0.0299\n",
      "Epoch [2/20], Loss: 0.0457\n",
      "Epoch [2/20], Loss: 0.0251\n",
      "Epoch [2/20], Loss: 0.0238\n",
      "Epoch [2/20], Loss: 0.0253\n",
      "Epoch [3/20], Loss: 0.0174\n",
      "Epoch [3/20], Loss: 0.0235\n",
      "Epoch [3/20], Loss: 0.0203\n",
      "Epoch [3/20], Loss: 0.0431\n",
      "Epoch [4/20], Loss: 0.0241\n",
      "Epoch [4/20], Loss: 0.0261\n",
      "Epoch [4/20], Loss: 0.0239\n",
      "Epoch [4/20], Loss: 0.0300\n",
      "Epoch [5/20], Loss: 0.0262\n",
      "Epoch [5/20], Loss: 0.0214\n",
      "Epoch [5/20], Loss: 0.0217\n",
      "Epoch [5/20], Loss: 0.0277\n",
      "Epoch [6/20], Loss: 0.0271\n",
      "Epoch [6/20], Loss: 0.0245\n",
      "Epoch [6/20], Loss: 0.0216\n",
      "Epoch [6/20], Loss: 0.0235\n",
      "Epoch [7/20], Loss: 0.0177\n",
      "Epoch [7/20], Loss: 0.0198\n",
      "Epoch [7/20], Loss: 0.0304\n",
      "Epoch [7/20], Loss: 0.0289\n",
      "Epoch [8/20], Loss: 0.0172\n",
      "Epoch [8/20], Loss: 0.0217\n",
      "Epoch [8/20], Loss: 0.0268\n",
      "Epoch [8/20], Loss: 0.0270\n",
      "Epoch [9/20], Loss: 0.0178\n",
      "Epoch [9/20], Loss: 0.0218\n",
      "Epoch [9/20], Loss: 0.0274\n",
      "Epoch [9/20], Loss: 0.0266\n",
      "Epoch [10/20], Loss: 0.0207\n",
      "Epoch [10/20], Loss: 0.0234\n",
      "Epoch [10/20], Loss: 0.0243\n",
      "Epoch [10/20], Loss: 0.0262\n",
      "Epoch [11/20], Loss: 0.0240\n",
      "Epoch [11/20], Loss: 0.0208\n",
      "Epoch [11/20], Loss: 0.0235\n",
      "Epoch [11/20], Loss: 0.0242\n",
      "Epoch [12/20], Loss: 0.0233\n",
      "Epoch [12/20], Loss: 0.0254\n",
      "Epoch [12/20], Loss: 0.0207\n",
      "Epoch [12/20], Loss: 0.0245\n",
      "Epoch [13/20], Loss: 0.0230\n",
      "Epoch [13/20], Loss: 0.0201\n",
      "Epoch [13/20], Loss: 0.0220\n",
      "Epoch [13/20], Loss: 0.0274\n",
      "Epoch [14/20], Loss: 0.0199\n",
      "Epoch [14/20], Loss: 0.0240\n",
      "Epoch [14/20], Loss: 0.0236\n",
      "Epoch [14/20], Loss: 0.0245\n",
      "Epoch [15/20], Loss: 0.0212\n",
      "Epoch [15/20], Loss: 0.0226\n",
      "Epoch [15/20], Loss: 0.0212\n",
      "Epoch [15/20], Loss: 0.0236\n",
      "Epoch [16/20], Loss: 0.0239\n",
      "Epoch [16/20], Loss: 0.0193\n",
      "Epoch [16/20], Loss: 0.0239\n",
      "Epoch [16/20], Loss: 0.0249\n",
      "Epoch [17/20], Loss: 0.0223\n",
      "Epoch [17/20], Loss: 0.0204\n",
      "Epoch [17/20], Loss: 0.0211\n",
      "Epoch [17/20], Loss: 0.0250\n",
      "Epoch [18/20], Loss: 0.0171\n",
      "Epoch [18/20], Loss: 0.0221\n",
      "Epoch [18/20], Loss: 0.0249\n",
      "Epoch [18/20], Loss: 0.0243\n",
      "Epoch [19/20], Loss: 0.0177\n",
      "Epoch [19/20], Loss: 0.0241\n",
      "Epoch [19/20], Loss: 0.0242\n",
      "Epoch [19/20], Loss: 0.0268\n",
      "Epoch [20/20], Loss: 0.0200\n",
      "Epoch [20/20], Loss: 0.0198\n",
      "Epoch [20/20], Loss: 0.0234\n",
      "Epoch [20/20], Loss: 0.0261\n",
      "Epoch [1/20], Loss: 0.0217\n",
      "Epoch [1/20], Loss: 0.1261\n",
      "Epoch [1/20], Loss: 0.0290\n",
      "Epoch [1/20], Loss: 0.0328\n",
      "Epoch [1/20], Loss: 0.0342\n",
      "Epoch [2/20], Loss: 0.0270\n",
      "Epoch [2/20], Loss: 0.0202\n",
      "Epoch [2/20], Loss: 0.0525\n",
      "Epoch [2/20], Loss: 0.0279\n",
      "Epoch [2/20], Loss: 0.0301\n",
      "Epoch [3/20], Loss: 0.0269\n",
      "Epoch [3/20], Loss: 0.0206\n",
      "Epoch [3/20], Loss: 0.0288\n",
      "Epoch [3/20], Loss: 0.0256\n",
      "Epoch [3/20], Loss: 0.0430\n",
      "Epoch [4/20], Loss: 0.0238\n",
      "Epoch [4/20], Loss: 0.0296\n",
      "Epoch [4/20], Loss: 0.0247\n",
      "Epoch [4/20], Loss: 0.0288\n",
      "Epoch [4/20], Loss: 0.0350\n",
      "Epoch [5/20], Loss: 0.0202\n",
      "Epoch [5/20], Loss: 0.0253\n",
      "Epoch [5/20], Loss: 0.0320\n",
      "Epoch [5/20], Loss: 0.0319\n",
      "Epoch [5/20], Loss: 0.0287\n",
      "Epoch [6/20], Loss: 0.0215\n",
      "Epoch [6/20], Loss: 0.0210\n",
      "Epoch [6/20], Loss: 0.0287\n",
      "Epoch [6/20], Loss: 0.0297\n",
      "Epoch [6/20], Loss: 0.0389\n",
      "Epoch [7/20], Loss: 0.0232\n",
      "Epoch [7/20], Loss: 0.0281\n",
      "Epoch [7/20], Loss: 0.0253\n",
      "Epoch [7/20], Loss: 0.0294\n",
      "Epoch [7/20], Loss: 0.0295\n",
      "Epoch [8/20], Loss: 0.0237\n",
      "Epoch [8/20], Loss: 0.0277\n",
      "Epoch [8/20], Loss: 0.0280\n",
      "Epoch [8/20], Loss: 0.0302\n",
      "Epoch [8/20], Loss: 0.0278\n",
      "Epoch [9/20], Loss: 0.0208\n",
      "Epoch [9/20], Loss: 0.0276\n",
      "Epoch [9/20], Loss: 0.0321\n",
      "Epoch [9/20], Loss: 0.0261\n",
      "Epoch [9/20], Loss: 0.0264\n",
      "Epoch [10/20], Loss: 0.0215\n",
      "Epoch [10/20], Loss: 0.0269\n",
      "Epoch [10/20], Loss: 0.0307\n",
      "Epoch [10/20], Loss: 0.0248\n",
      "Epoch [10/20], Loss: 0.0332\n",
      "Epoch [11/20], Loss: 0.0215\n",
      "Epoch [11/20], Loss: 0.0282\n",
      "Epoch [11/20], Loss: 0.0237\n",
      "Epoch [11/20], Loss: 0.0289\n",
      "Epoch [11/20], Loss: 0.0328\n",
      "Epoch [12/20], Loss: 0.0188\n",
      "Epoch [12/20], Loss: 0.0273\n",
      "Epoch [12/20], Loss: 0.0282\n",
      "Epoch [12/20], Loss: 0.0277\n",
      "Epoch [12/20], Loss: 0.0313\n",
      "Epoch [13/20], Loss: 0.0217\n",
      "Epoch [13/20], Loss: 0.0251\n",
      "Epoch [13/20], Loss: 0.0238\n",
      "Epoch [13/20], Loss: 0.0317\n",
      "Epoch [13/20], Loss: 0.0309\n",
      "Epoch [14/20], Loss: 0.0233\n",
      "Epoch [14/20], Loss: 0.0254\n",
      "Epoch [14/20], Loss: 0.0233\n",
      "Epoch [14/20], Loss: 0.0283\n",
      "Epoch [14/20], Loss: 0.0356\n",
      "Epoch [15/20], Loss: 0.0269\n",
      "Epoch [15/20], Loss: 0.0217\n",
      "Epoch [15/20], Loss: 0.0282\n",
      "Epoch [15/20], Loss: 0.0283\n",
      "Epoch [15/20], Loss: 0.0286\n",
      "Epoch [16/20], Loss: 0.0248\n",
      "Epoch [16/20], Loss: 0.0286\n",
      "Epoch [16/20], Loss: 0.0257\n",
      "Epoch [16/20], Loss: 0.0257\n",
      "Epoch [16/20], Loss: 0.0263\n",
      "Epoch [17/20], Loss: 0.0250\n",
      "Epoch [17/20], Loss: 0.0221\n",
      "Epoch [17/20], Loss: 0.0252\n",
      "Epoch [17/20], Loss: 0.0340\n",
      "Epoch [17/20], Loss: 0.0309\n",
      "Epoch [18/20], Loss: 0.0218\n",
      "Epoch [18/20], Loss: 0.0217\n",
      "Epoch [18/20], Loss: 0.0247\n",
      "Epoch [18/20], Loss: 0.0304\n",
      "Epoch [18/20], Loss: 0.0350\n",
      "Epoch [19/20], Loss: 0.0245\n",
      "Epoch [19/20], Loss: 0.0278\n",
      "Epoch [19/20], Loss: 0.0247\n",
      "Epoch [19/20], Loss: 0.0265\n",
      "Epoch [19/20], Loss: 0.0310\n",
      "Epoch [20/20], Loss: 0.0204\n",
      "Epoch [20/20], Loss: 0.0249\n",
      "Epoch [20/20], Loss: 0.0246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/20], Loss: 0.0282\n",
      "Epoch [20/20], Loss: 0.0350\n",
      "Epoch [1/20], Loss: 0.0195\n",
      "Epoch [1/20], Loss: 0.0255\n",
      "Epoch [1/20], Loss: 0.0312\n",
      "Epoch [1/20], Loss: 0.1160\n",
      "Epoch [1/20], Loss: 0.0355\n",
      "Epoch [1/20], Loss: 0.0357\n",
      "Epoch [2/20], Loss: 0.0265\n",
      "Epoch [2/20], Loss: 0.0219\n",
      "Epoch [2/20], Loss: 0.0407\n",
      "Epoch [2/20], Loss: 0.0260\n",
      "Epoch [2/20], Loss: 0.0539\n",
      "Epoch [2/20], Loss: 0.0399\n",
      "Epoch [3/20], Loss: 0.0249\n",
      "Epoch [3/20], Loss: 0.0308\n",
      "Epoch [3/20], Loss: 0.0240\n",
      "Epoch [3/20], Loss: 0.0310\n",
      "Epoch [3/20], Loss: 0.0395\n",
      "Epoch [3/20], Loss: 0.0455\n",
      "Epoch [4/20], Loss: 0.0294\n",
      "Epoch [4/20], Loss: 0.0274\n",
      "Epoch [4/20], Loss: 0.0290\n",
      "Epoch [4/20], Loss: 0.0355\n",
      "Epoch [4/20], Loss: 0.0295\n",
      "Epoch [4/20], Loss: 0.0361\n",
      "Epoch [5/20], Loss: 0.0191\n",
      "Epoch [5/20], Loss: 0.0354\n",
      "Epoch [5/20], Loss: 0.0321\n",
      "Epoch [5/20], Loss: 0.0298\n",
      "Epoch [5/20], Loss: 0.0355\n",
      "Epoch [5/20], Loss: 0.0334\n",
      "Epoch [6/20], Loss: 0.0239\n",
      "Epoch [6/20], Loss: 0.0216\n",
      "Epoch [6/20], Loss: 0.0310\n",
      "Epoch [6/20], Loss: 0.0363\n",
      "Epoch [6/20], Loss: 0.0307\n",
      "Epoch [6/20], Loss: 0.0421\n",
      "Epoch [7/20], Loss: 0.0232\n",
      "Epoch [7/20], Loss: 0.0303\n",
      "Epoch [7/20], Loss: 0.0331\n",
      "Epoch [7/20], Loss: 0.0330\n",
      "Epoch [7/20], Loss: 0.0302\n",
      "Epoch [7/20], Loss: 0.0344\n",
      "Epoch [8/20], Loss: 0.0265\n",
      "Epoch [8/20], Loss: 0.0314\n",
      "Epoch [8/20], Loss: 0.0310\n",
      "Epoch [8/20], Loss: 0.0297\n",
      "Epoch [8/20], Loss: 0.0316\n",
      "Epoch [8/20], Loss: 0.0311\n",
      "Epoch [9/20], Loss: 0.0279\n",
      "Epoch [9/20], Loss: 0.0231\n",
      "Epoch [9/20], Loss: 0.0289\n",
      "Epoch [9/20], Loss: 0.0297\n",
      "Epoch [9/20], Loss: 0.0294\n",
      "Epoch [9/20], Loss: 0.0438\n",
      "Epoch [10/20], Loss: 0.0275\n",
      "Epoch [10/20], Loss: 0.0302\n",
      "Epoch [10/20], Loss: 0.0256\n",
      "Epoch [10/20], Loss: 0.0275\n",
      "Epoch [10/20], Loss: 0.0364\n",
      "Epoch [10/20], Loss: 0.0353\n",
      "Epoch [11/20], Loss: 0.0271\n",
      "Epoch [11/20], Loss: 0.0240\n",
      "Epoch [11/20], Loss: 0.0288\n",
      "Epoch [11/20], Loss: 0.0311\n",
      "Epoch [11/20], Loss: 0.0386\n",
      "Epoch [11/20], Loss: 0.0308\n",
      "Epoch [12/20], Loss: 0.0264\n",
      "Epoch [12/20], Loss: 0.0262\n",
      "Epoch [12/20], Loss: 0.0242\n",
      "Epoch [12/20], Loss: 0.0285\n",
      "Epoch [12/20], Loss: 0.0336\n",
      "Epoch [12/20], Loss: 0.0377\n",
      "Epoch [13/20], Loss: 0.0241\n",
      "Epoch [13/20], Loss: 0.0358\n",
      "Epoch [13/20], Loss: 0.0254\n",
      "Epoch [13/20], Loss: 0.0313\n",
      "Epoch [13/20], Loss: 0.0326\n",
      "Epoch [13/20], Loss: 0.0339\n",
      "Epoch [14/20], Loss: 0.0228\n",
      "Epoch [14/20], Loss: 0.0212\n",
      "Epoch [14/20], Loss: 0.0331\n",
      "Epoch [14/20], Loss: 0.0317\n",
      "Epoch [14/20], Loss: 0.0344\n",
      "Epoch [14/20], Loss: 0.0355\n",
      "Epoch [15/20], Loss: 0.0279\n",
      "Epoch [15/20], Loss: 0.0235\n",
      "Epoch [15/20], Loss: 0.0325\n",
      "Epoch [15/20], Loss: 0.0305\n",
      "Epoch [15/20], Loss: 0.0298\n",
      "Epoch [15/20], Loss: 0.0359\n",
      "Epoch [16/20], Loss: 0.0197\n",
      "Epoch [16/20], Loss: 0.0240\n",
      "Epoch [16/20], Loss: 0.0303\n",
      "Epoch [16/20], Loss: 0.0336\n",
      "Epoch [16/20], Loss: 0.0306\n",
      "Epoch [16/20], Loss: 0.0413\n",
      "Epoch [17/20], Loss: 0.0309\n",
      "Epoch [17/20], Loss: 0.0226\n",
      "Epoch [17/20], Loss: 0.0286\n",
      "Epoch [17/20], Loss: 0.0335\n",
      "Epoch [17/20], Loss: 0.0320\n",
      "Epoch [17/20], Loss: 0.0323\n",
      "Epoch [18/20], Loss: 0.0240\n",
      "Epoch [18/20], Loss: 0.0303\n",
      "Epoch [18/20], Loss: 0.0342\n",
      "Epoch [18/20], Loss: 0.0310\n",
      "Epoch [18/20], Loss: 0.0291\n",
      "Epoch [18/20], Loss: 0.0309\n",
      "Epoch [19/20], Loss: 0.0267\n",
      "Epoch [19/20], Loss: 0.0309\n",
      "Epoch [19/20], Loss: 0.0256\n",
      "Epoch [19/20], Loss: 0.0278\n",
      "Epoch [19/20], Loss: 0.0284\n",
      "Epoch [19/20], Loss: 0.0361\n",
      "Epoch [20/20], Loss: 0.0320\n",
      "Epoch [20/20], Loss: 0.0264\n",
      "Epoch [20/20], Loss: 0.0284\n",
      "Epoch [20/20], Loss: 0.0272\n",
      "Epoch [20/20], Loss: 0.0352\n",
      "Epoch [20/20], Loss: 0.0310\n",
      "Epoch [1/20], Loss: 0.1267\n",
      "Epoch [1/20], Loss: 0.0412\n",
      "Epoch [1/20], Loss: 0.0357\n",
      "Epoch [1/20], Loss: 0.0405\n",
      "Epoch [1/20], Loss: 0.0307\n",
      "Epoch [1/20], Loss: 0.0332\n",
      "Epoch [1/20], Loss: 0.0392\n",
      "Epoch [2/20], Loss: 0.0235\n",
      "Epoch [2/20], Loss: 0.0252\n",
      "Epoch [2/20], Loss: 0.0684\n",
      "Epoch [2/20], Loss: 0.0435\n",
      "Epoch [2/20], Loss: 0.0310\n",
      "Epoch [2/20], Loss: 0.0378\n",
      "Epoch [2/20], Loss: 0.0441\n",
      "Epoch [3/20], Loss: 0.0288\n",
      "Epoch [3/20], Loss: 0.0313\n",
      "Epoch [3/20], Loss: 0.0461\n",
      "Epoch [3/20], Loss: 0.0325\n",
      "Epoch [3/20], Loss: 0.0362\n",
      "Epoch [3/20], Loss: 0.0402\n",
      "Epoch [3/20], Loss: 0.0395\n",
      "Epoch [4/20], Loss: 0.0240\n",
      "Epoch [4/20], Loss: 0.0277\n",
      "Epoch [4/20], Loss: 0.0355\n",
      "Epoch [4/20], Loss: 0.0266\n",
      "Epoch [4/20], Loss: 0.0379\n",
      "Epoch [4/20], Loss: 0.0483\n",
      "Epoch [4/20], Loss: 0.0403\n",
      "Epoch [5/20], Loss: 0.0336\n",
      "Epoch [5/20], Loss: 0.0326\n",
      "Epoch [5/20], Loss: 0.0340\n",
      "Epoch [5/20], Loss: 0.0263\n",
      "Epoch [5/20], Loss: 0.0310\n",
      "Epoch [5/20], Loss: 0.0385\n",
      "Epoch [5/20], Loss: 0.0500\n",
      "Epoch [6/20], Loss: 0.0341\n",
      "Epoch [6/20], Loss: 0.0308\n",
      "Epoch [6/20], Loss: 0.0354\n",
      "Epoch [6/20], Loss: 0.0272\n",
      "Epoch [6/20], Loss: 0.0335\n",
      "Epoch [6/20], Loss: 0.0374\n",
      "Epoch [6/20], Loss: 0.0346\n",
      "Epoch [7/20], Loss: 0.0395\n",
      "Epoch [7/20], Loss: 0.0337\n",
      "Epoch [7/20], Loss: 0.0266\n",
      "Epoch [7/20], Loss: 0.0322\n",
      "Epoch [7/20], Loss: 0.0314\n",
      "Epoch [7/20], Loss: 0.0399\n",
      "Epoch [7/20], Loss: 0.0436\n",
      "Epoch [8/20], Loss: 0.0284\n",
      "Epoch [8/20], Loss: 0.0356\n",
      "Epoch [8/20], Loss: 0.0254\n",
      "Epoch [8/20], Loss: 0.0336\n",
      "Epoch [8/20], Loss: 0.0454\n",
      "Epoch [8/20], Loss: 0.0347\n",
      "Epoch [8/20], Loss: 0.0345\n",
      "Epoch [9/20], Loss: 0.0231\n",
      "Epoch [9/20], Loss: 0.0288\n",
      "Epoch [9/20], Loss: 0.0358\n",
      "Epoch [9/20], Loss: 0.0346\n",
      "Epoch [9/20], Loss: 0.0295\n",
      "Epoch [9/20], Loss: 0.0350\n",
      "Epoch [9/20], Loss: 0.0435\n",
      "Epoch [10/20], Loss: 0.0243\n",
      "Epoch [10/20], Loss: 0.0268\n",
      "Epoch [10/20], Loss: 0.0441\n",
      "Epoch [10/20], Loss: 0.0280\n",
      "Epoch [10/20], Loss: 0.0397\n",
      "Epoch [10/20], Loss: 0.0380\n",
      "Epoch [10/20], Loss: 0.0418\n",
      "Epoch [11/20], Loss: 0.0293\n",
      "Epoch [11/20], Loss: 0.0271\n",
      "Epoch [11/20], Loss: 0.0263\n",
      "Epoch [11/20], Loss: 0.0319\n",
      "Epoch [11/20], Loss: 0.0313\n",
      "Epoch [11/20], Loss: 0.0412\n",
      "Epoch [11/20], Loss: 0.0470\n",
      "Epoch [12/20], Loss: 0.0249\n",
      "Epoch [12/20], Loss: 0.0366\n",
      "Epoch [12/20], Loss: 0.0262\n",
      "Epoch [12/20], Loss: 0.0335\n",
      "Epoch [12/20], Loss: 0.0350\n",
      "Epoch [12/20], Loss: 0.0398\n",
      "Epoch [12/20], Loss: 0.0415\n",
      "Epoch [13/20], Loss: 0.0316\n",
      "Epoch [13/20], Loss: 0.0287\n",
      "Epoch [13/20], Loss: 0.0272\n",
      "Epoch [13/20], Loss: 0.0288\n",
      "Epoch [13/20], Loss: 0.0388\n",
      "Epoch [13/20], Loss: 0.0352\n",
      "Epoch [13/20], Loss: 0.0446\n",
      "Epoch [14/20], Loss: 0.0256\n",
      "Epoch [14/20], Loss: 0.0299\n",
      "Epoch [14/20], Loss: 0.0312\n",
      "Epoch [14/20], Loss: 0.0356\n",
      "Epoch [14/20], Loss: 0.0312\n",
      "Epoch [14/20], Loss: 0.0342\n",
      "Epoch [14/20], Loss: 0.0519\n",
      "Epoch [15/20], Loss: 0.0307\n",
      "Epoch [15/20], Loss: 0.0282\n",
      "Epoch [15/20], Loss: 0.0281\n",
      "Epoch [15/20], Loss: 0.0298\n",
      "Epoch [15/20], Loss: 0.0402\n",
      "Epoch [15/20], Loss: 0.0356\n",
      "Epoch [15/20], Loss: 0.0410\n",
      "Epoch [16/20], Loss: 0.0311\n",
      "Epoch [16/20], Loss: 0.0258\n",
      "Epoch [16/20], Loss: 0.0294\n",
      "Epoch [16/20], Loss: 0.0395\n",
      "Epoch [16/20], Loss: 0.0343\n",
      "Epoch [16/20], Loss: 0.0392\n",
      "Epoch [16/20], Loss: 0.0329\n",
      "Epoch [17/20], Loss: 0.0283\n",
      "Epoch [17/20], Loss: 0.0335\n",
      "Epoch [17/20], Loss: 0.0310\n",
      "Epoch [17/20], Loss: 0.0367\n",
      "Epoch [17/20], Loss: 0.0326\n",
      "Epoch [17/20], Loss: 0.0363\n",
      "Epoch [17/20], Loss: 0.0343\n",
      "Epoch [18/20], Loss: 0.0302\n",
      "Epoch [18/20], Loss: 0.0317\n",
      "Epoch [18/20], Loss: 0.0284\n",
      "Epoch [18/20], Loss: 0.0266\n",
      "Epoch [18/20], Loss: 0.0313\n",
      "Epoch [18/20], Loss: 0.0389\n",
      "Epoch [18/20], Loss: 0.0532\n",
      "Epoch [19/20], Loss: 0.0348\n",
      "Epoch [19/20], Loss: 0.0244\n",
      "Epoch [19/20], Loss: 0.0340\n",
      "Epoch [19/20], Loss: 0.0320\n",
      "Epoch [19/20], Loss: 0.0333\n",
      "Epoch [19/20], Loss: 0.0378\n",
      "Epoch [19/20], Loss: 0.0409\n",
      "Epoch [20/20], Loss: 0.0317\n",
      "Epoch [20/20], Loss: 0.0300\n",
      "Epoch [20/20], Loss: 0.0290\n",
      "Epoch [20/20], Loss: 0.0282\n",
      "Epoch [20/20], Loss: 0.0316\n",
      "Epoch [20/20], Loss: 0.0501\n",
      "Epoch [20/20], Loss: 0.0375\n",
      "Epoch [1/20], Loss: 0.1279\n",
      "Epoch [1/20], Loss: 0.0385\n",
      "Epoch [1/20], Loss: 0.0273\n",
      "Epoch [1/20], Loss: 0.0366\n",
      "Epoch [1/20], Loss: 0.0414\n",
      "Epoch [1/20], Loss: 0.0331\n",
      "Epoch [1/20], Loss: 0.0433\n",
      "Epoch [1/20], Loss: 0.0431\n",
      "Epoch [2/20], Loss: 0.0247\n",
      "Epoch [2/20], Loss: 0.0370\n",
      "Epoch [2/20], Loss: 0.0251\n",
      "Epoch [2/20], Loss: 0.0399\n",
      "Epoch [2/20], Loss: 0.0350\n",
      "Epoch [2/20], Loss: 0.0356\n",
      "Epoch [2/20], Loss: 0.0820\n",
      "Epoch [2/20], Loss: 0.0439\n",
      "Epoch [3/20], Loss: 0.0350\n",
      "Epoch [3/20], Loss: 0.0337\n",
      "Epoch [3/20], Loss: 0.0258\n",
      "Epoch [3/20], Loss: 0.0403\n",
      "Epoch [3/20], Loss: 0.0476\n",
      "Epoch [3/20], Loss: 0.0402\n",
      "Epoch [3/20], Loss: 0.0458\n",
      "Epoch [3/20], Loss: 0.0420\n",
      "Epoch [4/20], Loss: 0.0269\n",
      "Epoch [4/20], Loss: 0.0241\n",
      "Epoch [4/20], Loss: 0.0364\n",
      "Epoch [4/20], Loss: 0.0390\n",
      "Epoch [4/20], Loss: 0.0404\n",
      "Epoch [4/20], Loss: 0.0382\n",
      "Epoch [4/20], Loss: 0.0504\n",
      "Epoch [4/20], Loss: 0.0471\n",
      "Epoch [5/20], Loss: 0.0373\n",
      "Epoch [5/20], Loss: 0.0347\n",
      "Epoch [5/20], Loss: 0.0302\n",
      "Epoch [5/20], Loss: 0.0285\n",
      "Epoch [5/20], Loss: 0.0526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/20], Loss: 0.0388\n",
      "Epoch [5/20], Loss: 0.0408\n",
      "Epoch [5/20], Loss: 0.0458\n",
      "Epoch [6/20], Loss: 0.0236\n",
      "Epoch [6/20], Loss: 0.0276\n",
      "Epoch [6/20], Loss: 0.0452\n",
      "Epoch [6/20], Loss: 0.0371\n",
      "Epoch [6/20], Loss: 0.0305\n",
      "Epoch [6/20], Loss: 0.0410\n",
      "Epoch [6/20], Loss: 0.0437\n",
      "Epoch [6/20], Loss: 0.0511\n",
      "Epoch [7/20], Loss: 0.0276\n",
      "Epoch [7/20], Loss: 0.0252\n",
      "Epoch [7/20], Loss: 0.0308\n",
      "Epoch [7/20], Loss: 0.0379\n",
      "Epoch [7/20], Loss: 0.0342\n",
      "Epoch [7/20], Loss: 0.0407\n",
      "Epoch [7/20], Loss: 0.0503\n",
      "Epoch [7/20], Loss: 0.0450\n",
      "Epoch [8/20], Loss: 0.0398\n",
      "Epoch [8/20], Loss: 0.0301\n",
      "Epoch [8/20], Loss: 0.0335\n",
      "Epoch [8/20], Loss: 0.0267\n",
      "Epoch [8/20], Loss: 0.0437\n",
      "Epoch [8/20], Loss: 0.0425\n",
      "Epoch [8/20], Loss: 0.0356\n",
      "Epoch [8/20], Loss: 0.0416\n",
      "Epoch [9/20], Loss: 0.0232\n",
      "Epoch [9/20], Loss: 0.0323\n",
      "Epoch [9/20], Loss: 0.0338\n",
      "Epoch [9/20], Loss: 0.0486\n",
      "Epoch [9/20], Loss: 0.0386\n",
      "Epoch [9/20], Loss: 0.0486\n",
      "Epoch [9/20], Loss: 0.0484\n",
      "Epoch [9/20], Loss: 0.0390\n",
      "Epoch [10/20], Loss: 0.0227\n",
      "Epoch [10/20], Loss: 0.0290\n",
      "Epoch [10/20], Loss: 0.0379\n",
      "Epoch [10/20], Loss: 0.0445\n",
      "Epoch [10/20], Loss: 0.0399\n",
      "Epoch [10/20], Loss: 0.0403\n",
      "Epoch [10/20], Loss: 0.0430\n",
      "Epoch [10/20], Loss: 0.0432\n",
      "Epoch [11/20], Loss: 0.0261\n",
      "Epoch [11/20], Loss: 0.0259\n",
      "Epoch [11/20], Loss: 0.0440\n",
      "Epoch [11/20], Loss: 0.0288\n",
      "Epoch [11/20], Loss: 0.0392\n",
      "Epoch [11/20], Loss: 0.0459\n",
      "Epoch [11/20], Loss: 0.0420\n",
      "Epoch [11/20], Loss: 0.0436\n",
      "Epoch [12/20], Loss: 0.0354\n",
      "Epoch [12/20], Loss: 0.0329\n",
      "Epoch [12/20], Loss: 0.0291\n",
      "Epoch [12/20], Loss: 0.0397\n",
      "Epoch [12/20], Loss: 0.0389\n",
      "Epoch [12/20], Loss: 0.0357\n",
      "Epoch [12/20], Loss: 0.0434\n",
      "Epoch [12/20], Loss: 0.0433\n",
      "Epoch [13/20], Loss: 0.0381\n",
      "Epoch [13/20], Loss: 0.0433\n",
      "Epoch [13/20], Loss: 0.0334\n",
      "Epoch [13/20], Loss: 0.0323\n",
      "Epoch [13/20], Loss: 0.0365\n",
      "Epoch [13/20], Loss: 0.0414\n",
      "Epoch [13/20], Loss: 0.0362\n",
      "Epoch [13/20], Loss: 0.0414\n",
      "Epoch [14/20], Loss: 0.0277\n",
      "Epoch [14/20], Loss: 0.0302\n",
      "Epoch [14/20], Loss: 0.0272\n",
      "Epoch [14/20], Loss: 0.0388\n",
      "Epoch [14/20], Loss: 0.0388\n",
      "Epoch [14/20], Loss: 0.0567\n",
      "Epoch [14/20], Loss: 0.0474\n",
      "Epoch [14/20], Loss: 0.0375\n",
      "Epoch [15/20], Loss: 0.0278\n",
      "Epoch [15/20], Loss: 0.0321\n",
      "Epoch [15/20], Loss: 0.0274\n",
      "Epoch [15/20], Loss: 0.0406\n",
      "Epoch [15/20], Loss: 0.0410\n",
      "Epoch [15/20], Loss: 0.0391\n",
      "Epoch [15/20], Loss: 0.0409\n",
      "Epoch [15/20], Loss: 0.0378\n",
      "Epoch [16/20], Loss: 0.0349\n",
      "Epoch [16/20], Loss: 0.0227\n",
      "Epoch [16/20], Loss: 0.0255\n",
      "Epoch [16/20], Loss: 0.0397\n",
      "Epoch [16/20], Loss: 0.0347\n",
      "Epoch [16/20], Loss: 0.0585\n",
      "Epoch [16/20], Loss: 0.0433\n",
      "Epoch [16/20], Loss: 0.0433\n",
      "Epoch [17/20], Loss: 0.0363\n",
      "Epoch [17/20], Loss: 0.0319\n",
      "Epoch [17/20], Loss: 0.0287\n",
      "Epoch [17/20], Loss: 0.0374\n",
      "Epoch [17/20], Loss: 0.0319\n",
      "Epoch [17/20], Loss: 0.0450\n",
      "Epoch [17/20], Loss: 0.0413\n",
      "Epoch [17/20], Loss: 0.0422\n",
      "Epoch [18/20], Loss: 0.0267\n",
      "Epoch [18/20], Loss: 0.0384\n",
      "Epoch [18/20], Loss: 0.0356\n",
      "Epoch [18/20], Loss: 0.0390\n",
      "Epoch [18/20], Loss: 0.0344\n",
      "Epoch [18/20], Loss: 0.0343\n",
      "Epoch [18/20], Loss: 0.0470\n",
      "Epoch [18/20], Loss: 0.0439\n",
      "Epoch [19/20], Loss: 0.0343\n",
      "Epoch [19/20], Loss: 0.0329\n",
      "Epoch [19/20], Loss: 0.0322\n",
      "Epoch [19/20], Loss: 0.0479\n",
      "Epoch [19/20], Loss: 0.0423\n",
      "Epoch [19/20], Loss: 0.0366\n",
      "Epoch [19/20], Loss: 0.0371\n",
      "Epoch [19/20], Loss: 0.0392\n",
      "Epoch [20/20], Loss: 0.0321\n",
      "Epoch [20/20], Loss: 0.0314\n",
      "Epoch [20/20], Loss: 0.0411\n",
      "Epoch [20/20], Loss: 0.0360\n",
      "Epoch [20/20], Loss: 0.0424\n",
      "Epoch [20/20], Loss: 0.0337\n",
      "Epoch [20/20], Loss: 0.0359\n",
      "Epoch [20/20], Loss: 0.0392\n",
      "Epoch [1/20], Loss: 0.0291\n",
      "Epoch [1/20], Loss: 0.1239\n",
      "Epoch [1/20], Loss: 0.0527\n",
      "Epoch [1/20], Loss: 0.0454\n",
      "Epoch [1/20], Loss: 0.0348\n",
      "Epoch [1/20], Loss: 0.0395\n",
      "Epoch [1/20], Loss: 0.0384\n",
      "Epoch [1/20], Loss: 0.0438\n",
      "Epoch [1/20], Loss: 0.0544\n",
      "Epoch [2/20], Loss: 0.0281\n",
      "Epoch [2/20], Loss: 0.0285\n",
      "Epoch [2/20], Loss: 0.0315\n",
      "Epoch [2/20], Loss: 0.0299\n",
      "Epoch [2/20], Loss: 0.0404\n",
      "Epoch [2/20], Loss: 0.0364\n",
      "Epoch [2/20], Loss: 0.0776\n",
      "Epoch [2/20], Loss: 0.0517\n",
      "Epoch [2/20], Loss: 0.0664\n",
      "Epoch [3/20], Loss: 0.0384\n",
      "Epoch [3/20], Loss: 0.0407\n",
      "Epoch [3/20], Loss: 0.0373\n",
      "Epoch [3/20], Loss: 0.0340\n",
      "Epoch [3/20], Loss: 0.0437\n",
      "Epoch [3/20], Loss: 0.0395\n",
      "Epoch [3/20], Loss: 0.0566\n",
      "Epoch [3/20], Loss: 0.0489\n",
      "Epoch [3/20], Loss: 0.0444\n",
      "Epoch [4/20], Loss: 0.0286\n",
      "Epoch [4/20], Loss: 0.0381\n",
      "Epoch [4/20], Loss: 0.0394\n",
      "Epoch [4/20], Loss: 0.0506\n",
      "Epoch [4/20], Loss: 0.0338\n",
      "Epoch [4/20], Loss: 0.0482\n",
      "Epoch [4/20], Loss: 0.0407\n",
      "Epoch [4/20], Loss: 0.0480\n",
      "Epoch [4/20], Loss: 0.0515\n",
      "Epoch [5/20], Loss: 0.0372\n",
      "Epoch [5/20], Loss: 0.0425\n",
      "Epoch [5/20], Loss: 0.0405\n",
      "Epoch [5/20], Loss: 0.0347\n",
      "Epoch [5/20], Loss: 0.0392\n",
      "Epoch [5/20], Loss: 0.0354\n",
      "Epoch [5/20], Loss: 0.0503\n",
      "Epoch [5/20], Loss: 0.0462\n",
      "Epoch [5/20], Loss: 0.0443\n",
      "Epoch [6/20], Loss: 0.0372\n",
      "Epoch [6/20], Loss: 0.0302\n",
      "Epoch [6/20], Loss: 0.0313\n",
      "Epoch [6/20], Loss: 0.0490\n",
      "Epoch [6/20], Loss: 0.0387\n",
      "Epoch [6/20], Loss: 0.0319\n",
      "Epoch [6/20], Loss: 0.0522\n",
      "Epoch [6/20], Loss: 0.0465\n",
      "Epoch [6/20], Loss: 0.0477\n",
      "Epoch [7/20], Loss: 0.0317\n",
      "Epoch [7/20], Loss: 0.0369\n",
      "Epoch [7/20], Loss: 0.0481\n",
      "Epoch [7/20], Loss: 0.0356\n",
      "Epoch [7/20], Loss: 0.0457\n",
      "Epoch [7/20], Loss: 0.0375\n",
      "Epoch [7/20], Loss: 0.0356\n",
      "Epoch [7/20], Loss: 0.0428\n",
      "Epoch [7/20], Loss: 0.0555\n",
      "Epoch [8/20], Loss: 0.0411\n",
      "Epoch [8/20], Loss: 0.0297\n",
      "Epoch [8/20], Loss: 0.0278\n",
      "Epoch [8/20], Loss: 0.0418\n",
      "Epoch [8/20], Loss: 0.0522\n",
      "Epoch [8/20], Loss: 0.0361\n",
      "Epoch [8/20], Loss: 0.0390\n",
      "Epoch [8/20], Loss: 0.0450\n",
      "Epoch [8/20], Loss: 0.0500\n",
      "Epoch [9/20], Loss: 0.0367\n",
      "Epoch [9/20], Loss: 0.0349\n",
      "Epoch [9/20], Loss: 0.0412\n",
      "Epoch [9/20], Loss: 0.0379\n",
      "Epoch [9/20], Loss: 0.0344\n",
      "Epoch [9/20], Loss: 0.0467\n",
      "Epoch [9/20], Loss: 0.0359\n",
      "Epoch [9/20], Loss: 0.0519\n",
      "Epoch [9/20], Loss: 0.0413\n",
      "Epoch [10/20], Loss: 0.0358\n",
      "Epoch [10/20], Loss: 0.0259\n",
      "Epoch [10/20], Loss: 0.0397\n",
      "Epoch [10/20], Loss: 0.0328\n",
      "Epoch [10/20], Loss: 0.0426\n",
      "Epoch [10/20], Loss: 0.0360\n",
      "Epoch [10/20], Loss: 0.0447\n",
      "Epoch [10/20], Loss: 0.0511\n",
      "Epoch [10/20], Loss: 0.0622\n",
      "Epoch [11/20], Loss: 0.0367\n",
      "Epoch [11/20], Loss: 0.0275\n",
      "Epoch [11/20], Loss: 0.0389\n",
      "Epoch [11/20], Loss: 0.0334\n",
      "Epoch [11/20], Loss: 0.0376\n",
      "Epoch [11/20], Loss: 0.0486\n",
      "Epoch [11/20], Loss: 0.0396\n",
      "Epoch [11/20], Loss: 0.0464\n",
      "Epoch [11/20], Loss: 0.0472\n",
      "Epoch [12/20], Loss: 0.0215\n",
      "Epoch [12/20], Loss: 0.0291\n",
      "Epoch [12/20], Loss: 0.0423\n",
      "Epoch [12/20], Loss: 0.0345\n",
      "Epoch [12/20], Loss: 0.0455\n",
      "Epoch [12/20], Loss: 0.0395\n",
      "Epoch [12/20], Loss: 0.0533\n",
      "Epoch [12/20], Loss: 0.0465\n",
      "Epoch [12/20], Loss: 0.0500\n",
      "Epoch [13/20], Loss: 0.0258\n",
      "Epoch [13/20], Loss: 0.0318\n",
      "Epoch [13/20], Loss: 0.0268\n",
      "Epoch [13/20], Loss: 0.0390\n",
      "Epoch [13/20], Loss: 0.0376\n",
      "Epoch [13/20], Loss: 0.0474\n",
      "Epoch [13/20], Loss: 0.0471\n",
      "Epoch [13/20], Loss: 0.0500\n",
      "Epoch [13/20], Loss: 0.0480\n",
      "Epoch [14/20], Loss: 0.0217\n",
      "Epoch [14/20], Loss: 0.0344\n",
      "Epoch [14/20], Loss: 0.0323\n",
      "Epoch [14/20], Loss: 0.0324\n",
      "Epoch [14/20], Loss: 0.0466\n",
      "Epoch [14/20], Loss: 0.0415\n",
      "Epoch [14/20], Loss: 0.0405\n",
      "Epoch [14/20], Loss: 0.0612\n",
      "Epoch [14/20], Loss: 0.0551\n",
      "Epoch [15/20], Loss: 0.0285\n",
      "Epoch [15/20], Loss: 0.0390\n",
      "Epoch [15/20], Loss: 0.0434\n",
      "Epoch [15/20], Loss: 0.0411\n",
      "Epoch [15/20], Loss: 0.0431\n",
      "Epoch [15/20], Loss: 0.0356\n",
      "Epoch [15/20], Loss: 0.0414\n",
      "Epoch [15/20], Loss: 0.0504\n",
      "Epoch [15/20], Loss: 0.0539\n",
      "Epoch [16/20], Loss: 0.0332\n",
      "Epoch [16/20], Loss: 0.0313\n",
      "Epoch [16/20], Loss: 0.0312\n",
      "Epoch [16/20], Loss: 0.0416\n",
      "Epoch [16/20], Loss: 0.0429\n",
      "Epoch [16/20], Loss: 0.0379\n",
      "Epoch [16/20], Loss: 0.0365\n",
      "Epoch [16/20], Loss: 0.0486\n",
      "Epoch [16/20], Loss: 0.0518\n",
      "Epoch [17/20], Loss: 0.0397\n",
      "Epoch [17/20], Loss: 0.0370\n",
      "Epoch [17/20], Loss: 0.0329\n",
      "Epoch [17/20], Loss: 0.0442\n",
      "Epoch [17/20], Loss: 0.0410\n",
      "Epoch [17/20], Loss: 0.0437\n",
      "Epoch [17/20], Loss: 0.0411\n",
      "Epoch [17/20], Loss: 0.0396\n",
      "Epoch [17/20], Loss: 0.0526\n",
      "Epoch [18/20], Loss: 0.0339\n",
      "Epoch [18/20], Loss: 0.0452\n",
      "Epoch [18/20], Loss: 0.0354\n",
      "Epoch [18/20], Loss: 0.0356\n",
      "Epoch [18/20], Loss: 0.0347\n",
      "Epoch [18/20], Loss: 0.0478\n",
      "Epoch [18/20], Loss: 0.0431\n",
      "Epoch [18/20], Loss: 0.0364\n",
      "Epoch [18/20], Loss: 0.0492\n",
      "Epoch [19/20], Loss: 0.0349\n",
      "Epoch [19/20], Loss: 0.0329\n",
      "Epoch [19/20], Loss: 0.0274\n",
      "Epoch [19/20], Loss: 0.0356\n",
      "Epoch [19/20], Loss: 0.0326\n",
      "Epoch [19/20], Loss: 0.0604\n",
      "Epoch [19/20], Loss: 0.0421\n",
      "Epoch [19/20], Loss: 0.0491\n",
      "Epoch [19/20], Loss: 0.0520\n",
      "Epoch [20/20], Loss: 0.0222\n",
      "Epoch [20/20], Loss: 0.0359\n",
      "Epoch [20/20], Loss: 0.0414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/20], Loss: 0.0323\n",
      "Epoch [20/20], Loss: 0.0435\n",
      "Epoch [20/20], Loss: 0.0367\n",
      "Epoch [20/20], Loss: 0.0422\n",
      "Epoch [20/20], Loss: 0.0465\n",
      "Epoch [20/20], Loss: 0.0492\n",
      "Epoch [1/20], Loss: 0.0219\n",
      "Epoch [1/20], Loss: 0.0382\n",
      "Epoch [1/20], Loss: 0.0319\n",
      "Epoch [1/20], Loss: 0.0460\n",
      "Epoch [1/20], Loss: 0.0390\n",
      "Epoch [1/20], Loss: 0.0447\n",
      "Epoch [1/20], Loss: 0.0471\n",
      "Epoch [1/20], Loss: 0.0573\n",
      "Epoch [1/20], Loss: 0.1237\n",
      "Epoch [1/20], Loss: 0.0664\n",
      "Epoch [2/20], Loss: 0.0335\n",
      "Epoch [2/20], Loss: 0.0397\n",
      "Epoch [2/20], Loss: 0.0497\n",
      "Epoch [2/20], Loss: 0.0468\n",
      "Epoch [2/20], Loss: 0.0417\n",
      "Epoch [2/20], Loss: 0.0449\n",
      "Epoch [2/20], Loss: 0.0493\n",
      "Epoch [2/20], Loss: 0.0486\n",
      "Epoch [2/20], Loss: 0.0531\n",
      "Epoch [2/20], Loss: 0.0500\n",
      "Epoch [3/20], Loss: 0.0228\n",
      "Epoch [3/20], Loss: 0.0557\n",
      "Epoch [3/20], Loss: 0.0318\n",
      "Epoch [3/20], Loss: 0.0334\n",
      "Epoch [3/20], Loss: 0.0406\n",
      "Epoch [3/20], Loss: 0.0562\n",
      "Epoch [3/20], Loss: 0.0514\n",
      "Epoch [3/20], Loss: 0.0471\n",
      "Epoch [3/20], Loss: 0.0566\n",
      "Epoch [3/20], Loss: 0.0615\n",
      "Epoch [4/20], Loss: 0.0300\n",
      "Epoch [4/20], Loss: 0.0364\n",
      "Epoch [4/20], Loss: 0.0493\n",
      "Epoch [4/20], Loss: 0.0323\n",
      "Epoch [4/20], Loss: 0.0615\n",
      "Epoch [4/20], Loss: 0.0421\n",
      "Epoch [4/20], Loss: 0.0425\n",
      "Epoch [4/20], Loss: 0.0436\n",
      "Epoch [4/20], Loss: 0.0507\n",
      "Epoch [4/20], Loss: 0.0513\n",
      "Epoch [5/20], Loss: 0.0433\n",
      "Epoch [5/20], Loss: 0.0467\n",
      "Epoch [5/20], Loss: 0.0493\n",
      "Epoch [5/20], Loss: 0.0339\n",
      "Epoch [5/20], Loss: 0.0613\n",
      "Epoch [5/20], Loss: 0.0380\n",
      "Epoch [5/20], Loss: 0.0424\n",
      "Epoch [5/20], Loss: 0.0446\n",
      "Epoch [5/20], Loss: 0.0569\n",
      "Epoch [5/20], Loss: 0.0410\n",
      "Epoch [6/20], Loss: 0.0338\n",
      "Epoch [6/20], Loss: 0.0302\n",
      "Epoch [6/20], Loss: 0.0246\n",
      "Epoch [6/20], Loss: 0.0442\n",
      "Epoch [6/20], Loss: 0.0451\n",
      "Epoch [6/20], Loss: 0.0574\n",
      "Epoch [6/20], Loss: 0.0422\n",
      "Epoch [6/20], Loss: 0.0561\n",
      "Epoch [6/20], Loss: 0.0526\n",
      "Epoch [6/20], Loss: 0.0617\n",
      "Epoch [7/20], Loss: 0.0367\n",
      "Epoch [7/20], Loss: 0.0375\n",
      "Epoch [7/20], Loss: 0.0420\n",
      "Epoch [7/20], Loss: 0.0446\n",
      "Epoch [7/20], Loss: 0.0372\n",
      "Epoch [7/20], Loss: 0.0480\n",
      "Epoch [7/20], Loss: 0.0449\n",
      "Epoch [7/20], Loss: 0.0456\n",
      "Epoch [7/20], Loss: 0.0445\n",
      "Epoch [7/20], Loss: 0.0551\n",
      "Epoch [8/20], Loss: 0.0327\n",
      "Epoch [8/20], Loss: 0.0379\n",
      "Epoch [8/20], Loss: 0.0384\n",
      "Epoch [8/20], Loss: 0.0367\n",
      "Epoch [8/20], Loss: 0.0433\n",
      "Epoch [8/20], Loss: 0.0466\n",
      "Epoch [8/20], Loss: 0.0382\n",
      "Epoch [8/20], Loss: 0.0640\n",
      "Epoch [8/20], Loss: 0.0481\n",
      "Epoch [8/20], Loss: 0.0555\n",
      "Epoch [9/20], Loss: 0.0397\n",
      "Epoch [9/20], Loss: 0.0344\n",
      "Epoch [9/20], Loss: 0.0390\n",
      "Epoch [9/20], Loss: 0.0456\n",
      "Epoch [9/20], Loss: 0.0393\n",
      "Epoch [9/20], Loss: 0.0450\n",
      "Epoch [9/20], Loss: 0.0425\n",
      "Epoch [9/20], Loss: 0.0518\n",
      "Epoch [9/20], Loss: 0.0480\n",
      "Epoch [9/20], Loss: 0.0518\n",
      "Epoch [10/20], Loss: 0.0300\n",
      "Epoch [10/20], Loss: 0.0312\n",
      "Epoch [10/20], Loss: 0.0386\n",
      "Epoch [10/20], Loss: 0.0349\n",
      "Epoch [10/20], Loss: 0.0431\n",
      "Epoch [10/20], Loss: 0.0613\n",
      "Epoch [10/20], Loss: 0.0491\n",
      "Epoch [10/20], Loss: 0.0423\n",
      "Epoch [10/20], Loss: 0.0532\n",
      "Epoch [10/20], Loss: 0.0462\n",
      "Epoch [11/20], Loss: 0.0418\n",
      "Epoch [11/20], Loss: 0.0355\n",
      "Epoch [11/20], Loss: 0.0392\n",
      "Epoch [11/20], Loss: 0.0460\n",
      "Epoch [11/20], Loss: 0.0435\n",
      "Epoch [11/20], Loss: 0.0477\n",
      "Epoch [11/20], Loss: 0.0429\n",
      "Epoch [11/20], Loss: 0.0449\n",
      "Epoch [11/20], Loss: 0.0606\n",
      "Epoch [11/20], Loss: 0.0420\n",
      "Epoch [12/20], Loss: 0.0372\n",
      "Epoch [12/20], Loss: 0.0401\n",
      "Epoch [12/20], Loss: 0.0406\n",
      "Epoch [12/20], Loss: 0.0407\n",
      "Epoch [12/20], Loss: 0.0313\n",
      "Epoch [12/20], Loss: 0.0463\n",
      "Epoch [12/20], Loss: 0.0478\n",
      "Epoch [12/20], Loss: 0.0469\n",
      "Epoch [12/20], Loss: 0.0549\n",
      "Epoch [12/20], Loss: 0.0504\n",
      "Epoch [13/20], Loss: 0.0370\n",
      "Epoch [13/20], Loss: 0.0353\n",
      "Epoch [13/20], Loss: 0.0300\n",
      "Epoch [13/20], Loss: 0.0418\n",
      "Epoch [13/20], Loss: 0.0450\n",
      "Epoch [13/20], Loss: 0.0516\n",
      "Epoch [13/20], Loss: 0.0436\n",
      "Epoch [13/20], Loss: 0.0477\n",
      "Epoch [13/20], Loss: 0.0517\n",
      "Epoch [13/20], Loss: 0.0492\n",
      "Epoch [14/20], Loss: 0.0249\n",
      "Epoch [14/20], Loss: 0.0477\n",
      "Epoch [14/20], Loss: 0.0294\n",
      "Epoch [14/20], Loss: 0.0408\n",
      "Epoch [14/20], Loss: 0.0458\n",
      "Epoch [14/20], Loss: 0.0427\n",
      "Epoch [14/20], Loss: 0.0388\n",
      "Epoch [14/20], Loss: 0.0516\n",
      "Epoch [14/20], Loss: 0.0598\n",
      "Epoch [14/20], Loss: 0.0485\n",
      "Epoch [15/20], Loss: 0.0382\n",
      "Epoch [15/20], Loss: 0.0259\n",
      "Epoch [15/20], Loss: 0.0455\n",
      "Epoch [15/20], Loss: 0.0388\n",
      "Epoch [15/20], Loss: 0.0445\n",
      "Epoch [15/20], Loss: 0.0492\n",
      "Epoch [15/20], Loss: 0.0434\n",
      "Epoch [15/20], Loss: 0.0482\n",
      "Epoch [15/20], Loss: 0.0517\n",
      "Epoch [15/20], Loss: 0.0555\n",
      "Epoch [16/20], Loss: 0.0338\n",
      "Epoch [16/20], Loss: 0.0316\n",
      "Epoch [16/20], Loss: 0.0392\n",
      "Epoch [16/20], Loss: 0.0335\n",
      "Epoch [16/20], Loss: 0.0351\n",
      "Epoch [16/20], Loss: 0.0506\n",
      "Epoch [16/20], Loss: 0.0506\n",
      "Epoch [16/20], Loss: 0.0542\n",
      "Epoch [16/20], Loss: 0.0481\n",
      "Epoch [16/20], Loss: 0.0531\n",
      "Epoch [17/20], Loss: 0.0308\n",
      "Epoch [17/20], Loss: 0.0364\n",
      "Epoch [17/20], Loss: 0.0493\n",
      "Epoch [17/20], Loss: 0.0410\n",
      "Epoch [17/20], Loss: 0.0422\n",
      "Epoch [17/20], Loss: 0.0351\n",
      "Epoch [17/20], Loss: 0.0453\n",
      "Epoch [17/20], Loss: 0.0406\n",
      "Epoch [17/20], Loss: 0.0564\n",
      "Epoch [17/20], Loss: 0.0506\n",
      "Epoch [18/20], Loss: 0.0429\n",
      "Epoch [18/20], Loss: 0.0257\n",
      "Epoch [18/20], Loss: 0.0414\n",
      "Epoch [18/20], Loss: 0.0352\n",
      "Epoch [18/20], Loss: 0.0405\n",
      "Epoch [18/20], Loss: 0.0699\n",
      "Epoch [18/20], Loss: 0.0545\n",
      "Epoch [18/20], Loss: 0.0491\n",
      "Epoch [18/20], Loss: 0.0518\n",
      "Epoch [18/20], Loss: 0.0426\n",
      "Epoch [19/20], Loss: 0.0375\n",
      "Epoch [19/20], Loss: 0.0435\n",
      "Epoch [19/20], Loss: 0.0338\n",
      "Epoch [19/20], Loss: 0.0333\n",
      "Epoch [19/20], Loss: 0.0342\n",
      "Epoch [19/20], Loss: 0.0430\n",
      "Epoch [19/20], Loss: 0.0476\n",
      "Epoch [19/20], Loss: 0.0512\n",
      "Epoch [19/20], Loss: 0.0500\n",
      "Epoch [19/20], Loss: 0.0592\n",
      "Epoch [20/20], Loss: 0.0290\n",
      "Epoch [20/20], Loss: 0.0529\n",
      "Epoch [20/20], Loss: 0.0507\n",
      "Epoch [20/20], Loss: 0.0482\n",
      "Epoch [20/20], Loss: 0.0415\n",
      "Epoch [20/20], Loss: 0.0422\n",
      "Epoch [20/20], Loss: 0.0446\n",
      "Epoch [20/20], Loss: 0.0453\n",
      "Epoch [20/20], Loss: 0.0515\n",
      "Epoch [20/20], Loss: 0.0431\n"
     ]
    }
   ],
   "source": [
    "resall = train_on_all( get_random_feature_model() ,train_loader,test_loader,num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EWC 5e-06\n",
      "Epoch [1/80], Loss: 0.1034\n",
      "Epoch [2/80], Loss: 0.0257\n",
      "Epoch [3/80], Loss: 0.0206\n",
      "Epoch [4/80], Loss: 0.0183\n",
      "Epoch [5/80], Loss: 0.0171\n",
      "Epoch [6/80], Loss: 0.0162\n",
      "Epoch [7/80], Loss: 0.0156\n",
      "Epoch [8/80], Loss: 0.0151\n",
      "Epoch [9/80], Loss: 0.0147\n",
      "Epoch [10/80], Loss: 0.0144\n",
      "Epoch [11/80], Loss: 0.0141\n",
      "Epoch [12/80], Loss: 0.0139\n",
      "Epoch [13/80], Loss: 0.0136\n",
      "Epoch [14/80], Loss: 0.0134\n",
      "Epoch [15/80], Loss: 0.0133\n",
      "Epoch [16/80], Loss: 0.0131\n",
      "Epoch [17/80], Loss: 0.0130\n",
      "Epoch [18/80], Loss: 0.0129\n",
      "Epoch [19/80], Loss: 0.0127\n",
      "Epoch [20/80], Loss: 0.0126\n",
      "Epoch [21/80], Loss: 0.0126\n",
      "Epoch [22/80], Loss: 0.0125\n",
      "Epoch [23/80], Loss: 0.0124\n",
      "Epoch [24/80], Loss: 0.0123\n",
      "Epoch [25/80], Loss: 0.0123\n",
      "Epoch [26/80], Loss: 0.0122\n",
      "Epoch [27/80], Loss: 0.0121\n",
      "Epoch [28/80], Loss: 0.0121\n",
      "Epoch [29/80], Loss: 0.0120\n",
      "Epoch [30/80], Loss: 0.0120\n",
      "Epoch [31/80], Loss: 0.0119\n",
      "Epoch [32/80], Loss: 0.0119\n",
      "Epoch [33/80], Loss: 0.0119\n",
      "Epoch [34/80], Loss: 0.0118\n",
      "Epoch [35/80], Loss: 0.0118\n",
      "Epoch [36/80], Loss: 0.0118\n",
      "Epoch [37/80], Loss: 0.0117\n",
      "Epoch [38/80], Loss: 0.0117\n",
      "Epoch [39/80], Loss: 0.0117\n",
      "Epoch [40/80], Loss: 0.0116\n",
      "Epoch [41/80], Loss: 0.0116\n",
      "Epoch [42/80], Loss: 0.0116\n",
      "Epoch [43/80], Loss: 0.0116\n",
      "Epoch [44/80], Loss: 0.0116\n",
      "Epoch [45/80], Loss: 0.0115\n",
      "Epoch [46/80], Loss: 0.0115\n",
      "Epoch [47/80], Loss: 0.0115\n",
      "Epoch [48/80], Loss: 0.0115\n",
      "Epoch [49/80], Loss: 0.0115\n",
      "Epoch [50/80], Loss: 0.0114\n",
      "Epoch [51/80], Loss: 0.0114\n",
      "Epoch [52/80], Loss: 0.0114\n",
      "Epoch [53/80], Loss: 0.0114\n",
      "Epoch [54/80], Loss: 0.0114\n",
      "Epoch [55/80], Loss: 0.0114\n",
      "Epoch [56/80], Loss: 0.0113\n",
      "Epoch [57/80], Loss: 0.0113\n",
      "Epoch [58/80], Loss: 0.0113\n",
      "Epoch [59/80], Loss: 0.0113\n",
      "Epoch [60/80], Loss: 0.0113\n",
      "Epoch [61/80], Loss: 0.0113\n",
      "Epoch [62/80], Loss: 0.0113\n",
      "Epoch [63/80], Loss: 0.0113\n",
      "Epoch [64/80], Loss: 0.0112\n",
      "Epoch [65/80], Loss: 0.0112\n",
      "Epoch [66/80], Loss: 0.0112\n",
      "Epoch [67/80], Loss: 0.0112\n",
      "Epoch [68/80], Loss: 0.0112\n",
      "Epoch [69/80], Loss: 0.0112\n",
      "Epoch [70/80], Loss: 0.0112\n",
      "Epoch [71/80], Loss: 0.0112\n",
      "Epoch [72/80], Loss: 0.0112\n",
      "Epoch [73/80], Loss: 0.0112\n",
      "Epoch [74/80], Loss: 0.0112\n",
      "Epoch [75/80], Loss: 0.0111\n",
      "Epoch [76/80], Loss: 0.0111\n",
      "Epoch [77/80], Loss: 0.0111\n",
      "Epoch [78/80], Loss: 0.0111\n",
      "Epoch [79/80], Loss: 0.0111\n",
      "Epoch [80/80], Loss: 0.0111\n",
      "generate task data..\n",
      "task data norm and number entries: tensor(451093.3438, device='cuda:0') torch.Size([47050])\n",
      "..done\n",
      "test performance :  [96.97999573  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "individual errors:  [tensor(96.9800, device='cuda:0')]\n",
      "Epoch [1/80], Loss: 0.2356\n",
      "Epoch [2/80], Loss: 0.0463\n",
      "Epoch [3/80], Loss: 0.0412\n",
      "Epoch [4/80], Loss: 0.0390\n",
      "Epoch [5/80], Loss: 0.0376\n",
      "Epoch [6/80], Loss: 0.0368\n",
      "Epoch [7/80], Loss: 0.0362\n",
      "Epoch [8/80], Loss: 0.0358\n",
      "Epoch [9/80], Loss: 0.0354\n",
      "Epoch [10/80], Loss: 0.0352\n",
      "Epoch [11/80], Loss: 0.0350\n",
      "Epoch [12/80], Loss: 0.0348\n",
      "Epoch [13/80], Loss: 0.0347\n",
      "Epoch [14/80], Loss: 0.0346\n",
      "Epoch [15/80], Loss: 0.0345\n",
      "Epoch [16/80], Loss: 0.0345\n",
      "Epoch [17/80], Loss: 0.0344\n",
      "Epoch [18/80], Loss: 0.0344\n",
      "Epoch [19/80], Loss: 0.0343\n",
      "Epoch [20/80], Loss: 0.0343\n",
      "Epoch [21/80], Loss: 0.0343\n",
      "Epoch [22/80], Loss: 0.0343\n",
      "Epoch [23/80], Loss: 0.0342\n",
      "Epoch [24/80], Loss: 0.0342\n",
      "Epoch [25/80], Loss: 0.0342\n",
      "Epoch [26/80], Loss: 0.0342\n",
      "Epoch [27/80], Loss: 0.0342\n",
      "Epoch [28/80], Loss: 0.0342\n",
      "Epoch [29/80], Loss: 0.0342\n",
      "Epoch [30/80], Loss: 0.0342\n",
      "Epoch [31/80], Loss: 0.0342\n",
      "Epoch [32/80], Loss: 0.0342\n",
      "Epoch [33/80], Loss: 0.0342\n",
      "Epoch [34/80], Loss: 0.0342\n",
      "Epoch [35/80], Loss: 0.0342\n",
      "Epoch [36/80], Loss: 0.0342\n",
      "Epoch [37/80], Loss: 0.0342\n",
      "Epoch [38/80], Loss: 0.0342\n",
      "Epoch [39/80], Loss: 0.0342\n",
      "Epoch [40/80], Loss: 0.0342\n",
      "Epoch [41/80], Loss: 0.0342\n",
      "Epoch [42/80], Loss: 0.0342\n",
      "Epoch [43/80], Loss: 0.0342\n",
      "Epoch [44/80], Loss: 0.0342\n",
      "Epoch [45/80], Loss: 0.0342\n",
      "Epoch [46/80], Loss: 0.0342\n",
      "Epoch [47/80], Loss: 0.0342\n",
      "Epoch [48/80], Loss: 0.0342\n",
      "Epoch [49/80], Loss: 0.0342\n",
      "Epoch [50/80], Loss: 0.0342\n",
      "Epoch [51/80], Loss: 0.0342\n",
      "Epoch [52/80], Loss: 0.0342\n",
      "Epoch [53/80], Loss: 0.0342\n",
      "Epoch [54/80], Loss: 0.0342\n",
      "Epoch [55/80], Loss: 0.0342\n",
      "Epoch [56/80], Loss: 0.0342\n",
      "Epoch [57/80], Loss: 0.0342\n",
      "Epoch [58/80], Loss: 0.0342\n",
      "Epoch [59/80], Loss: 0.0342\n",
      "Epoch [60/80], Loss: 0.0342\n",
      "Epoch [61/80], Loss: 0.0342\n",
      "Epoch [62/80], Loss: 0.0342\n",
      "Epoch [63/80], Loss: 0.0342\n",
      "Epoch [64/80], Loss: 0.0342\n",
      "Epoch [65/80], Loss: 0.0342\n",
      "Epoch [66/80], Loss: 0.0342\n",
      "Epoch [67/80], Loss: 0.0342\n",
      "Epoch [68/80], Loss: 0.0342\n",
      "Epoch [69/80], Loss: 0.0342\n",
      "Epoch [70/80], Loss: 0.0342\n",
      "Epoch [71/80], Loss: 0.0342\n",
      "Epoch [72/80], Loss: 0.0342\n",
      "Epoch [73/80], Loss: 0.0342\n",
      "Epoch [74/80], Loss: 0.0342\n",
      "Epoch [75/80], Loss: 0.0342\n",
      "Epoch [76/80], Loss: 0.0342\n",
      "Epoch [77/80], Loss: 0.0342\n",
      "Epoch [78/80], Loss: 0.0342\n",
      "Epoch [79/80], Loss: 0.0342\n",
      "Epoch [80/80], Loss: 0.0342\n",
      "update data..\n",
      "task data norm and number entries: tensor(455096.7812, device='cuda:0') torch.Size([47050])\n",
      "..done\n",
      "test performance :  [96.97999573 94.16499329  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "individual errors:  [tensor(96.3800, device='cuda:0'), tensor(91.9500, device='cuda:0')]\n",
      "Epoch [1/80], Loss: 0.1445\n",
      "Epoch [2/80], Loss: 0.0497\n",
      "Epoch [3/80], Loss: 0.0474\n",
      "Epoch [4/80], Loss: 0.0464\n",
      "Epoch [5/80], Loss: 0.0459\n",
      "Epoch [6/80], Loss: 0.0455\n",
      "Epoch [7/80], Loss: 0.0453\n",
      "Epoch [8/80], Loss: 0.0451\n",
      "Epoch [9/80], Loss: 0.0449\n",
      "Epoch [10/80], Loss: 0.0448\n",
      "Epoch [11/80], Loss: 0.0448\n",
      "Epoch [12/80], Loss: 0.0447\n",
      "Epoch [13/80], Loss: 0.0446\n",
      "Epoch [14/80], Loss: 0.0446\n",
      "Epoch [15/80], Loss: 0.0445\n",
      "Epoch [16/80], Loss: 0.0445\n",
      "Epoch [17/80], Loss: 0.0445\n",
      "Epoch [18/80], Loss: 0.0445\n",
      "Epoch [19/80], Loss: 0.0444\n",
      "Epoch [20/80], Loss: 0.0444\n",
      "Epoch [21/80], Loss: 0.0444\n",
      "Epoch [22/80], Loss: 0.0444\n",
      "Epoch [23/80], Loss: 0.0444\n",
      "Epoch [24/80], Loss: 0.0444\n",
      "Epoch [25/80], Loss: 0.0443\n",
      "Epoch [26/80], Loss: 0.0443\n",
      "Epoch [27/80], Loss: 0.0443\n",
      "Epoch [28/80], Loss: 0.0443\n",
      "Epoch [29/80], Loss: 0.0443\n",
      "Epoch [30/80], Loss: 0.0443\n",
      "Epoch [31/80], Loss: 0.0443\n",
      "Epoch [32/80], Loss: 0.0443\n",
      "Epoch [33/80], Loss: 0.0443\n",
      "Epoch [34/80], Loss: 0.0443\n",
      "Epoch [35/80], Loss: 0.0443\n",
      "Epoch [36/80], Loss: 0.0443\n",
      "Epoch [37/80], Loss: 0.0443\n",
      "Epoch [38/80], Loss: 0.0443\n",
      "Epoch [39/80], Loss: 0.0443\n",
      "Epoch [40/80], Loss: 0.0443\n",
      "Epoch [41/80], Loss: 0.0443\n",
      "Epoch [42/80], Loss: 0.0443\n",
      "Epoch [43/80], Loss: 0.0443\n",
      "Epoch [44/80], Loss: 0.0443\n",
      "Epoch [45/80], Loss: 0.0443\n",
      "Epoch [46/80], Loss: 0.0443\n",
      "Epoch [47/80], Loss: 0.0443\n",
      "Epoch [48/80], Loss: 0.0443\n",
      "Epoch [49/80], Loss: 0.0443\n",
      "Epoch [50/80], Loss: 0.0443\n",
      "Epoch [51/80], Loss: 0.0443\n",
      "Epoch [52/80], Loss: 0.0443\n",
      "Epoch [53/80], Loss: 0.0443\n",
      "Epoch [54/80], Loss: 0.0443\n",
      "Epoch [55/80], Loss: 0.0443\n",
      "Epoch [56/80], Loss: 0.0443\n",
      "Epoch [57/80], Loss: 0.0443\n",
      "Epoch [58/80], Loss: 0.0443\n",
      "Epoch [59/80], Loss: 0.0443\n",
      "Epoch [60/80], Loss: 0.0443\n",
      "Epoch [61/80], Loss: 0.0443\n",
      "Epoch [62/80], Loss: 0.0443\n",
      "Epoch [63/80], Loss: 0.0443\n",
      "Epoch [64/80], Loss: 0.0443\n",
      "Epoch [65/80], Loss: 0.0442\n",
      "Epoch [66/80], Loss: 0.0442\n",
      "Epoch [67/80], Loss: 0.0442\n",
      "Epoch [68/80], Loss: 0.0442\n",
      "Epoch [69/80], Loss: 0.0442\n",
      "Epoch [70/80], Loss: 0.0442\n",
      "Epoch [71/80], Loss: 0.0442\n",
      "Epoch [72/80], Loss: 0.0442\n",
      "Epoch [73/80], Loss: 0.0442\n",
      "Epoch [74/80], Loss: 0.0442\n",
      "Epoch [75/80], Loss: 0.0442\n",
      "Epoch [76/80], Loss: 0.0442\n",
      "Epoch [77/80], Loss: 0.0442\n",
      "Epoch [78/80], Loss: 0.0442\n",
      "Epoch [79/80], Loss: 0.0442\n",
      "Epoch [80/80], Loss: 0.0442\n",
      "update data..\n",
      "task data norm and number entries: tensor(450464.8750, device='cuda:0') torch.Size([47050])\n",
      "..done\n",
      "test performance :  [96.97999573 94.16499329 91.31999207  0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "individual errors:  [tensor(95.9000, device='cuda:0'), tensor(90.5500, device='cuda:0'), tensor(87.5100, device='cuda:0')]\n",
      "Epoch [1/80], Loss: 0.1361\n",
      "Epoch [2/80], Loss: 0.0518\n",
      "Epoch [3/80], Loss: 0.0504\n",
      "Epoch [4/80], Loss: 0.0499\n",
      "Epoch [5/80], Loss: 0.0497\n",
      "Epoch [6/80], Loss: 0.0495\n",
      "Epoch [7/80], Loss: 0.0495\n",
      "Epoch [8/80], Loss: 0.0494\n",
      "Epoch [9/80], Loss: 0.0494\n",
      "Epoch [10/80], Loss: 0.0494\n",
      "Epoch [11/80], Loss: 0.0494\n",
      "Epoch [12/80], Loss: 0.0493\n",
      "Epoch [13/80], Loss: 0.0493\n",
      "Epoch [14/80], Loss: 0.0493\n",
      "Epoch [15/80], Loss: 0.0493\n",
      "Epoch [16/80], Loss: 0.0493\n",
      "Epoch [17/80], Loss: 0.0493\n",
      "Epoch [18/80], Loss: 0.0493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/80], Loss: 0.0493\n",
      "Epoch [20/80], Loss: 0.0493\n",
      "Epoch [21/80], Loss: 0.0493\n",
      "Epoch [22/80], Loss: 0.0493\n",
      "Epoch [23/80], Loss: 0.0493\n",
      "Epoch [24/80], Loss: 0.0493\n",
      "Epoch [25/80], Loss: 0.0493\n",
      "Epoch [26/80], Loss: 0.0493\n",
      "Epoch [27/80], Loss: 0.0493\n",
      "Epoch [28/80], Loss: 0.0493\n",
      "Epoch [29/80], Loss: 0.0493\n",
      "Epoch [30/80], Loss: 0.0493\n",
      "Epoch [31/80], Loss: 0.0493\n",
      "Epoch [32/80], Loss: 0.0493\n",
      "Epoch [33/80], Loss: 0.0493\n",
      "Epoch [34/80], Loss: 0.0493\n",
      "Epoch [35/80], Loss: 0.0493\n",
      "Epoch [36/80], Loss: 0.0493\n",
      "Epoch [37/80], Loss: 0.0493\n",
      "Epoch [38/80], Loss: 0.0493\n",
      "Epoch [39/80], Loss: 0.0493\n",
      "Epoch [40/80], Loss: 0.0493\n",
      "Epoch [41/80], Loss: 0.0493\n",
      "Epoch [42/80], Loss: 0.0493\n",
      "Epoch [43/80], Loss: 0.0493\n",
      "Epoch [44/80], Loss: 0.0493\n",
      "Epoch [45/80], Loss: 0.0493\n",
      "Epoch [46/80], Loss: 0.0493\n",
      "Epoch [47/80], Loss: 0.0493\n",
      "Epoch [48/80], Loss: 0.0493\n",
      "Epoch [49/80], Loss: 0.0493\n",
      "Epoch [50/80], Loss: 0.0493\n",
      "Epoch [51/80], Loss: 0.0493\n",
      "Epoch [52/80], Loss: 0.0493\n",
      "Epoch [53/80], Loss: 0.0493\n",
      "Epoch [54/80], Loss: 0.0493\n",
      "Epoch [55/80], Loss: 0.0493\n",
      "Epoch [56/80], Loss: 0.0493\n",
      "Epoch [57/80], Loss: 0.0493\n",
      "Epoch [58/80], Loss: 0.0493\n",
      "Epoch [59/80], Loss: 0.0493\n",
      "Epoch [60/80], Loss: 0.0493\n",
      "Epoch [61/80], Loss: 0.0493\n",
      "Epoch [62/80], Loss: 0.0493\n",
      "Epoch [63/80], Loss: 0.0493\n",
      "Epoch [64/80], Loss: 0.0493\n",
      "Epoch [65/80], Loss: 0.0493\n",
      "Epoch [66/80], Loss: 0.0493\n",
      "Epoch [67/80], Loss: 0.0493\n",
      "Epoch [68/80], Loss: 0.0493\n",
      "Epoch [69/80], Loss: 0.0493\n",
      "Epoch [70/80], Loss: 0.0493\n",
      "Epoch [71/80], Loss: 0.0493\n",
      "Epoch [72/80], Loss: 0.0493\n",
      "Epoch [73/80], Loss: 0.0493\n",
      "Epoch [74/80], Loss: 0.0493\n",
      "Epoch [75/80], Loss: 0.0493\n",
      "Epoch [76/80], Loss: 0.0493\n",
      "Epoch [77/80], Loss: 0.0493\n",
      "Epoch [78/80], Loss: 0.0493\n",
      "Epoch [79/80], Loss: 0.0493\n",
      "Epoch [80/80], Loss: 0.0493\n",
      "update data..\n",
      "task data norm and number entries: tensor(435705.5625, device='cuda:0') torch.Size([47050])\n",
      "..done\n",
      "test performance :  [96.97999573 94.16499329 91.31999207 88.03499603  0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "individual errors:  [tensor(94.7000, device='cuda:0'), tensor(88.7000, device='cuda:0'), tensor(83.9700, device='cuda:0'), tensor(84.7700, device='cuda:0')]\n",
      "Epoch [1/80], Loss: 0.1190\n",
      "Epoch [2/80], Loss: 0.0548\n",
      "Epoch [3/80], Loss: 0.0539\n",
      "Epoch [4/80], Loss: 0.0537\n",
      "Epoch [5/80], Loss: 0.0536\n",
      "Epoch [6/80], Loss: 0.0536\n",
      "Epoch [7/80], Loss: 0.0536\n",
      "Epoch [8/80], Loss: 0.0536\n",
      "Epoch [9/80], Loss: 0.0536\n",
      "Epoch [10/80], Loss: 0.0536\n",
      "Epoch [11/80], Loss: 0.0536\n",
      "Epoch [12/80], Loss: 0.0536\n",
      "Epoch [13/80], Loss: 0.0536\n",
      "Epoch [14/80], Loss: 0.0536\n",
      "Epoch [15/80], Loss: 0.0537\n",
      "Epoch [16/80], Loss: 0.0537\n",
      "Epoch [17/80], Loss: 0.0537\n",
      "Epoch [18/80], Loss: 0.0537\n",
      "Epoch [19/80], Loss: 0.0537\n",
      "Epoch [20/80], Loss: 0.0537\n",
      "Epoch [21/80], Loss: 0.0537\n",
      "Epoch [22/80], Loss: 0.0537\n",
      "Epoch [23/80], Loss: 0.0537\n",
      "Epoch [24/80], Loss: 0.0537\n",
      "Epoch [25/80], Loss: 0.0537\n",
      "Epoch [26/80], Loss: 0.0537\n",
      "Epoch [27/80], Loss: 0.0537\n",
      "Epoch [28/80], Loss: 0.0537\n",
      "Epoch [29/80], Loss: 0.0537\n",
      "Epoch [30/80], Loss: 0.0537\n",
      "Epoch [31/80], Loss: 0.0537\n",
      "Epoch [32/80], Loss: 0.0537\n",
      "Epoch [33/80], Loss: 0.0537\n",
      "Epoch [34/80], Loss: 0.0537\n",
      "Epoch [35/80], Loss: 0.0537\n",
      "Epoch [36/80], Loss: 0.0537\n",
      "Epoch [37/80], Loss: 0.0537\n",
      "Epoch [38/80], Loss: 0.0537\n",
      "Epoch [39/80], Loss: 0.0537\n",
      "Epoch [40/80], Loss: 0.0537\n",
      "Epoch [41/80], Loss: 0.0537\n",
      "Epoch [42/80], Loss: 0.0537\n",
      "Epoch [43/80], Loss: 0.0537\n",
      "Epoch [44/80], Loss: 0.0537\n",
      "Epoch [45/80], Loss: 0.0537\n",
      "Epoch [46/80], Loss: 0.0537\n",
      "Epoch [47/80], Loss: 0.0537\n",
      "Epoch [48/80], Loss: 0.0537\n",
      "Epoch [49/80], Loss: 0.0537\n",
      "Epoch [50/80], Loss: 0.0537\n",
      "Epoch [51/80], Loss: 0.0537\n",
      "Epoch [52/80], Loss: 0.0537\n",
      "Epoch [53/80], Loss: 0.0537\n",
      "Epoch [54/80], Loss: 0.0537\n",
      "Epoch [55/80], Loss: 0.0537\n",
      "Epoch [56/80], Loss: 0.0537\n",
      "Epoch [57/80], Loss: 0.0537\n",
      "Epoch [58/80], Loss: 0.0537\n",
      "Epoch [59/80], Loss: 0.0537\n",
      "Epoch [60/80], Loss: 0.0537\n",
      "Epoch [61/80], Loss: 0.0537\n",
      "Epoch [62/80], Loss: 0.0537\n",
      "Epoch [63/80], Loss: 0.0537\n",
      "Epoch [64/80], Loss: 0.0537\n",
      "Epoch [65/80], Loss: 0.0537\n",
      "Epoch [66/80], Loss: 0.0537\n",
      "Epoch [67/80], Loss: 0.0537\n",
      "Epoch [68/80], Loss: 0.0537\n",
      "Epoch [69/80], Loss: 0.0537\n",
      "Epoch [70/80], Loss: 0.0537\n",
      "Epoch [71/80], Loss: 0.0537\n",
      "Epoch [72/80], Loss: 0.0537\n",
      "Epoch [73/80], Loss: 0.0537\n",
      "Epoch [74/80], Loss: 0.0537\n",
      "Epoch [75/80], Loss: 0.0537\n",
      "Epoch [76/80], Loss: 0.0537\n",
      "Epoch [77/80], Loss: 0.0537\n",
      "Epoch [78/80], Loss: 0.0537\n",
      "Epoch [79/80], Loss: 0.0537\n",
      "Epoch [80/80], Loss: 0.0537\n",
      "update data..\n",
      "task data norm and number entries: tensor(444921.4062, device='cuda:0') torch.Size([47050])\n",
      "..done\n",
      "test performance :  [96.97999573 94.16499329 91.31999207 88.03499603 86.69999695  0.\n",
      "  0.          0.          0.          0.        ]\n",
      "individual errors:  [tensor(94.8600, device='cuda:0'), tensor(88.9200, device='cuda:0'), tensor(84.4300, device='cuda:0'), tensor(83.2500, device='cuda:0'), tensor(82.0400, device='cuda:0')]\n",
      "Epoch [1/80], Loss: 0.1309\n",
      "Epoch [2/80], Loss: 0.0611\n",
      "Epoch [3/80], Loss: 0.0603\n",
      "Epoch [4/80], Loss: 0.0601\n",
      "Epoch [5/80], Loss: 0.0600\n",
      "Epoch [6/80], Loss: 0.0599\n",
      "Epoch [7/80], Loss: 0.0599\n",
      "Epoch [8/80], Loss: 0.0599\n",
      "Epoch [9/80], Loss: 0.0599\n",
      "Epoch [10/80], Loss: 0.0599\n",
      "Epoch [11/80], Loss: 0.0599\n",
      "Epoch [12/80], Loss: 0.0599\n",
      "Epoch [13/80], Loss: 0.0599\n",
      "Epoch [14/80], Loss: 0.0599\n",
      "Epoch [15/80], Loss: 0.0599\n",
      "Epoch [16/80], Loss: 0.0599\n",
      "Epoch [17/80], Loss: 0.0599\n",
      "Epoch [18/80], Loss: 0.0599\n",
      "Epoch [19/80], Loss: 0.0599\n",
      "Epoch [20/80], Loss: 0.0599\n",
      "Epoch [21/80], Loss: 0.0599\n",
      "Epoch [22/80], Loss: 0.0599\n",
      "Epoch [23/80], Loss: 0.0599\n",
      "Epoch [24/80], Loss: 0.0599\n",
      "Epoch [25/80], Loss: 0.0599\n",
      "Epoch [26/80], Loss: 0.0599\n",
      "Epoch [27/80], Loss: 0.0599\n",
      "Epoch [28/80], Loss: 0.0599\n",
      "Epoch [29/80], Loss: 0.0599\n",
      "Epoch [30/80], Loss: 0.0599\n",
      "Epoch [31/80], Loss: 0.0599\n",
      "Epoch [32/80], Loss: 0.0599\n",
      "Epoch [33/80], Loss: 0.0599\n",
      "Epoch [34/80], Loss: 0.0599\n",
      "Epoch [35/80], Loss: 0.0599\n",
      "Epoch [36/80], Loss: 0.0599\n",
      "Epoch [37/80], Loss: 0.0599\n",
      "Epoch [38/80], Loss: 0.0599\n",
      "Epoch [39/80], Loss: 0.0599\n",
      "Epoch [40/80], Loss: 0.0599\n",
      "Epoch [41/80], Loss: 0.0599\n",
      "Epoch [42/80], Loss: 0.0599\n",
      "Epoch [43/80], Loss: 0.0599\n",
      "Epoch [44/80], Loss: 0.0599\n",
      "Epoch [45/80], Loss: 0.0599\n",
      "Epoch [46/80], Loss: 0.0599\n",
      "Epoch [47/80], Loss: 0.0599\n",
      "Epoch [48/80], Loss: 0.0599\n",
      "Epoch [49/80], Loss: 0.0599\n",
      "Epoch [50/80], Loss: 0.0599\n",
      "Epoch [51/80], Loss: 0.0599\n",
      "Epoch [52/80], Loss: 0.0599\n",
      "Epoch [53/80], Loss: 0.0599\n",
      "Epoch [54/80], Loss: 0.0599\n",
      "Epoch [55/80], Loss: 0.0599\n",
      "Epoch [56/80], Loss: 0.0599\n",
      "Epoch [57/80], Loss: 0.0599\n",
      "Epoch [58/80], Loss: 0.0599\n",
      "Epoch [59/80], Loss: 0.0599\n",
      "Epoch [60/80], Loss: 0.0599\n",
      "Epoch [61/80], Loss: 0.0599\n",
      "Epoch [62/80], Loss: 0.0599\n",
      "Epoch [63/80], Loss: 0.0599\n",
      "Epoch [64/80], Loss: 0.0599\n",
      "Epoch [65/80], Loss: 0.0599\n",
      "Epoch [66/80], Loss: 0.0599\n",
      "Epoch [67/80], Loss: 0.0599\n",
      "Epoch [68/80], Loss: 0.0599\n",
      "Epoch [69/80], Loss: 0.0599\n",
      "Epoch [70/80], Loss: 0.0599\n",
      "Epoch [71/80], Loss: 0.0599\n",
      "Epoch [72/80], Loss: 0.0599\n",
      "Epoch [73/80], Loss: 0.0599\n",
      "Epoch [74/80], Loss: 0.0599\n",
      "Epoch [75/80], Loss: 0.0599\n",
      "Epoch [76/80], Loss: 0.0599\n",
      "Epoch [77/80], Loss: 0.0599\n",
      "Epoch [78/80], Loss: 0.0599\n",
      "Epoch [79/80], Loss: 0.0599\n",
      "Epoch [80/80], Loss: 0.0599\n",
      "update data..\n",
      "task data norm and number entries: tensor(444082.3125, device='cuda:0') torch.Size([47050])\n",
      "..done\n",
      "test performance :  [96.97999573 94.16499329 91.31999207 88.03499603 86.69999695 83.82666779\n",
      "  0.          0.          0.          0.        ]\n",
      "individual errors:  [tensor(93.5400, device='cuda:0'), tensor(87.6200, device='cuda:0'), tensor(82.5000, device='cuda:0'), tensor(79.6600, device='cuda:0'), tensor(79.6300, device='cuda:0'), tensor(80.0100, device='cuda:0')]\n",
      "Epoch [1/80], Loss: 0.1394\n",
      "Epoch [2/80], Loss: 0.0620\n",
      "Epoch [3/80], Loss: 0.0613\n",
      "Epoch [4/80], Loss: 0.0611\n",
      "Epoch [5/80], Loss: 0.0610\n",
      "Epoch [6/80], Loss: 0.0610\n",
      "Epoch [7/80], Loss: 0.0610\n",
      "Epoch [8/80], Loss: 0.0610\n",
      "Epoch [9/80], Loss: 0.0610\n",
      "Epoch [10/80], Loss: 0.0610\n",
      "Epoch [11/80], Loss: 0.0610\n",
      "Epoch [12/80], Loss: 0.0610\n",
      "Epoch [13/80], Loss: 0.0610\n",
      "Epoch [14/80], Loss: 0.0610\n",
      "Epoch [15/80], Loss: 0.0610\n",
      "Epoch [16/80], Loss: 0.0610\n",
      "Epoch [17/80], Loss: 0.0610\n",
      "Epoch [18/80], Loss: 0.0610\n",
      "Epoch [19/80], Loss: 0.0610\n",
      "Epoch [20/80], Loss: 0.0610\n",
      "Epoch [21/80], Loss: 0.0610\n",
      "Epoch [22/80], Loss: 0.0610\n",
      "Epoch [23/80], Loss: 0.0610\n",
      "Epoch [24/80], Loss: 0.0610\n",
      "Epoch [25/80], Loss: 0.0610\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/80], Loss: 0.0610\n",
      "Epoch [27/80], Loss: 0.0610\n",
      "Epoch [28/80], Loss: 0.0610\n",
      "Epoch [29/80], Loss: 0.0610\n",
      "Epoch [30/80], Loss: 0.0610\n",
      "Epoch [31/80], Loss: 0.0610\n",
      "Epoch [32/80], Loss: 0.0610\n",
      "Epoch [33/80], Loss: 0.0610\n",
      "Epoch [34/80], Loss: 0.0610\n",
      "Epoch [35/80], Loss: 0.0610\n",
      "Epoch [36/80], Loss: 0.0610\n",
      "Epoch [37/80], Loss: 0.0610\n",
      "Epoch [38/80], Loss: 0.0610\n",
      "Epoch [39/80], Loss: 0.0610\n",
      "Epoch [40/80], Loss: 0.0610\n",
      "Epoch [41/80], Loss: 0.0610\n",
      "Epoch [42/80], Loss: 0.0610\n",
      "Epoch [43/80], Loss: 0.0610\n",
      "Epoch [44/80], Loss: 0.0610\n",
      "Epoch [45/80], Loss: 0.0610\n",
      "Epoch [46/80], Loss: 0.0610\n",
      "Epoch [47/80], Loss: 0.0610\n",
      "Epoch [48/80], Loss: 0.0610\n",
      "Epoch [49/80], Loss: 0.0610\n",
      "Epoch [50/80], Loss: 0.0610\n",
      "Epoch [51/80], Loss: 0.0610\n",
      "Epoch [52/80], Loss: 0.0610\n",
      "Epoch [53/80], Loss: 0.0610\n",
      "Epoch [54/80], Loss: 0.0610\n",
      "Epoch [55/80], Loss: 0.0610\n",
      "Epoch [56/80], Loss: 0.0610\n",
      "Epoch [57/80], Loss: 0.0610\n",
      "Epoch [58/80], Loss: 0.0610\n",
      "Epoch [59/80], Loss: 0.0610\n",
      "Epoch [60/80], Loss: 0.0610\n",
      "Epoch [61/80], Loss: 0.0610\n",
      "Epoch [62/80], Loss: 0.0610\n",
      "Epoch [63/80], Loss: 0.0610\n",
      "Epoch [64/80], Loss: 0.0610\n",
      "Epoch [65/80], Loss: 0.0610\n",
      "Epoch [66/80], Loss: 0.0610\n",
      "Epoch [67/80], Loss: 0.0610\n",
      "Epoch [68/80], Loss: 0.0610\n",
      "Epoch [69/80], Loss: 0.0610\n",
      "Epoch [70/80], Loss: 0.0610\n",
      "Epoch [71/80], Loss: 0.0610\n",
      "Epoch [72/80], Loss: 0.0610\n",
      "Epoch [73/80], Loss: 0.0610\n",
      "Epoch [74/80], Loss: 0.0610\n",
      "Epoch [75/80], Loss: 0.0610\n",
      "Epoch [76/80], Loss: 0.0610\n",
      "Epoch [77/80], Loss: 0.0610\n",
      "Epoch [78/80], Loss: 0.0610\n",
      "Epoch [79/80], Loss: 0.0610\n",
      "Epoch [80/80], Loss: 0.0610\n",
      "update data..\n",
      "task data norm and number entries: tensor(455247.8438, device='cuda:0') torch.Size([47050])\n",
      "..done\n",
      "test performance :  [96.97999573 94.16499329 91.31999207 88.03499603 86.69999695 83.82666779\n",
      " 80.39571381  0.          0.          0.        ]\n",
      "individual errors:  [tensor(91.1900, device='cuda:0'), tensor(83.1200, device='cuda:0'), tensor(81., device='cuda:0'), tensor(74.9100, device='cuda:0'), tensor(77.8400, device='cuda:0'), tensor(77.3900, device='cuda:0'), tensor(77.3200, device='cuda:0')]\n",
      "Epoch [1/80], Loss: 0.1268\n",
      "Epoch [2/80], Loss: 0.0630\n",
      "Epoch [3/80], Loss: 0.0626\n",
      "Epoch [4/80], Loss: 0.0625\n",
      "Epoch [5/80], Loss: 0.0624\n",
      "Epoch [6/80], Loss: 0.0624\n",
      "Epoch [7/80], Loss: 0.0623\n",
      "Epoch [8/80], Loss: 0.0623\n",
      "Epoch [9/80], Loss: 0.0623\n",
      "Epoch [10/80], Loss: 0.0623\n",
      "Epoch [11/80], Loss: 0.0622\n",
      "Epoch [12/80], Loss: 0.0622\n",
      "Epoch [13/80], Loss: 0.0622\n",
      "Epoch [14/80], Loss: 0.0622\n",
      "Epoch [15/80], Loss: 0.0622\n",
      "Epoch [16/80], Loss: 0.0622\n",
      "Epoch [17/80], Loss: 0.0622\n",
      "Epoch [18/80], Loss: 0.0622\n",
      "Epoch [19/80], Loss: 0.0622\n",
      "Epoch [20/80], Loss: 0.0622\n",
      "Epoch [21/80], Loss: 0.0622\n",
      "Epoch [22/80], Loss: 0.0622\n",
      "Epoch [23/80], Loss: 0.0622\n",
      "Epoch [24/80], Loss: 0.0622\n",
      "Epoch [25/80], Loss: 0.0622\n",
      "Epoch [26/80], Loss: 0.0622\n",
      "Epoch [27/80], Loss: 0.0622\n",
      "Epoch [28/80], Loss: 0.0622\n",
      "Epoch [29/80], Loss: 0.0622\n",
      "Epoch [30/80], Loss: 0.0622\n",
      "Epoch [31/80], Loss: 0.0622\n",
      "Epoch [32/80], Loss: 0.0622\n",
      "Epoch [33/80], Loss: 0.0622\n",
      "Epoch [34/80], Loss: 0.0622\n",
      "Epoch [35/80], Loss: 0.0621\n",
      "Epoch [36/80], Loss: 0.0621\n",
      "Epoch [37/80], Loss: 0.0621\n",
      "Epoch [38/80], Loss: 0.0621\n",
      "Epoch [39/80], Loss: 0.0621\n",
      "Epoch [40/80], Loss: 0.0621\n",
      "Epoch [41/80], Loss: 0.0621\n",
      "Epoch [42/80], Loss: 0.0621\n",
      "Epoch [43/80], Loss: 0.0621\n",
      "Epoch [44/80], Loss: 0.0621\n",
      "Epoch [45/80], Loss: 0.0621\n",
      "Epoch [46/80], Loss: 0.0621\n",
      "Epoch [47/80], Loss: 0.0621\n",
      "Epoch [48/80], Loss: 0.0621\n",
      "Epoch [49/80], Loss: 0.0621\n",
      "Epoch [50/80], Loss: 0.0621\n",
      "Epoch [51/80], Loss: 0.0621\n",
      "Epoch [52/80], Loss: 0.0621\n",
      "Epoch [53/80], Loss: 0.0621\n",
      "Epoch [54/80], Loss: 0.0621\n",
      "Epoch [55/80], Loss: 0.0621\n",
      "Epoch [56/80], Loss: 0.0621\n",
      "Epoch [57/80], Loss: 0.0621\n",
      "Epoch [58/80], Loss: 0.0621\n",
      "Epoch [59/80], Loss: 0.0621\n",
      "Epoch [60/80], Loss: 0.0621\n",
      "Epoch [61/80], Loss: 0.0621\n",
      "Epoch [62/80], Loss: 0.0621\n",
      "Epoch [63/80], Loss: 0.0621\n",
      "Epoch [64/80], Loss: 0.0621\n",
      "Epoch [65/80], Loss: 0.0621\n",
      "Epoch [66/80], Loss: 0.0621\n",
      "Epoch [67/80], Loss: 0.0621\n",
      "Epoch [68/80], Loss: 0.0621\n",
      "Epoch [69/80], Loss: 0.0621\n",
      "Epoch [70/80], Loss: 0.0621\n",
      "Epoch [71/80], Loss: 0.0621\n",
      "Epoch [72/80], Loss: 0.0621\n",
      "Epoch [73/80], Loss: 0.0621\n",
      "Epoch [74/80], Loss: 0.0621\n",
      "Epoch [75/80], Loss: 0.0621\n",
      "Epoch [76/80], Loss: 0.0621\n",
      "Epoch [77/80], Loss: 0.0621\n",
      "Epoch [78/80], Loss: 0.0621\n",
      "Epoch [79/80], Loss: 0.0621\n",
      "Epoch [80/80], Loss: 0.0621\n",
      "update data..\n",
      "task data norm and number entries: tensor(440730.7500, device='cuda:0') torch.Size([47050])\n",
      "..done\n",
      "test performance :  [96.97999573 94.16499329 91.31999207 88.03499603 86.69999695 83.82666779\n",
      " 80.39571381 77.96375275  0.          0.        ]\n",
      "individual errors:  [tensor(90.9200, device='cuda:0'), tensor(83.2000, device='cuda:0'), tensor(80.3100, device='cuda:0'), tensor(69.4000, device='cuda:0'), tensor(77.4100, device='cuda:0'), tensor(71.9300, device='cuda:0'), tensor(74.1400, device='cuda:0'), tensor(76.4000, device='cuda:0')]\n",
      "Epoch [1/80], Loss: 0.1239\n",
      "Epoch [2/80], Loss: 0.0658\n",
      "Epoch [3/80], Loss: 0.0652\n",
      "Epoch [4/80], Loss: 0.0651\n",
      "Epoch [5/80], Loss: 0.0650\n",
      "Epoch [6/80], Loss: 0.0650\n",
      "Epoch [7/80], Loss: 0.0650\n",
      "Epoch [8/80], Loss: 0.0650\n",
      "Epoch [9/80], Loss: 0.0650\n",
      "Epoch [10/80], Loss: 0.0650\n",
      "Epoch [11/80], Loss: 0.0650\n",
      "Epoch [12/80], Loss: 0.0650\n",
      "Epoch [13/80], Loss: 0.0650\n",
      "Epoch [14/80], Loss: 0.0650\n",
      "Epoch [15/80], Loss: 0.0650\n",
      "Epoch [16/80], Loss: 0.0650\n",
      "Epoch [17/80], Loss: 0.0650\n",
      "Epoch [18/80], Loss: 0.0650\n",
      "Epoch [19/80], Loss: 0.0650\n",
      "Epoch [20/80], Loss: 0.0650\n",
      "Epoch [21/80], Loss: 0.0650\n",
      "Epoch [22/80], Loss: 0.0650\n",
      "Epoch [23/80], Loss: 0.0650\n",
      "Epoch [24/80], Loss: 0.0650\n",
      "Epoch [25/80], Loss: 0.0650\n",
      "Epoch [26/80], Loss: 0.0650\n",
      "Epoch [27/80], Loss: 0.0650\n",
      "Epoch [28/80], Loss: 0.0650\n",
      "Epoch [29/80], Loss: 0.0650\n",
      "Epoch [30/80], Loss: 0.0650\n",
      "Epoch [31/80], Loss: 0.0650\n",
      "Epoch [32/80], Loss: 0.0650\n",
      "Epoch [33/80], Loss: 0.0650\n",
      "Epoch [34/80], Loss: 0.0650\n",
      "Epoch [35/80], Loss: 0.0650\n",
      "Epoch [36/80], Loss: 0.0650\n",
      "Epoch [37/80], Loss: 0.0650\n",
      "Epoch [38/80], Loss: 0.0650\n",
      "Epoch [39/80], Loss: 0.0650\n",
      "Epoch [40/80], Loss: 0.0650\n",
      "Epoch [41/80], Loss: 0.0650\n",
      "Epoch [42/80], Loss: 0.0650\n",
      "Epoch [43/80], Loss: 0.0650\n",
      "Epoch [44/80], Loss: 0.0650\n",
      "Epoch [45/80], Loss: 0.0650\n",
      "Epoch [46/80], Loss: 0.0650\n",
      "Epoch [47/80], Loss: 0.0650\n",
      "Epoch [48/80], Loss: 0.0650\n",
      "Epoch [49/80], Loss: 0.0650\n",
      "Epoch [50/80], Loss: 0.0650\n",
      "Epoch [51/80], Loss: 0.0650\n",
      "Epoch [52/80], Loss: 0.0650\n",
      "Epoch [53/80], Loss: 0.0650\n",
      "Epoch [54/80], Loss: 0.0650\n",
      "Epoch [55/80], Loss: 0.0650\n",
      "Epoch [56/80], Loss: 0.0650\n",
      "Epoch [57/80], Loss: 0.0650\n",
      "Epoch [58/80], Loss: 0.0650\n",
      "Epoch [59/80], Loss: 0.0650\n",
      "Epoch [60/80], Loss: 0.0650\n",
      "Epoch [61/80], Loss: 0.0650\n",
      "Epoch [62/80], Loss: 0.0650\n",
      "Epoch [63/80], Loss: 0.0650\n",
      "Epoch [64/80], Loss: 0.0650\n",
      "Epoch [65/80], Loss: 0.0650\n",
      "Epoch [66/80], Loss: 0.0650\n",
      "Epoch [67/80], Loss: 0.0650\n",
      "Epoch [68/80], Loss: 0.0650\n",
      "Epoch [69/80], Loss: 0.0650\n",
      "Epoch [70/80], Loss: 0.0650\n",
      "Epoch [71/80], Loss: 0.0650\n",
      "Epoch [72/80], Loss: 0.0650\n",
      "Epoch [73/80], Loss: 0.0650\n",
      "Epoch [74/80], Loss: 0.0650\n",
      "Epoch [75/80], Loss: 0.0650\n",
      "Epoch [76/80], Loss: 0.0650\n",
      "Epoch [77/80], Loss: 0.0650\n",
      "Epoch [78/80], Loss: 0.0650\n",
      "Epoch [79/80], Loss: 0.0650\n",
      "Epoch [80/80], Loss: 0.0650\n",
      "update data..\n",
      "task data norm and number entries: tensor(436112.2812, device='cuda:0') torch.Size([47050])\n",
      "..done\n",
      "test performance :  [96.97999573 94.16499329 91.31999207 88.03499603 86.69999695 83.82666779\n",
      " 80.39571381 77.96375275 75.66222382  0.        ]\n",
      "individual errors:  [tensor(88.6300, device='cuda:0'), tensor(80.3300, device='cuda:0'), tensor(78.2300, device='cuda:0'), tensor(67.1600, device='cuda:0'), tensor(75.6300, device='cuda:0'), tensor(69.6800, device='cuda:0'), tensor(71.8900, device='cuda:0'), tensor(74.0800, device='cuda:0'), tensor(75.3300, device='cuda:0')]\n",
      "Epoch [1/80], Loss: 0.1479\n",
      "Epoch [2/80], Loss: 0.0664\n",
      "Epoch [3/80], Loss: 0.0660\n",
      "Epoch [4/80], Loss: 0.0660\n",
      "Epoch [5/80], Loss: 0.0660\n",
      "Epoch [6/80], Loss: 0.0660\n",
      "Epoch [7/80], Loss: 0.0660\n",
      "Epoch [8/80], Loss: 0.0660\n",
      "Epoch [9/80], Loss: 0.0660\n",
      "Epoch [10/80], Loss: 0.0660\n",
      "Epoch [11/80], Loss: 0.0659\n",
      "Epoch [12/80], Loss: 0.0659\n",
      "Epoch [13/80], Loss: 0.0659\n",
      "Epoch [14/80], Loss: 0.0659\n",
      "Epoch [15/80], Loss: 0.0659\n",
      "Epoch [16/80], Loss: 0.0659\n",
      "Epoch [17/80], Loss: 0.0659\n",
      "Epoch [18/80], Loss: 0.0659\n",
      "Epoch [19/80], Loss: 0.0659\n",
      "Epoch [20/80], Loss: 0.0659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/80], Loss: 0.0659\n",
      "Epoch [22/80], Loss: 0.0659\n",
      "Epoch [23/80], Loss: 0.0659\n",
      "Epoch [24/80], Loss: 0.0659\n",
      "Epoch [25/80], Loss: 0.0659\n",
      "Epoch [26/80], Loss: 0.0659\n",
      "Epoch [27/80], Loss: 0.0659\n",
      "Epoch [28/80], Loss: 0.0659\n",
      "Epoch [29/80], Loss: 0.0659\n",
      "Epoch [30/80], Loss: 0.0659\n",
      "Epoch [31/80], Loss: 0.0659\n",
      "Epoch [32/80], Loss: 0.0659\n",
      "Epoch [33/80], Loss: 0.0659\n",
      "Epoch [34/80], Loss: 0.0659\n",
      "Epoch [35/80], Loss: 0.0659\n",
      "Epoch [36/80], Loss: 0.0659\n",
      "Epoch [37/80], Loss: 0.0659\n",
      "Epoch [38/80], Loss: 0.0659\n",
      "Epoch [39/80], Loss: 0.0659\n",
      "Epoch [40/80], Loss: 0.0659\n",
      "Epoch [41/80], Loss: 0.0659\n",
      "Epoch [42/80], Loss: 0.0659\n",
      "Epoch [43/80], Loss: 0.0659\n",
      "Epoch [44/80], Loss: 0.0659\n",
      "Epoch [45/80], Loss: 0.0659\n",
      "Epoch [46/80], Loss: 0.0659\n",
      "Epoch [47/80], Loss: 0.0659\n",
      "Epoch [48/80], Loss: 0.0659\n",
      "Epoch [49/80], Loss: 0.0659\n",
      "Epoch [50/80], Loss: 0.0659\n",
      "Epoch [51/80], Loss: 0.0659\n",
      "Epoch [52/80], Loss: 0.0659\n",
      "Epoch [53/80], Loss: 0.0659\n",
      "Epoch [54/80], Loss: 0.0659\n",
      "Epoch [55/80], Loss: 0.0659\n",
      "Epoch [56/80], Loss: 0.0659\n",
      "Epoch [57/80], Loss: 0.0659\n",
      "Epoch [58/80], Loss: 0.0659\n",
      "Epoch [59/80], Loss: 0.0659\n",
      "Epoch [60/80], Loss: 0.0659\n",
      "Epoch [61/80], Loss: 0.0659\n",
      "Epoch [62/80], Loss: 0.0659\n",
      "Epoch [63/80], Loss: 0.0659\n",
      "Epoch [64/80], Loss: 0.0659\n",
      "Epoch [65/80], Loss: 0.0659\n",
      "Epoch [66/80], Loss: 0.0659\n",
      "Epoch [67/80], Loss: 0.0659\n",
      "Epoch [68/80], Loss: 0.0659\n",
      "Epoch [69/80], Loss: 0.0659\n",
      "Epoch [70/80], Loss: 0.0659\n",
      "Epoch [71/80], Loss: 0.0659\n",
      "Epoch [72/80], Loss: 0.0659\n",
      "Epoch [73/80], Loss: 0.0659\n",
      "Epoch [74/80], Loss: 0.0659\n",
      "Epoch [75/80], Loss: 0.0659\n",
      "Epoch [76/80], Loss: 0.0659\n",
      "Epoch [77/80], Loss: 0.0659\n",
      "Epoch [78/80], Loss: 0.0659\n",
      "Epoch [79/80], Loss: 0.0659\n",
      "Epoch [80/80], Loss: 0.0659\n",
      "test performance :  [96.97999573 94.16499329 91.31999207 88.03499603 86.69999695 83.82666779\n",
      " 80.39571381 77.96375275 75.66222382 73.40899658]\n",
      "individual errors:  [tensor(87.6900, device='cuda:0'), tensor(78.5500, device='cuda:0'), tensor(74.9200, device='cuda:0'), tensor(65.2400, device='cuda:0'), tensor(72.3500, device='cuda:0'), tensor(67.0700, device='cuda:0'), tensor(72.0500, device='cuda:0'), tensor(72.1700, device='cuda:0'), tensor(70.0200, device='cuda:0'), tensor(74.0300, device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "lam = 5e-6\n",
    "resewc = run_simulation( get_random_feature_model(),train_loader,test_loader,EWC(lam=lam),num_epochs=num_epochs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 5e-06\n",
      "Epoch [1/80], Loss: 0.1068\n",
      "Epoch [2/80], Loss: 0.0253\n",
      "Epoch [3/80], Loss: 0.0202\n",
      "Epoch [4/80], Loss: 0.0179\n",
      "Epoch [5/80], Loss: 0.0167\n",
      "Epoch [6/80], Loss: 0.0159\n",
      "Epoch [7/80], Loss: 0.0154\n",
      "Epoch [8/80], Loss: 0.0149\n",
      "Epoch [9/80], Loss: 0.0146\n",
      "Epoch [10/80], Loss: 0.0143\n",
      "Epoch [11/80], Loss: 0.0140\n",
      "Epoch [12/80], Loss: 0.0138\n",
      "Epoch [13/80], Loss: 0.0136\n",
      "Epoch [14/80], Loss: 0.0135\n",
      "Epoch [15/80], Loss: 0.0133\n",
      "Epoch [16/80], Loss: 0.0132\n",
      "Epoch [17/80], Loss: 0.0131\n",
      "Epoch [18/80], Loss: 0.0130\n",
      "Epoch [19/80], Loss: 0.0129\n",
      "Epoch [20/80], Loss: 0.0128\n",
      "Epoch [21/80], Loss: 0.0127\n",
      "Epoch [22/80], Loss: 0.0126\n",
      "Epoch [23/80], Loss: 0.0126\n",
      "Epoch [24/80], Loss: 0.0125\n",
      "Epoch [25/80], Loss: 0.0124\n",
      "Epoch [26/80], Loss: 0.0124\n",
      "Epoch [27/80], Loss: 0.0123\n",
      "Epoch [28/80], Loss: 0.0123\n",
      "Epoch [29/80], Loss: 0.0123\n",
      "Epoch [30/80], Loss: 0.0122\n",
      "Epoch [31/80], Loss: 0.0122\n",
      "Epoch [32/80], Loss: 0.0122\n",
      "Epoch [33/80], Loss: 0.0121\n",
      "Epoch [34/80], Loss: 0.0121\n",
      "Epoch [35/80], Loss: 0.0121\n",
      "Epoch [36/80], Loss: 0.0120\n",
      "Epoch [37/80], Loss: 0.0120\n",
      "Epoch [38/80], Loss: 0.0120\n",
      "Epoch [39/80], Loss: 0.0120\n",
      "Epoch [40/80], Loss: 0.0120\n",
      "Epoch [41/80], Loss: 0.0119\n",
      "Epoch [42/80], Loss: 0.0119\n",
      "Epoch [43/80], Loss: 0.0119\n",
      "Epoch [44/80], Loss: 0.0119\n",
      "Epoch [45/80], Loss: 0.0119\n",
      "Epoch [46/80], Loss: 0.0118\n",
      "Epoch [47/80], Loss: 0.0118\n",
      "Epoch [48/80], Loss: 0.0118\n",
      "Epoch [49/80], Loss: 0.0118\n",
      "Epoch [50/80], Loss: 0.0118\n",
      "Epoch [51/80], Loss: 0.0118\n",
      "Epoch [52/80], Loss: 0.0118\n",
      "Epoch [53/80], Loss: 0.0118\n",
      "Epoch [54/80], Loss: 0.0117\n",
      "Epoch [55/80], Loss: 0.0117\n",
      "Epoch [56/80], Loss: 0.0117\n",
      "Epoch [57/80], Loss: 0.0117\n",
      "Epoch [58/80], Loss: 0.0117\n",
      "Epoch [59/80], Loss: 0.0117\n",
      "Epoch [60/80], Loss: 0.0117\n",
      "Epoch [61/80], Loss: 0.0117\n",
      "Epoch [62/80], Loss: 0.0117\n",
      "Epoch [63/80], Loss: 0.0117\n",
      "Epoch [64/80], Loss: 0.0117\n",
      "Epoch [65/80], Loss: 0.0116\n",
      "Epoch [66/80], Loss: 0.0116\n",
      "Epoch [67/80], Loss: 0.0116\n",
      "Epoch [68/80], Loss: 0.0116\n",
      "Epoch [69/80], Loss: 0.0116\n",
      "Epoch [70/80], Loss: 0.0116\n",
      "Epoch [71/80], Loss: 0.0116\n",
      "Epoch [72/80], Loss: 0.0116\n",
      "Epoch [73/80], Loss: 0.0116\n",
      "Epoch [74/80], Loss: 0.0116\n",
      "Epoch [75/80], Loss: 0.0116\n",
      "Epoch [76/80], Loss: 0.0116\n",
      "Epoch [77/80], Loss: 0.0116\n",
      "Epoch [78/80], Loss: 0.0116\n",
      "Epoch [79/80], Loss: 0.0116\n",
      "Epoch [80/80], Loss: 0.0116\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task data norm and number entries: tensor(309806.8750, device='cuda:0') torch.Size([47050])\n",
      "test performance :  [96.93999481  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "individual errors:  [tensor(96.9400, device='cuda:0')]\n",
      "Epoch [1/80], Loss: 0.1653\n",
      "Epoch [2/80], Loss: 0.0513\n",
      "Epoch [3/80], Loss: 0.0506\n",
      "Epoch [4/80], Loss: 0.0505\n",
      "Epoch [5/80], Loss: 0.0505\n",
      "Epoch [6/80], Loss: 0.0505\n",
      "Epoch [7/80], Loss: 0.0505\n",
      "Epoch [8/80], Loss: 0.0505\n",
      "Epoch [9/80], Loss: 0.0505\n",
      "Epoch [10/80], Loss: 0.0505\n",
      "Epoch [11/80], Loss: 0.0505\n",
      "Epoch [12/80], Loss: 0.0505\n",
      "Epoch [13/80], Loss: 0.0505\n",
      "Epoch [14/80], Loss: 0.0505\n",
      "Epoch [15/80], Loss: 0.0505\n",
      "Epoch [16/80], Loss: 0.0505\n",
      "Epoch [17/80], Loss: 0.0505\n",
      "Epoch [18/80], Loss: 0.0505\n",
      "Epoch [19/80], Loss: 0.0505\n",
      "Epoch [20/80], Loss: 0.0505\n",
      "Epoch [21/80], Loss: 0.0505\n",
      "Epoch [22/80], Loss: 0.0505\n",
      "Epoch [23/80], Loss: 0.0505\n",
      "Epoch [24/80], Loss: 0.0505\n",
      "Epoch [25/80], Loss: 0.0505\n",
      "Epoch [26/80], Loss: 0.0505\n",
      "Epoch [27/80], Loss: 0.0505\n",
      "Epoch [28/80], Loss: 0.0505\n",
      "Epoch [29/80], Loss: 0.0505\n",
      "Epoch [30/80], Loss: 0.0505\n",
      "Epoch [31/80], Loss: 0.0505\n",
      "Epoch [32/80], Loss: 0.0505\n",
      "Epoch [33/80], Loss: 0.0505\n",
      "Epoch [34/80], Loss: 0.0505\n",
      "Epoch [35/80], Loss: 0.0505\n",
      "Epoch [36/80], Loss: 0.0505\n",
      "Epoch [37/80], Loss: 0.0505\n",
      "Epoch [38/80], Loss: 0.0505\n",
      "Epoch [39/80], Loss: 0.0505\n",
      "Epoch [40/80], Loss: 0.0505\n",
      "Epoch [41/80], Loss: 0.0505\n",
      "Epoch [42/80], Loss: 0.0505\n",
      "Epoch [43/80], Loss: 0.0505\n",
      "Epoch [44/80], Loss: 0.0505\n",
      "Epoch [45/80], Loss: 0.0505\n",
      "Epoch [46/80], Loss: 0.0505\n",
      "Epoch [47/80], Loss: 0.0505\n",
      "Epoch [48/80], Loss: 0.0505\n",
      "Epoch [49/80], Loss: 0.0505\n",
      "Epoch [50/80], Loss: 0.0505\n",
      "Epoch [51/80], Loss: 0.0505\n",
      "Epoch [52/80], Loss: 0.0505\n",
      "Epoch [53/80], Loss: 0.0505\n",
      "Epoch [54/80], Loss: 0.0505\n",
      "Epoch [55/80], Loss: 0.0505\n",
      "Epoch [56/80], Loss: 0.0505\n",
      "Epoch [57/80], Loss: 0.0505\n",
      "Epoch [58/80], Loss: 0.0505\n",
      "Epoch [59/80], Loss: 0.0505\n",
      "Epoch [60/80], Loss: 0.0505\n",
      "Epoch [61/80], Loss: 0.0505\n",
      "Epoch [62/80], Loss: 0.0505\n",
      "Epoch [63/80], Loss: 0.0505\n",
      "Epoch [64/80], Loss: 0.0505\n",
      "Epoch [65/80], Loss: 0.0505\n",
      "Epoch [66/80], Loss: 0.0505\n",
      "Epoch [67/80], Loss: 0.0505\n",
      "Epoch [68/80], Loss: 0.0505\n",
      "Epoch [69/80], Loss: 0.0505\n",
      "Epoch [70/80], Loss: 0.0505\n",
      "Epoch [71/80], Loss: 0.0505\n",
      "Epoch [72/80], Loss: 0.0505\n",
      "Epoch [73/80], Loss: 0.0505\n",
      "Epoch [74/80], Loss: 0.0505\n",
      "Epoch [75/80], Loss: 0.0505\n",
      "Epoch [76/80], Loss: 0.0505\n",
      "Epoch [77/80], Loss: 0.0505\n",
      "Epoch [78/80], Loss: 0.0505\n",
      "Epoch [79/80], Loss: 0.0505\n",
      "Epoch [80/80], Loss: 0.0505\n",
      "test performance :  [96.93999481 89.94499207  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "individual errors:  [tensor(94.5900, device='cuda:0'), tensor(85.3000, device='cuda:0')]\n",
      "Epoch [1/80], Loss: 0.1464\n",
      "Epoch [2/80], Loss: 0.0591\n",
      "Epoch [3/80], Loss: 0.0587\n",
      "Epoch [4/80], Loss: 0.0586\n",
      "Epoch [5/80], Loss: 0.0586\n",
      "Epoch [6/80], Loss: 0.0586\n",
      "Epoch [7/80], Loss: 0.0586\n",
      "Epoch [8/80], Loss: 0.0586\n",
      "Epoch [9/80], Loss: 0.0586\n",
      "Epoch [10/80], Loss: 0.0586\n",
      "Epoch [11/80], Loss: 0.0586\n",
      "Epoch [12/80], Loss: 0.0586\n",
      "Epoch [13/80], Loss: 0.0586\n",
      "Epoch [14/80], Loss: 0.0586\n",
      "Epoch [15/80], Loss: 0.0586\n",
      "Epoch [16/80], Loss: 0.0586\n",
      "Epoch [17/80], Loss: 0.0586\n",
      "Epoch [18/80], Loss: 0.0586\n",
      "Epoch [19/80], Loss: 0.0586\n",
      "Epoch [20/80], Loss: 0.0586\n",
      "Epoch [21/80], Loss: 0.0586\n",
      "Epoch [22/80], Loss: 0.0586\n",
      "Epoch [23/80], Loss: 0.0586\n",
      "Epoch [24/80], Loss: 0.0586\n",
      "Epoch [25/80], Loss: 0.0586\n",
      "Epoch [26/80], Loss: 0.0586\n",
      "Epoch [27/80], Loss: 0.0586\n",
      "Epoch [28/80], Loss: 0.0586\n",
      "Epoch [29/80], Loss: 0.0586\n",
      "Epoch [30/80], Loss: 0.0586\n",
      "Epoch [31/80], Loss: 0.0586\n",
      "Epoch [32/80], Loss: 0.0586\n",
      "Epoch [33/80], Loss: 0.0586\n",
      "Epoch [34/80], Loss: 0.0586\n",
      "Epoch [35/80], Loss: 0.0586\n",
      "Epoch [36/80], Loss: 0.0586\n",
      "Epoch [37/80], Loss: 0.0586\n",
      "Epoch [38/80], Loss: 0.0586\n",
      "Epoch [39/80], Loss: 0.0586\n",
      "Epoch [40/80], Loss: 0.0586\n",
      "Epoch [41/80], Loss: 0.0586\n",
      "Epoch [42/80], Loss: 0.0586\n",
      "Epoch [43/80], Loss: 0.0586\n",
      "Epoch [44/80], Loss: 0.0586\n",
      "Epoch [45/80], Loss: 0.0586\n",
      "Epoch [46/80], Loss: 0.0586\n",
      "Epoch [47/80], Loss: 0.0586\n",
      "Epoch [48/80], Loss: 0.0586\n",
      "Epoch [49/80], Loss: 0.0586\n",
      "Epoch [50/80], Loss: 0.0586\n",
      "Epoch [51/80], Loss: 0.0586\n",
      "Epoch [52/80], Loss: 0.0586\n",
      "Epoch [53/80], Loss: 0.0586\n",
      "Epoch [54/80], Loss: 0.0586\n",
      "Epoch [55/80], Loss: 0.0586\n",
      "Epoch [56/80], Loss: 0.0586\n",
      "Epoch [57/80], Loss: 0.0586\n",
      "Epoch [58/80], Loss: 0.0586\n",
      "Epoch [59/80], Loss: 0.0586\n",
      "Epoch [60/80], Loss: 0.0586\n",
      "Epoch [61/80], Loss: 0.0586\n",
      "Epoch [62/80], Loss: 0.0586\n",
      "Epoch [63/80], Loss: 0.0586\n",
      "Epoch [64/80], Loss: 0.0586\n",
      "Epoch [65/80], Loss: 0.0586\n",
      "Epoch [66/80], Loss: 0.0586\n",
      "Epoch [67/80], Loss: 0.0586\n",
      "Epoch [68/80], Loss: 0.0586\n",
      "Epoch [69/80], Loss: 0.0586\n",
      "Epoch [70/80], Loss: 0.0586\n",
      "Epoch [71/80], Loss: 0.0586\n",
      "Epoch [72/80], Loss: 0.0586\n",
      "Epoch [73/80], Loss: 0.0586\n",
      "Epoch [74/80], Loss: 0.0586\n",
      "Epoch [75/80], Loss: 0.0586\n",
      "Epoch [76/80], Loss: 0.0586\n",
      "Epoch [77/80], Loss: 0.0586\n",
      "Epoch [78/80], Loss: 0.0586\n",
      "Epoch [79/80], Loss: 0.0586\n",
      "Epoch [80/80], Loss: 0.0586\n",
      "test performance :  [96.93999481 89.94499207 85.92666626  0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "individual errors:  [tensor(92.7100, device='cuda:0'), tensor(83.0900, device='cuda:0'), tensor(81.9800, device='cuda:0')]\n",
      "Epoch [1/80], Loss: 0.1471\n",
      "Epoch [2/80], Loss: 0.0619\n",
      "Epoch [3/80], Loss: 0.0618\n",
      "Epoch [4/80], Loss: 0.0618\n",
      "Epoch [5/80], Loss: 0.0618\n",
      "Epoch [6/80], Loss: 0.0618\n",
      "Epoch [7/80], Loss: 0.0618\n",
      "Epoch [8/80], Loss: 0.0618\n",
      "Epoch [9/80], Loss: 0.0618\n",
      "Epoch [10/80], Loss: 0.0618\n",
      "Epoch [11/80], Loss: 0.0618\n",
      "Epoch [12/80], Loss: 0.0618\n",
      "Epoch [13/80], Loss: 0.0618\n",
      "Epoch [14/80], Loss: 0.0618\n",
      "Epoch [15/80], Loss: 0.0618\n",
      "Epoch [16/80], Loss: 0.0618\n",
      "Epoch [17/80], Loss: 0.0618\n",
      "Epoch [18/80], Loss: 0.0618\n",
      "Epoch [19/80], Loss: 0.0618\n",
      "Epoch [20/80], Loss: 0.0618\n",
      "Epoch [21/80], Loss: 0.0618\n",
      "Epoch [22/80], Loss: 0.0618\n",
      "Epoch [23/80], Loss: 0.0618\n",
      "Epoch [24/80], Loss: 0.0618\n",
      "Epoch [25/80], Loss: 0.0618\n",
      "Epoch [26/80], Loss: 0.0618\n",
      "Epoch [27/80], Loss: 0.0618\n",
      "Epoch [28/80], Loss: 0.0618\n",
      "Epoch [29/80], Loss: 0.0618\n",
      "Epoch [30/80], Loss: 0.0618\n",
      "Epoch [31/80], Loss: 0.0618\n",
      "Epoch [32/80], Loss: 0.0618\n",
      "Epoch [33/80], Loss: 0.0618\n",
      "Epoch [34/80], Loss: 0.0618\n",
      "Epoch [35/80], Loss: 0.0618\n",
      "Epoch [36/80], Loss: 0.0618\n",
      "Epoch [37/80], Loss: 0.0618\n",
      "Epoch [38/80], Loss: 0.0618\n",
      "Epoch [39/80], Loss: 0.0618\n",
      "Epoch [40/80], Loss: 0.0618\n",
      "Epoch [41/80], Loss: 0.0618\n",
      "Epoch [42/80], Loss: 0.0618\n",
      "Epoch [43/80], Loss: 0.0618\n",
      "Epoch [44/80], Loss: 0.0618\n",
      "Epoch [45/80], Loss: 0.0618\n",
      "Epoch [46/80], Loss: 0.0618\n",
      "Epoch [47/80], Loss: 0.0618\n",
      "Epoch [48/80], Loss: 0.0618\n",
      "Epoch [49/80], Loss: 0.0618\n",
      "Epoch [50/80], Loss: 0.0618\n",
      "Epoch [51/80], Loss: 0.0618\n",
      "Epoch [52/80], Loss: 0.0618\n",
      "Epoch [53/80], Loss: 0.0618\n",
      "Epoch [54/80], Loss: 0.0618\n",
      "Epoch [55/80], Loss: 0.0618\n",
      "Epoch [56/80], Loss: 0.0618\n",
      "Epoch [57/80], Loss: 0.0618\n",
      "Epoch [58/80], Loss: 0.0618\n",
      "Epoch [59/80], Loss: 0.0618\n",
      "Epoch [60/80], Loss: 0.0618\n",
      "Epoch [61/80], Loss: 0.0618\n",
      "Epoch [62/80], Loss: 0.0618\n",
      "Epoch [63/80], Loss: 0.0618\n",
      "Epoch [64/80], Loss: 0.0618\n",
      "Epoch [65/80], Loss: 0.0618\n",
      "Epoch [66/80], Loss: 0.0618\n",
      "Epoch [67/80], Loss: 0.0618\n",
      "Epoch [68/80], Loss: 0.0618\n",
      "Epoch [69/80], Loss: 0.0618\n",
      "Epoch [70/80], Loss: 0.0618\n",
      "Epoch [71/80], Loss: 0.0618\n",
      "Epoch [72/80], Loss: 0.0618\n",
      "Epoch [73/80], Loss: 0.0618\n",
      "Epoch [74/80], Loss: 0.0618\n",
      "Epoch [75/80], Loss: 0.0618\n",
      "Epoch [76/80], Loss: 0.0618\n",
      "Epoch [77/80], Loss: 0.0618\n",
      "Epoch [78/80], Loss: 0.0618\n",
      "Epoch [79/80], Loss: 0.0618\n",
      "Epoch [80/80], Loss: 0.0618\n",
      "test performance :  [96.93999481 89.94499207 85.92666626 80.84749603  0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "individual errors:  [tensor(91.9200, device='cuda:0'), tensor(77.6700, device='cuda:0'), tensor(75.4900, device='cuda:0'), tensor(78.3100, device='cuda:0')]\n",
      "Epoch [1/80], Loss: 0.1310\n",
      "Epoch [2/80], Loss: 0.0599\n",
      "Epoch [3/80], Loss: 0.0599\n",
      "Epoch [4/80], Loss: 0.0599\n",
      "Epoch [5/80], Loss: 0.0599\n",
      "Epoch [6/80], Loss: 0.0599\n",
      "Epoch [7/80], Loss: 0.0599\n",
      "Epoch [8/80], Loss: 0.0599\n",
      "Epoch [9/80], Loss: 0.0599\n",
      "Epoch [10/80], Loss: 0.0599\n",
      "Epoch [11/80], Loss: 0.0599\n",
      "Epoch [12/80], Loss: 0.0599\n",
      "Epoch [13/80], Loss: 0.0599\n",
      "Epoch [14/80], Loss: 0.0599\n",
      "Epoch [15/80], Loss: 0.0599\n",
      "Epoch [16/80], Loss: 0.0599\n",
      "Epoch [17/80], Loss: 0.0599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/80], Loss: 0.0599\n",
      "Epoch [19/80], Loss: 0.0599\n",
      "Epoch [20/80], Loss: 0.0599\n",
      "Epoch [21/80], Loss: 0.0599\n",
      "Epoch [22/80], Loss: 0.0599\n",
      "Epoch [23/80], Loss: 0.0599\n",
      "Epoch [24/80], Loss: 0.0599\n",
      "Epoch [25/80], Loss: 0.0599\n",
      "Epoch [26/80], Loss: 0.0599\n",
      "Epoch [27/80], Loss: 0.0599\n",
      "Epoch [28/80], Loss: 0.0599\n",
      "Epoch [29/80], Loss: 0.0599\n",
      "Epoch [30/80], Loss: 0.0599\n",
      "Epoch [31/80], Loss: 0.0599\n",
      "Epoch [32/80], Loss: 0.0599\n",
      "Epoch [33/80], Loss: 0.0599\n",
      "Epoch [34/80], Loss: 0.0599\n",
      "Epoch [35/80], Loss: 0.0599\n",
      "Epoch [36/80], Loss: 0.0599\n",
      "Epoch [37/80], Loss: 0.0599\n",
      "Epoch [38/80], Loss: 0.0599\n",
      "Epoch [39/80], Loss: 0.0599\n",
      "Epoch [40/80], Loss: 0.0599\n",
      "Epoch [41/80], Loss: 0.0599\n",
      "Epoch [42/80], Loss: 0.0599\n",
      "Epoch [43/80], Loss: 0.0599\n",
      "Epoch [44/80], Loss: 0.0599\n",
      "Epoch [45/80], Loss: 0.0599\n",
      "Epoch [46/80], Loss: 0.0599\n",
      "Epoch [47/80], Loss: 0.0599\n",
      "Epoch [48/80], Loss: 0.0599\n",
      "Epoch [49/80], Loss: 0.0599\n",
      "Epoch [50/80], Loss: 0.0599\n",
      "Epoch [51/80], Loss: 0.0599\n",
      "Epoch [52/80], Loss: 0.0599\n",
      "Epoch [53/80], Loss: 0.0599\n",
      "Epoch [54/80], Loss: 0.0599\n",
      "Epoch [55/80], Loss: 0.0599\n",
      "Epoch [56/80], Loss: 0.0599\n",
      "Epoch [57/80], Loss: 0.0599\n",
      "Epoch [58/80], Loss: 0.0599\n",
      "Epoch [59/80], Loss: 0.0599\n",
      "Epoch [60/80], Loss: 0.0599\n",
      "Epoch [61/80], Loss: 0.0599\n",
      "Epoch [62/80], Loss: 0.0599\n",
      "Epoch [63/80], Loss: 0.0599\n",
      "Epoch [64/80], Loss: 0.0599\n",
      "Epoch [65/80], Loss: 0.0599\n",
      "Epoch [66/80], Loss: 0.0599\n",
      "Epoch [67/80], Loss: 0.0599\n",
      "Epoch [68/80], Loss: 0.0599\n",
      "Epoch [69/80], Loss: 0.0599\n",
      "Epoch [70/80], Loss: 0.0599\n",
      "Epoch [71/80], Loss: 0.0599\n",
      "Epoch [72/80], Loss: 0.0599\n",
      "Epoch [73/80], Loss: 0.0599\n",
      "Epoch [74/80], Loss: 0.0599\n",
      "Epoch [75/80], Loss: 0.0599\n",
      "Epoch [76/80], Loss: 0.0599\n",
      "Epoch [77/80], Loss: 0.0599\n",
      "Epoch [78/80], Loss: 0.0599\n",
      "Epoch [79/80], Loss: 0.0599\n",
      "Epoch [80/80], Loss: 0.0599\n",
      "test performance :  [96.93999481 89.94499207 85.92666626 80.84749603 79.96199799  0.\n",
      "  0.          0.          0.          0.        ]\n",
      "individual errors:  [tensor(92.7100, device='cuda:0'), tensor(78.8700, device='cuda:0'), tensor(77.6600, device='cuda:0'), tensor(75.1600, device='cuda:0'), tensor(75.4100, device='cuda:0')]\n",
      "Epoch [1/80], Loss: 0.1341\n",
      "Epoch [2/80], Loss: 0.0683\n",
      "Epoch [3/80], Loss: 0.0683\n",
      "Epoch [4/80], Loss: 0.0683\n",
      "Epoch [5/80], Loss: 0.0683\n",
      "Epoch [6/80], Loss: 0.0683\n",
      "Epoch [7/80], Loss: 0.0683\n",
      "Epoch [8/80], Loss: 0.0683\n",
      "Epoch [9/80], Loss: 0.0683\n",
      "Epoch [10/80], Loss: 0.0683\n",
      "Epoch [11/80], Loss: 0.0683\n",
      "Epoch [12/80], Loss: 0.0683\n",
      "Epoch [13/80], Loss: 0.0683\n",
      "Epoch [14/80], Loss: 0.0683\n",
      "Epoch [15/80], Loss: 0.0683\n",
      "Epoch [16/80], Loss: 0.0683\n",
      "Epoch [17/80], Loss: 0.0683\n",
      "Epoch [18/80], Loss: 0.0683\n",
      "Epoch [19/80], Loss: 0.0683\n",
      "Epoch [20/80], Loss: 0.0683\n",
      "Epoch [21/80], Loss: 0.0683\n",
      "Epoch [22/80], Loss: 0.0683\n",
      "Epoch [23/80], Loss: 0.0683\n",
      "Epoch [24/80], Loss: 0.0683\n",
      "Epoch [25/80], Loss: 0.0683\n",
      "Epoch [26/80], Loss: 0.0683\n",
      "Epoch [27/80], Loss: 0.0683\n",
      "Epoch [28/80], Loss: 0.0683\n",
      "Epoch [29/80], Loss: 0.0683\n",
      "Epoch [30/80], Loss: 0.0683\n",
      "Epoch [31/80], Loss: 0.0683\n",
      "Epoch [32/80], Loss: 0.0683\n",
      "Epoch [33/80], Loss: 0.0683\n",
      "Epoch [34/80], Loss: 0.0683\n",
      "Epoch [35/80], Loss: 0.0683\n",
      "Epoch [36/80], Loss: 0.0683\n",
      "Epoch [37/80], Loss: 0.0683\n",
      "Epoch [38/80], Loss: 0.0683\n",
      "Epoch [39/80], Loss: 0.0683\n",
      "Epoch [40/80], Loss: 0.0683\n",
      "Epoch [41/80], Loss: 0.0683\n",
      "Epoch [42/80], Loss: 0.0683\n",
      "Epoch [43/80], Loss: 0.0683\n",
      "Epoch [44/80], Loss: 0.0683\n",
      "Epoch [45/80], Loss: 0.0683\n",
      "Epoch [46/80], Loss: 0.0683\n",
      "Epoch [47/80], Loss: 0.0683\n",
      "Epoch [48/80], Loss: 0.0683\n",
      "Epoch [49/80], Loss: 0.0683\n",
      "Epoch [50/80], Loss: 0.0683\n",
      "Epoch [51/80], Loss: 0.0683\n",
      "Epoch [52/80], Loss: 0.0683\n",
      "Epoch [53/80], Loss: 0.0683\n",
      "Epoch [54/80], Loss: 0.0683\n",
      "Epoch [55/80], Loss: 0.0683\n",
      "Epoch [56/80], Loss: 0.0683\n",
      "Epoch [57/80], Loss: 0.0683\n",
      "Epoch [58/80], Loss: 0.0683\n",
      "Epoch [59/80], Loss: 0.0683\n",
      "Epoch [60/80], Loss: 0.0683\n",
      "Epoch [61/80], Loss: 0.0683\n",
      "Epoch [62/80], Loss: 0.0683\n",
      "Epoch [63/80], Loss: 0.0683\n",
      "Epoch [64/80], Loss: 0.0683\n",
      "Epoch [65/80], Loss: 0.0683\n",
      "Epoch [66/80], Loss: 0.0683\n",
      "Epoch [67/80], Loss: 0.0683\n",
      "Epoch [68/80], Loss: 0.0683\n",
      "Epoch [69/80], Loss: 0.0683\n",
      "Epoch [70/80], Loss: 0.0683\n",
      "Epoch [71/80], Loss: 0.0683\n",
      "Epoch [72/80], Loss: 0.0683\n",
      "Epoch [73/80], Loss: 0.0683\n",
      "Epoch [74/80], Loss: 0.0683\n",
      "Epoch [75/80], Loss: 0.0683\n",
      "Epoch [76/80], Loss: 0.0683\n",
      "Epoch [77/80], Loss: 0.0683\n",
      "Epoch [78/80], Loss: 0.0683\n",
      "Epoch [79/80], Loss: 0.0683\n",
      "Epoch [80/80], Loss: 0.0683\n",
      "test performance :  [96.93999481 89.94499207 85.92666626 80.84749603 79.96199799 74.68666077\n",
      "  0.          0.          0.          0.        ]\n",
      "individual errors:  [tensor(89.0600, device='cuda:0'), tensor(73.1100, device='cuda:0'), tensor(71.6800, device='cuda:0'), tensor(71.1200, device='cuda:0'), tensor(68.7600, device='cuda:0'), tensor(74.3900, device='cuda:0')]\n",
      "Epoch [1/80], Loss: 0.1352\n",
      "Epoch [2/80], Loss: 0.0711\n",
      "Epoch [3/80], Loss: 0.0711\n",
      "Epoch [4/80], Loss: 0.0711\n",
      "Epoch [5/80], Loss: 0.0711\n",
      "Epoch [6/80], Loss: 0.0711\n",
      "Epoch [7/80], Loss: 0.0711\n",
      "Epoch [8/80], Loss: 0.0711\n",
      "Epoch [9/80], Loss: 0.0711\n",
      "Epoch [10/80], Loss: 0.0711\n",
      "Epoch [11/80], Loss: 0.0711\n",
      "Epoch [12/80], Loss: 0.0711\n",
      "Epoch [13/80], Loss: 0.0711\n",
      "Epoch [14/80], Loss: 0.0711\n",
      "Epoch [15/80], Loss: 0.0711\n",
      "Epoch [16/80], Loss: 0.0711\n",
      "Epoch [17/80], Loss: 0.0711\n",
      "Epoch [18/80], Loss: 0.0711\n",
      "Epoch [19/80], Loss: 0.0711\n",
      "Epoch [20/80], Loss: 0.0711\n",
      "Epoch [21/80], Loss: 0.0711\n",
      "Epoch [22/80], Loss: 0.0711\n",
      "Epoch [23/80], Loss: 0.0711\n",
      "Epoch [24/80], Loss: 0.0711\n",
      "Epoch [25/80], Loss: 0.0711\n",
      "Epoch [26/80], Loss: 0.0711\n",
      "Epoch [27/80], Loss: 0.0711\n",
      "Epoch [28/80], Loss: 0.0711\n",
      "Epoch [29/80], Loss: 0.0711\n",
      "Epoch [30/80], Loss: 0.0711\n",
      "Epoch [31/80], Loss: 0.0711\n",
      "Epoch [32/80], Loss: 0.0711\n",
      "Epoch [33/80], Loss: 0.0711\n",
      "Epoch [34/80], Loss: 0.0711\n",
      "Epoch [35/80], Loss: 0.0711\n",
      "Epoch [36/80], Loss: 0.0711\n",
      "Epoch [37/80], Loss: 0.0711\n",
      "Epoch [38/80], Loss: 0.0711\n",
      "Epoch [39/80], Loss: 0.0711\n",
      "Epoch [40/80], Loss: 0.0711\n",
      "Epoch [41/80], Loss: 0.0711\n",
      "Epoch [42/80], Loss: 0.0711\n",
      "Epoch [43/80], Loss: 0.0711\n",
      "Epoch [44/80], Loss: 0.0711\n",
      "Epoch [45/80], Loss: 0.0711\n",
      "Epoch [46/80], Loss: 0.0711\n",
      "Epoch [47/80], Loss: 0.0711\n",
      "Epoch [48/80], Loss: 0.0711\n",
      "Epoch [49/80], Loss: 0.0711\n",
      "Epoch [50/80], Loss: 0.0711\n",
      "Epoch [51/80], Loss: 0.0711\n",
      "Epoch [52/80], Loss: 0.0711\n",
      "Epoch [53/80], Loss: 0.0711\n",
      "Epoch [54/80], Loss: 0.0711\n",
      "Epoch [55/80], Loss: 0.0711\n",
      "Epoch [56/80], Loss: 0.0711\n",
      "Epoch [57/80], Loss: 0.0711\n",
      "Epoch [58/80], Loss: 0.0711\n",
      "Epoch [59/80], Loss: 0.0711\n",
      "Epoch [60/80], Loss: 0.0711\n",
      "Epoch [61/80], Loss: 0.0711\n",
      "Epoch [62/80], Loss: 0.0711\n",
      "Epoch [63/80], Loss: 0.0711\n",
      "Epoch [64/80], Loss: 0.0711\n",
      "Epoch [65/80], Loss: 0.0711\n",
      "Epoch [66/80], Loss: 0.0711\n",
      "Epoch [67/80], Loss: 0.0711\n",
      "Epoch [68/80], Loss: 0.0711\n",
      "Epoch [69/80], Loss: 0.0711\n",
      "Epoch [70/80], Loss: 0.0711\n",
      "Epoch [71/80], Loss: 0.0711\n",
      "Epoch [72/80], Loss: 0.0711\n",
      "Epoch [73/80], Loss: 0.0711\n",
      "Epoch [74/80], Loss: 0.0711\n",
      "Epoch [75/80], Loss: 0.0711\n",
      "Epoch [76/80], Loss: 0.0711\n",
      "Epoch [77/80], Loss: 0.0711\n",
      "Epoch [78/80], Loss: 0.0711\n",
      "Epoch [79/80], Loss: 0.0711\n",
      "Epoch [80/80], Loss: 0.0711\n",
      "test performance :  [96.93999481 89.94499207 85.92666626 80.84749603 79.96199799 74.68666077\n",
      " 73.85857391  0.          0.          0.        ]\n",
      "individual errors:  [tensor(89.1800, device='cuda:0'), tensor(72.0200, device='cuda:0'), tensor(71.4200, device='cuda:0'), tensor(70.9300, device='cuda:0'), tensor(69.9900, device='cuda:0'), tensor(71.3400, device='cuda:0'), tensor(72.1300, device='cuda:0')]\n",
      "Epoch [1/80], Loss: 0.1432\n",
      "Epoch [2/80], Loss: 0.0751\n",
      "Epoch [3/80], Loss: 0.0751\n",
      "Epoch [4/80], Loss: 0.0751\n",
      "Epoch [5/80], Loss: 0.0751\n",
      "Epoch [6/80], Loss: 0.0751\n",
      "Epoch [7/80], Loss: 0.0751\n",
      "Epoch [8/80], Loss: 0.0751\n",
      "Epoch [9/80], Loss: 0.0751\n",
      "Epoch [10/80], Loss: 0.0751\n",
      "Epoch [11/80], Loss: 0.0751\n",
      "Epoch [12/80], Loss: 0.0751\n",
      "Epoch [13/80], Loss: 0.0751\n",
      "Epoch [14/80], Loss: 0.0751\n",
      "Epoch [15/80], Loss: 0.0751\n",
      "Epoch [16/80], Loss: 0.0751\n",
      "Epoch [17/80], Loss: 0.0751\n",
      "Epoch [18/80], Loss: 0.0751\n",
      "Epoch [19/80], Loss: 0.0751\n",
      "Epoch [20/80], Loss: 0.0751\n",
      "Epoch [21/80], Loss: 0.0751\n",
      "Epoch [22/80], Loss: 0.0751\n",
      "Epoch [23/80], Loss: 0.0751\n",
      "Epoch [24/80], Loss: 0.0751\n",
      "Epoch [25/80], Loss: 0.0751\n",
      "Epoch [26/80], Loss: 0.0751\n",
      "Epoch [27/80], Loss: 0.0751\n",
      "Epoch [28/80], Loss: 0.0751\n",
      "Epoch [29/80], Loss: 0.0751\n",
      "Epoch [30/80], Loss: 0.0751\n",
      "Epoch [31/80], Loss: 0.0751\n",
      "Epoch [32/80], Loss: 0.0751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [33/80], Loss: 0.0751\n",
      "Epoch [34/80], Loss: 0.0751\n",
      "Epoch [35/80], Loss: 0.0751\n",
      "Epoch [36/80], Loss: 0.0751\n",
      "Epoch [37/80], Loss: 0.0751\n",
      "Epoch [38/80], Loss: 0.0751\n",
      "Epoch [39/80], Loss: 0.0751\n",
      "Epoch [40/80], Loss: 0.0751\n",
      "Epoch [41/80], Loss: 0.0751\n",
      "Epoch [42/80], Loss: 0.0751\n",
      "Epoch [43/80], Loss: 0.0751\n",
      "Epoch [44/80], Loss: 0.0751\n",
      "Epoch [45/80], Loss: 0.0751\n",
      "Epoch [46/80], Loss: 0.0751\n",
      "Epoch [47/80], Loss: 0.0751\n",
      "Epoch [48/80], Loss: 0.0751\n",
      "Epoch [49/80], Loss: 0.0751\n",
      "Epoch [50/80], Loss: 0.0751\n",
      "Epoch [51/80], Loss: 0.0751\n",
      "Epoch [52/80], Loss: 0.0751\n",
      "Epoch [53/80], Loss: 0.0751\n",
      "Epoch [54/80], Loss: 0.0751\n",
      "Epoch [55/80], Loss: 0.0751\n",
      "Epoch [56/80], Loss: 0.0751\n",
      "Epoch [57/80], Loss: 0.0751\n",
      "Epoch [58/80], Loss: 0.0751\n",
      "Epoch [59/80], Loss: 0.0751\n",
      "Epoch [60/80], Loss: 0.0751\n",
      "Epoch [61/80], Loss: 0.0751\n",
      "Epoch [62/80], Loss: 0.0751\n",
      "Epoch [63/80], Loss: 0.0751\n",
      "Epoch [64/80], Loss: 0.0751\n",
      "Epoch [65/80], Loss: 0.0751\n",
      "Epoch [66/80], Loss: 0.0751\n",
      "Epoch [67/80], Loss: 0.0751\n",
      "Epoch [68/80], Loss: 0.0751\n",
      "Epoch [69/80], Loss: 0.0751\n",
      "Epoch [70/80], Loss: 0.0751\n",
      "Epoch [71/80], Loss: 0.0751\n",
      "Epoch [72/80], Loss: 0.0751\n",
      "Epoch [73/80], Loss: 0.0751\n",
      "Epoch [74/80], Loss: 0.0751\n",
      "Epoch [75/80], Loss: 0.0751\n",
      "Epoch [76/80], Loss: 0.0751\n",
      "Epoch [77/80], Loss: 0.0751\n",
      "Epoch [78/80], Loss: 0.0751\n",
      "Epoch [79/80], Loss: 0.0751\n",
      "Epoch [80/80], Loss: 0.0751\n",
      "test performance :  [96.93999481 89.94499207 85.92666626 80.84749603 79.96199799 74.68666077\n",
      " 73.85857391 70.91125488  0.          0.        ]\n",
      "individual errors:  [tensor(88.5800, device='cuda:0'), tensor(68.4300, device='cuda:0'), tensor(70.1200, device='cuda:0'), tensor(67.2400, device='cuda:0'), tensor(66.0700, device='cuda:0'), tensor(69.3900, device='cuda:0'), tensor(67.6400, device='cuda:0'), tensor(69.8200, device='cuda:0')]\n",
      "Epoch [1/80], Loss: 0.1488\n",
      "Epoch [2/80], Loss: 0.0708\n",
      "Epoch [3/80], Loss: 0.0708\n",
      "Epoch [4/80], Loss: 0.0708\n",
      "Epoch [5/80], Loss: 0.0708\n",
      "Epoch [6/80], Loss: 0.0708\n",
      "Epoch [7/80], Loss: 0.0708\n",
      "Epoch [8/80], Loss: 0.0708\n",
      "Epoch [9/80], Loss: 0.0708\n",
      "Epoch [10/80], Loss: 0.0708\n",
      "Epoch [11/80], Loss: 0.0708\n",
      "Epoch [12/80], Loss: 0.0708\n",
      "Epoch [13/80], Loss: 0.0708\n",
      "Epoch [14/80], Loss: 0.0708\n",
      "Epoch [15/80], Loss: 0.0708\n",
      "Epoch [16/80], Loss: 0.0708\n",
      "Epoch [17/80], Loss: 0.0708\n",
      "Epoch [18/80], Loss: 0.0708\n",
      "Epoch [19/80], Loss: 0.0708\n",
      "Epoch [20/80], Loss: 0.0708\n",
      "Epoch [21/80], Loss: 0.0708\n",
      "Epoch [22/80], Loss: 0.0708\n",
      "Epoch [23/80], Loss: 0.0708\n",
      "Epoch [24/80], Loss: 0.0708\n",
      "Epoch [25/80], Loss: 0.0708\n",
      "Epoch [26/80], Loss: 0.0708\n",
      "Epoch [27/80], Loss: 0.0708\n",
      "Epoch [28/80], Loss: 0.0708\n",
      "Epoch [29/80], Loss: 0.0708\n",
      "Epoch [30/80], Loss: 0.0708\n",
      "Epoch [31/80], Loss: 0.0708\n",
      "Epoch [32/80], Loss: 0.0708\n",
      "Epoch [33/80], Loss: 0.0708\n",
      "Epoch [34/80], Loss: 0.0708\n",
      "Epoch [35/80], Loss: 0.0708\n",
      "Epoch [36/80], Loss: 0.0708\n",
      "Epoch [37/80], Loss: 0.0708\n",
      "Epoch [38/80], Loss: 0.0708\n",
      "Epoch [39/80], Loss: 0.0708\n",
      "Epoch [40/80], Loss: 0.0708\n",
      "Epoch [41/80], Loss: 0.0708\n",
      "Epoch [42/80], Loss: 0.0708\n",
      "Epoch [43/80], Loss: 0.0708\n",
      "Epoch [44/80], Loss: 0.0708\n",
      "Epoch [45/80], Loss: 0.0708\n",
      "Epoch [46/80], Loss: 0.0708\n",
      "Epoch [47/80], Loss: 0.0708\n",
      "Epoch [48/80], Loss: 0.0708\n",
      "Epoch [49/80], Loss: 0.0708\n",
      "Epoch [50/80], Loss: 0.0708\n",
      "Epoch [51/80], Loss: 0.0708\n",
      "Epoch [52/80], Loss: 0.0708\n",
      "Epoch [53/80], Loss: 0.0708\n",
      "Epoch [54/80], Loss: 0.0708\n",
      "Epoch [55/80], Loss: 0.0708\n",
      "Epoch [56/80], Loss: 0.0708\n",
      "Epoch [57/80], Loss: 0.0708\n",
      "Epoch [58/80], Loss: 0.0708\n",
      "Epoch [59/80], Loss: 0.0708\n",
      "Epoch [60/80], Loss: 0.0708\n",
      "Epoch [61/80], Loss: 0.0708\n",
      "Epoch [62/80], Loss: 0.0708\n",
      "Epoch [63/80], Loss: 0.0708\n",
      "Epoch [64/80], Loss: 0.0708\n",
      "Epoch [65/80], Loss: 0.0708\n",
      "Epoch [66/80], Loss: 0.0708\n",
      "Epoch [67/80], Loss: 0.0708\n",
      "Epoch [68/80], Loss: 0.0708\n",
      "Epoch [69/80], Loss: 0.0708\n",
      "Epoch [70/80], Loss: 0.0708\n",
      "Epoch [71/80], Loss: 0.0708\n",
      "Epoch [72/80], Loss: 0.0708\n",
      "Epoch [73/80], Loss: 0.0708\n",
      "Epoch [74/80], Loss: 0.0708\n",
      "Epoch [75/80], Loss: 0.0708\n",
      "Epoch [76/80], Loss: 0.0708\n",
      "Epoch [77/80], Loss: 0.0708\n",
      "Epoch [78/80], Loss: 0.0708\n",
      "Epoch [79/80], Loss: 0.0708\n",
      "Epoch [80/80], Loss: 0.0708\n",
      "test performance :  [96.93999481 89.94499207 85.92666626 80.84749603 79.96199799 74.68666077\n",
      " 73.85857391 70.91125488 70.6966629   0.        ]\n",
      "individual errors:  [tensor(88.7000, device='cuda:0'), tensor(70.1200, device='cuda:0'), tensor(69.1400, device='cuda:0'), tensor(70.0200, device='cuda:0'), tensor(66.8700, device='cuda:0'), tensor(67.1300, device='cuda:0'), tensor(68.7300, device='cuda:0'), tensor(66.1600, device='cuda:0'), tensor(69.4000, device='cuda:0')]\n",
      "Epoch [1/80], Loss: 0.1455\n",
      "Epoch [2/80], Loss: 0.0783\n",
      "Epoch [3/80], Loss: 0.0783\n",
      "Epoch [4/80], Loss: 0.0783\n",
      "Epoch [5/80], Loss: 0.0783\n",
      "Epoch [6/80], Loss: 0.0783\n",
      "Epoch [7/80], Loss: 0.0783\n",
      "Epoch [8/80], Loss: 0.0783\n",
      "Epoch [9/80], Loss: 0.0783\n",
      "Epoch [10/80], Loss: 0.0783\n",
      "Epoch [11/80], Loss: 0.0783\n",
      "Epoch [12/80], Loss: 0.0783\n",
      "Epoch [13/80], Loss: 0.0783\n",
      "Epoch [14/80], Loss: 0.0783\n",
      "Epoch [15/80], Loss: 0.0783\n",
      "Epoch [16/80], Loss: 0.0783\n",
      "Epoch [17/80], Loss: 0.0783\n",
      "Epoch [18/80], Loss: 0.0783\n",
      "Epoch [19/80], Loss: 0.0783\n",
      "Epoch [20/80], Loss: 0.0783\n",
      "Epoch [21/80], Loss: 0.0783\n",
      "Epoch [22/80], Loss: 0.0783\n",
      "Epoch [23/80], Loss: 0.0783\n",
      "Epoch [24/80], Loss: 0.0783\n",
      "Epoch [25/80], Loss: 0.0783\n",
      "Epoch [26/80], Loss: 0.0783\n",
      "Epoch [27/80], Loss: 0.0783\n",
      "Epoch [28/80], Loss: 0.0783\n",
      "Epoch [29/80], Loss: 0.0783\n",
      "Epoch [30/80], Loss: 0.0783\n",
      "Epoch [31/80], Loss: 0.0783\n",
      "Epoch [32/80], Loss: 0.0783\n",
      "Epoch [33/80], Loss: 0.0783\n",
      "Epoch [34/80], Loss: 0.0783\n",
      "Epoch [35/80], Loss: 0.0783\n",
      "Epoch [36/80], Loss: 0.0783\n",
      "Epoch [37/80], Loss: 0.0783\n",
      "Epoch [38/80], Loss: 0.0783\n",
      "Epoch [39/80], Loss: 0.0783\n",
      "Epoch [40/80], Loss: 0.0783\n",
      "Epoch [41/80], Loss: 0.0783\n",
      "Epoch [42/80], Loss: 0.0783\n",
      "Epoch [43/80], Loss: 0.0783\n",
      "Epoch [44/80], Loss: 0.0783\n",
      "Epoch [45/80], Loss: 0.0783\n",
      "Epoch [46/80], Loss: 0.0783\n",
      "Epoch [47/80], Loss: 0.0783\n",
      "Epoch [48/80], Loss: 0.0783\n",
      "Epoch [49/80], Loss: 0.0783\n",
      "Epoch [50/80], Loss: 0.0783\n",
      "Epoch [51/80], Loss: 0.0783\n",
      "Epoch [52/80], Loss: 0.0783\n",
      "Epoch [53/80], Loss: 0.0783\n",
      "Epoch [54/80], Loss: 0.0783\n",
      "Epoch [55/80], Loss: 0.0783\n",
      "Epoch [56/80], Loss: 0.0783\n",
      "Epoch [57/80], Loss: 0.0783\n",
      "Epoch [58/80], Loss: 0.0783\n",
      "Epoch [59/80], Loss: 0.0783\n",
      "Epoch [60/80], Loss: 0.0783\n",
      "Epoch [61/80], Loss: 0.0783\n",
      "Epoch [62/80], Loss: 0.0783\n",
      "Epoch [63/80], Loss: 0.0783\n",
      "Epoch [64/80], Loss: 0.0783\n",
      "Epoch [65/80], Loss: 0.0783\n",
      "Epoch [66/80], Loss: 0.0783\n",
      "Epoch [67/80], Loss: 0.0783\n",
      "Epoch [68/80], Loss: 0.0783\n",
      "Epoch [69/80], Loss: 0.0783\n",
      "Epoch [70/80], Loss: 0.0783\n",
      "Epoch [71/80], Loss: 0.0783\n",
      "Epoch [72/80], Loss: 0.0783\n",
      "Epoch [73/80], Loss: 0.0783\n",
      "Epoch [74/80], Loss: 0.0783\n",
      "Epoch [75/80], Loss: 0.0783\n",
      "Epoch [76/80], Loss: 0.0783\n",
      "Epoch [77/80], Loss: 0.0783\n",
      "Epoch [78/80], Loss: 0.0783\n",
      "Epoch [79/80], Loss: 0.0783\n",
      "Epoch [80/80], Loss: 0.0783\n",
      "test performance :  [96.93999481 89.94499207 85.92666626 80.84749603 79.96199799 74.68666077\n",
      " 73.85857391 70.91125488 70.6966629  66.77899933]\n",
      "individual errors:  [tensor(87.8100, device='cuda:0'), tensor(67.7100, device='cuda:0'), tensor(67.8000, device='cuda:0'), tensor(62.4100, device='cuda:0'), tensor(59.8600, device='cuda:0'), tensor(64.9900, device='cuda:0'), tensor(65.6400, device='cuda:0'), tensor(61.2600, device='cuda:0'), tensor(63.9400, device='cuda:0'), tensor(66.3700, device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "lam = 5e-6\n",
    "l2new = run_simulation( get_random_feature_model() ,train_loader,test_loader,L2(lam=lam),num_epochs=num_epochs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(resewc,label='ewc')\n",
    "plt.plot(resall,label='all')\n",
    "plt.plot(respp,label='ewc++ 100')\n",
    "plt.plot(resppbig,label='ewc++ 400')\n",
    "#plt.plot(resppvery,label='ewc++ 800')\n",
    "plt.plot(l2new,label='l2')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 6)\n"
     ]
    }
   ],
   "source": [
    "results = [resewc,resall,respp,resppbig,l2new]\n",
    "\n",
    "def res_to_mtx(losses,filename):\n",
    "    ks = [i for i in range(len(losses[0]))]\n",
    "    A = [np.array(ks)]\n",
    "    A += losses\n",
    "    A = np.vstack( A ).T\n",
    "    print(A.shape)\n",
    "    np.savetxt(filename, A, delimiter=' ')\n",
    "\n",
    "res_to_mtx(results,\"MNIST_permutation_rand_feat.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
